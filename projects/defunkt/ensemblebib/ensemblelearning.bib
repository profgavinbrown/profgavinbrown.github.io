



#MCS00

@inproceedings{Benediktsson00Consensus,
  author       = {Jon Benediktsson and Johannes R. Sveinsson},
  title        = {Consensus Based Classification of Multisource Remote Sensing
                  Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {280-289},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570280.htm},
}

@inproceedings{Bruzzone00Combining,
  author       = {Lorenzo Bruzzone and Roberto Cossu and Diego Fern{\'a}ndez
                  Prieto},
  title        = {Combining Parametric and Nonparametric Classifiers for an
                  Unsupervised Updating of Land-Cover Maps.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {290-299},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570290.htm},
}

@inproceedings{Cappelli00Combining,
  author       = {Raffaele Cappelli and Dario Maio and Davide Maltoni},
  title        = {Combining Fingerprint Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {351-361},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570351.htm},
}

@INPROCEEDINGS{Cohen00Hybrid,
  author       = {Shimon Cohen and Nathan Intrator},
  title        = {A Hybrid Projection Based and Radial Basis Function
                  Architecture.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {147-156},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570147.htm},
}

@INPROCEEDINGS{Conversano00Supervised,
  author       = {Claudio Conversano and Roberta Siciliano and Francesco Mola},
  title        = {Supervised Classifier Combination through Generalized
                  Additive Multi-model.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {167-176},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570167.htm},
}

@INPROCEEDINGS{Cordella00Cascaded,
  author       = {Luigi P. Cordella and Pasquale Foggia and Carlo Sansone and
                  Francesco Tortorella and Mario Vento},
  title        = {A Cascaded Multiple Expert System for Verification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {330-339},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570330.htm},
}

@INPROCEEDINGS{Dietterich00Ensemble,
  author       = {Thomas G. Dietterich},
  title        = {Ensemble Methods in Machine Learning.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {1-15},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570001.htm},
}

@INPROCEEDINGS{Diez00Applying,
  author       = {Juan J. Rodr\'{\i}guez Diez and Carlos Alonso Gonz{\'a}lez},
  title        = {Applying Boosting to Similarity Literals for Time Series
                  Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {210-219},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570210.htm},
}

@INPROCEEDINGS{Duin00Experiments,
  author       = {Robert P. W. Duin and David M. J. Tax},
  title        = {Experiments with Classifier Combining Rules.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {16-29},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570016.htm},
}

@INPROCEEDINGS{Fanelli00Modular,
  author       = {Anna Maria Fanelli and Giovanna Castellano and C. Alessandro
                  Buscicchio},
  title        = {A Modular Neuro-Fuzzy Network for Musical Instruments
                  Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {372-382},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570372.htm},
}

@INPROCEEDINGS{Froeba00Statistical,
  author       = {Bernhard Fr{\"o}ba and Constanze Rothe and Christian
                  K{\"u}blbeck},
  title        = {Statistical Sensor Calibration for Fusion of Different
                  Classifiers in a Biometric Person Recognition Framework.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {362-371},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570362.htm},
}

@INPROCEEDINGS{Furlanello00Boosting,
  author       = {Cesare Furlanello and Stefano Merler},
  title        = {Boosting of Tree-Based Classifiers for Predictive Risk
                  Modeling in GIS.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {220-229},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570220.htm},
}

@INPROCEEDINGS{Giacinto00Dynamic,
  author       = {Giorgio Giacinto and Fabio Roli},
  title        = {Dynamic Classifier Selection.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {177-189},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570177.htm},
}

@INPROCEEDINGS{Griffith00Self,
  author       = {Niall Griffith and Derek Partridge},
  title        = {Self-Organizing Decomposition of Functions in the Context of
                  a Unified Framework for Multiple Classifier Systems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {250-259},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570250.htm},
}

@INPROCEEDINGS{Grim00Combining,
  author       = {Jiri Grim and Josef Kittler and Pavel Pudil and Petr Somol},
  title        = {Combining Multiple Classifiers in Probabilistic Neural
                  Networks.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {157-166},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570157.htm},
}

@INPROCEEDINGS{Happel00Analysis,
  author       = {Mark D. Happel and Peter Bock},
  title        = {Analysis of a Fusion Method for Combining Marginal
                  Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {137-146},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570137.htm},
}

@INPROCEEDINGS{Ho00Complexity,
  author       = {Tin Kam Ho},
  title        = {Complexity of Classification Problems and Comparative
                  Advantages of Combined Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {97-106},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570097.htm},
}

@INPROCEEDINGS{Ianakiev00Architecture,
  author       = {Krassimir G. Ianakiev and Venu Govindaraju},
  title        = {Architecture for Classifier Combination Using Entropy
                  Measures.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {340-350},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570340.htm},
}

@INPROCEEDINGS{Impedovo00Evaluation,
  author       = {Sebastiano Impedovo and A. Salzo},
  title        = {A New Evaluation Method for Expert Combination in
                  Multi-expert System Designing.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {230-239},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570230.htm},
}

@INPROCEEDINGS{Jiang00Some,
  author       = {Wenxin Jiang},
  title        = {Some Results on Weakly Accurate Base Learners for Boosting
                  Regression and Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {87-96},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570087.htm},
}

@INPROCEEDINGS{Jiang00Classifier,
  author       = {Xiaoyi Jiang and Keren Yu and Horst Bunke},
  title        = {Classifier Combination for Grammar-Guided Sentence
                  Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {383-392},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570383.htm},
}

@PROCEEDINGS{Kittler00Multiple,
  editor       = {Josef Kittler and Fabio Roli},
  title        = {Multiple Classifier Systems, First International Workshop,
                  MCS 2000, Cagliari, Italy, June 21-23, 2000, Proceedings},
  booktitle    = {Multiple Classifier Systems},
  publisher    = {Springer},
  series       = {Lecture Notes in Computer Science},
  volume       = 1857,
  year         = 2000,
  isbn         = {3-540-67704-6},
}

@INPROCEEDINGS{Kleinberg00Mathematically,
  author       = {Eugene M. Kleinberg},
  title        = {A Mathematically Rigorous Foundation for Supervised
                  Learning.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {67-76},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570067.htm},
}

@INPROCEEDINGS{Kumar00Hierarchical,
  author       = {Shailesh Kumar and Joydeep Ghosh and Melba M. Crawford},
  title        = {A Hierarchical Multiclassifier System for Hyperspectral Data
                  Analysis.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {270-279},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570270.htm},
}

@INPROCEEDINGS{Kumazawa00Shape,
  author       = {Itsuo Kumazawa},
  title        = {Shape Matching and Extraction by an Array of
                  Figure-and-Ground Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {393-402},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570393.htm},
}

@INPROCEEDINGS{Lam00Classifier,
  author       = {Louisa Lam},
  title        = {Classifier Combinations: Implementations and Theoretical
                  Issues.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {77-86},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570077.htm},
}

@INPROCEEDINGS{Latinne00Different,
  author       = {Patrice Latinne and Olivier Debeir and Christine
                  Decaestecker},
  title        = {Different Ways of Weakening Decision Trees and Their Impact
                  on Classification Accuracy of DT Combination.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {200-209},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570200.htm},
}

@INPROCEEDINGS{Lecce00Multi,
  author       = {Vincenzo Di Lecce and Giovanni Dimauro and Andrea Guerriero
                  and Sebastiano Impedovo and Giuseppe Pirlo and A. Salzo},
  title        = {A Multi-expert System for Dynamic Signature Verification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {320-329},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570320.htm},
}

@INPROCEEDINGS{Masulli00Effectiveness,
  author       = {Francesco Masulli and Giorgio Valentini},
  title        = {Effectiveness of Error Correcting Output Codes in Multiclass
                  Learning Problems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {107-116},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570107.htm},
}

@INPROCEEDINGS{Pekalska00Combining,
  author       = {Elzbieta Pekalska and Marina Skurichina and Robert P. W.
                  Duin},
  title        = {Combining Fisher Linear Discriminants for Dissimilarity
                  Representations.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {117-126},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570117.htm},
}

@INPROCEEDINGS{Sharkey00Test,
  author       = {Amanda J. C. Sharkey and Noel E. Sharkey and Uwe Gerecke and
                  Gopinath Odayammadath Chandroth},
  title        = {The "Test and Select" Approach to Ensemble Combination.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {30-44},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570030.htm},
}

@INPROCEEDINGS{Skurichina00Boosting,
  author       = {Marina Skurichina and Robert P. W. Duin},
  title        = {Boosting in Linear Discriminant Analysis.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {190-199},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570190.htm},
}

@INPROCEEDINGS{Slavik00Lexicon,
  author       = {Petr Slav\'{\i}k and Venu Govindaraju},
  title        = {Use of Lexicon Density in Evaluating Word Recognizers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {310-319},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570310.htm},
}

@INPROCEEDINGS{Srihari00Survey,
  author       = {Sargur N. Srihari},
  title        = {A Survey of Sequential Combination of Word Recognizers in
                  Handwritten Phrase Recognition at CEDAR.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {45-51},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570045.htm},
}

@INPROCEEDINGS{Suen00Multiple,
  author       = {Ching Y. Suen and Louisa Lam},
  title        = {Multiple Classifier Combination Methodologies for Different
                  Output Levels.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {52-66},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570052.htm},
}

@INPROCEEDINGS{Takahashi00Learning,
  author       = {Katsuhiko Takahashi and Atsushi Sato},
  title        = {A Learning Method of Feature Selection for Rough
                  Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {127-136},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570127.htm},
}

@INPROCEEDINGS{Wan00Multiple,
  author       = {Weijian Wan and Donald Fraser},
  title        = {A Multiple Self-Organizing Map Scheme for Remote Sensing
                  Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {300-309},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570300.htm},
}

@INPROCEEDINGS{Wang00Diversity,
  author       = {Wenjia Wang and Phillis Jones and Derek Partridge},
  title        = {Diversity between Neural Networks and Decision Trees for
                  Building Multiple Classifier Systems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {240-249},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570240.htm},
}

@INPROCEEDINGS{Windeatt00Classifier,
  author       = {Terry Windeatt},
  title        = {Classifier Instability and Partitioning.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2000,
  pages        = {260-269},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/1857/18570260.htm},
}



#MCS01

@inproceedings{Alkoot01Improving,
  author       = {Fuad M. Alkoot and Josef Kittler},
  title        = {Improving Product by Moderating k-NN Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {429-439},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960429.htm},
}

@INPROCEEDINGS{Briem01Boosting,
  author       = {Gunnar Jakob Briem and Jon Atli Benediktsson and Johannes R.
                  Sveinsson},
  title        = {Boosting, Bagging, and Consensus Based Classification of
                  Multisource Remote Sensing Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {279-288},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960279.htm},
}

@INPROCEEDINGS{Bruzzone01Robust,
  author       = {Lorenzo Bruzzone and Roberto Cossu},
  title        = {A Robust Multiple Classifier System for a Partially
                  Unsupervised Updating of Land-Cover Maps.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {259-268},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960259.htm},
}

@INPROCEEDINGS{Chen01Averaging,
  author       = {Dechang Chen and Jian Liu},
  title        = {Averaging Weak Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {119-125},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960119.htm},
}

@INPROCEEDINGS{Cohen01Automatic,
  author       = {Shimon Cohen and Nathan Intrator},
  title        = {Automatic Model Selection in a Hybrid Perceptron/Radial
                  Network.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {440-454},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960440.htm},
}

@INPROCEEDINGS{Dahmen01Combined,
  author       = {J{\"o}rg Dahmen and Daniel Keysers and Hermann Ney},
  title        = {Combined Classification of Handwritten Digits Using the
                  'Virtual Test Sample Method'.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {109-118},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960109.htm},
}

@INPROCEEDINGS{Dietrich01Classification,
  author       = {Christian Dietrich and Friedhelm Schwenker and G{\"u}nther
                  Palm},
  title        = {Classification of Time Series Utilizing Temporal and Decision
                  Fusion.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {378-387},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960378.htm},
}

@INPROCEEDINGS{Diez01Learning,
  author       = {Juan J. Rodr\'{\i}guez Diez and Carlos Alonso Gonz{\'a}lez},
  title        = {Learning Classification RBF Networks by Boosting.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {43-52},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960043.htm},
}

@INPROCEEDINGS{Dolenko01Solar,
  author       = {S. A. Dolenko and Yu. V. Orlov and I. G. Persiantsev and Ju.
                  S. Shugai and A. V. Dmitriev and A. V. Suvorova and I. S.
                  Veselovsky},
  title        = {Solar Wind Data Analysis Using Self-Organizing Hierarchical
                  Neural Network Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {289-298},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960289.htm},
}

@INPROCEEDINGS{Foggia01Automatic,
  author       = {Pasquale Foggia and Carlo Sansone and Francesco Tortorella
                  and Mario Vento},
  title        = {Automatic Classification of Clustered Microcalcifications by
                  a Multiple Classifier System.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {208-217},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960208.htm},
}

@INPROCEEDINGS{Fred01Finding,
  author       = {Ana L. N. Fred},
  title        = {Finding Consistent Clusters in Data Partitions.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {309-318},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960309.htm},
}

@INPROCEEDINGS{Froeba01Combination,
  author       = {Bernhard Fr{\"o}ba and Walter Zink},
  title        = {On the Combination of Different Template Matching Strategies
                  for Fast Face Detection.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {418-428},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960418.htm},
}

@INPROCEEDINGS{Frossyniotis01Multi,
  author       = {Dimitrios S. Frossyniotis and Andreas Stafylopatis},
  title        = {A Multi-SVM Classification System.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {198-207},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960198.htm},
}

@INPROCEEDINGS{Fumera01Error,
  author       = {Giorgio Fumera and Fabio Roli},
  title        = {Error Rejection in Linearly Combined Multiple Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {329-338},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960329.htm},
}

@INPROCEEDINGS{Ghaderi01Least,
  author       = {Reza Ghaderi and Terry Windeatt},
  title        = {Least Squares and Estimation Measures via Error Correcting
                  Output Code.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {148-157},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960148.htm},
}

@INPROCEEDINGS{Gini01Mixing,
  author       = {Giuseppina C. Gini and Marco Lorenzini and Emilio Benfenati
                  and Raffaella Brambilla and Luca Malv{\'e}},
  title        = {Mixing a Symbolic and a Subsymbolic Expert to Improve
                  Carcinogenicity Prediction of Aromatic Compounds.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {126-135},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960126.htm},
}

@INPROCEEDINGS{Grim01Information,
  author       = {Jiri Grim and Josef Kittler and Pavel Pudil and Petr Somol},
  title        = {Information Analysis of Multiple Classifier Fusion.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {168-177},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960168.htm},
}

@INPROCEEDINGS{Hand01Multiple,
  author       = {David J. Hand and Niall M. Adams and Mark G. Kelly},
  title        = {Multiple Classifier Systems Based on Interpretable Linear
                  Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {136-147},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960136.htm},
}

@INPROCEEDINGS{Hartono01Learning,
  author       = {Pitoyo Hartono and Shuji Hashimoto},
  title        = {Learning-Data Selection Mechanism through Neural Networks
                  Ensemble.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {188-197},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960188.htm},
}

@INPROCEEDINGS{Higgins01Application,
  author       = {Jonathan E. Higgins and Tony J. Dodd and Robert I. Damper},
  title        = {Application of Multiple Classifier Techniques to Subband
                  Speaker Identification with an HMM/ANN System.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {369-377},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960369.htm},
}

@INPROCEEDINGS{Ho01Data,
  author       = {Tin Kam Ho},
  title        = {Data Complexity Analysis for Classifier Combination.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {53-67},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960053.htm},
}

@INPROCEEDINGS{Jorgensen01Feature,
  author       = {Thomas Martini J{\o}rgensen and Christian Linneberg},
  title        = {Feature Weighted Ensemble Classifiers - A Modified Decision
                  Scheme.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {218-227},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960218.htm},
}

@INPROCEEDINGS{Kittler01Relationship,
  author       = {Josef Kittler and Fuad M. Alkoot},
  title        = {Relationship of Sum and Vote Fusion Strategies.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {339-348},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960339.htm},
}

@PROCEEDINGS{Kittler01Multiple,
  editor       = {Josef Kittler and Fabio Roli},
  title        = {Multiple Classifier Systems, Second International Workshop,
                  MCS 2001 Cambridge, UK, July 2-4, 2001, Proceedings},
  booktitle    = {Multiple Classifier Systems},
  publisher    = {Springer},
  series       = {Lecture Notes in Computer Science},
  volume       = 2096,
  year         = 2001,
  isbn         = {3-540-42284-6},
}

@INPROCEEDINGS{Kuncheva01Complexity,
  author       = {Ludmila I. Kuncheva and Fabio Roli and Gian Luca Marcialis
                  and Catherine A. Shipp},
  title        = {Complexity of Data Subsets Generated by the Random Subspace
                  Method: An Experimental Investigation.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {349-358},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960349.htm},
}

@INPROCEEDINGS{Kuncheva01Feature,
  author       = {Ludmila I. Kuncheva and Christopher J. Whitaker},
  title        = {Feature Subsets for Classifier Combination: An Enumerative
                  Experiment.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {228-237},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960228.htm},
}

@INPROCEEDINGS{Langdon01Genetic,
  author       = {William B. Langdon and Bernard F. Buxton},
  title        = {Genetic Programming for Improved Receiver Operating
                  Characteristics.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {68-77},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960068.htm},
}

@INPROCEEDINGS{Latinne01Limiting,
  author       = {Patrice Latinne and Olivier Debeir and Christine
                  Decaestecker},
  title        = {Limiting the Number of Trees in Random Forests.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {178-187},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960178.htm},
}

@INPROCEEDINGS{Luttrell01Self,
  author       = {Stephen P. Luttrell},
  title        = {A Self-Organising Approach to Multiple Classifier Fusion.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {319-328},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960319.htm},
}

@INPROCEEDINGS{Marti01Positional,
  author       = {Urs-Viktor Marti and Horst Bunke},
  title        = {Use of Positional Information in Sequence Alignment for
                  Multiple Classifier Combination.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {388-398},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960388.htm},
}

@INPROCEEDINGS{Masulli01Dependence,
  author       = {Francesco Masulli and Giorgio Valentini},
  title        = {Dependence among Codeword Bits Errors in ECOC Learning
                  Machines: An Experimental Analysis.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {158-167},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960158.htm},
}

@INPROCEEDINGS{Merler01Tuning,
  author       = {Stefano Merler and Cesare Furlanello and Barbara Larcher and
                  Andrea Sboner},
  title        = {Tuning Cost-Sensitive Boosting and Its Application to
                  Melanoma Diagnosis.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {32-42},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960032.htm},
}

@INPROCEEDINGS{Oza01Input,
  author       = {Nikunj C. Oza and Kagan Tumer},
  title        = {Input Decimation Ensembles: Decorrelation through
                  Dimensionality Reduction.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {238-247},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960238.htm},
}

@INPROCEEDINGS{Pekalska01Combining,
  author       = {Elzbieta Pekalska and Robert P. W. Duin},
  title        = {On Combining Dissimilarity Representations.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {359-368},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960359.htm},
}

@INPROCEEDINGS{Prabhakar01Decision,
  author       = {Salil Prabhakar and Anil K. Jain},
  title        = {Decision-Level Fusion in Fingerprint Verification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {88-98},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960088.htm},
}

@INPROCEEDINGS{Roli01Methods,
  author       = {Fabio Roli and Giorgio Giacinto and Gianni Vernazza},
  title        = {Methods for Designing Multiple Classifier Systems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {78-87},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960078.htm},
}

@INPROCEEDINGS{Ruta01Application,
  author       = {Dymitr Ruta and Bogdan Gabrys},
  title        = {Application of the Evolutionary Algorithms for Classifier
                  Selection in Multiple Classifier Systems with Majority
                  Voting.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {399-408},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960399.htm},
}

@INPROCEEDINGS{Schwenker01Tree,
  author       = {Friedhelm Schwenker and G{\"u}nther Palm},
  title        = {Tree-Structured Support Vector Machines for Multi-class
                  Pattern Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {409-417},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960409.htm},
}

@INPROCEEDINGS{Sirlantzis01Genetic,
  author       = {Konstantinos Sirlantzis and Michael C. Fairhurst and Sanaul
                  Hoque},
  title        = {Genetic Algorithms for Multi-classifier System Configuration:
                  A Case Study in Character Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {99-108},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960099.htm},
}

@INPROCEEDINGS{Skurichina01Bagging,
  author       = {Marina Skurichina and Robert P. W. Duin},
  title        = {Bagging and the Random Subspace Method for Redundant Feature
                  Spaces.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {1-10},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960001.htm},
}

@INPROCEEDINGS{Smits01Combining,
  author       = {Paul C. Smits},
  title        = {Combining Supervised Remote Sensing Image Classifiers Based
                  on Individual Class Performances.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {269-278},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960269.htm},
}

@INPROCEEDINGS{Tapia01Generalized,
  author       = {Elizabeth Tapia and Jos{\'e} Carlos Gonz{\'a}lez and Julio
                  Villena},
  title        = {A Generalized Class of Boosting Algorithms Based on Recursive
                  Decoding Models.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {22-31},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960022.htm},
}

@INPROCEEDINGS{Tax01Combining,
  author       = {David M. J. Tax and Robert P. W. Duin},
  title        = {Combining One-Class Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {299-308},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960299.htm},
}



@INPROCEEDINGS{Wickramaratna01Performance,
  author       = {Jeevani Wickramaratna and Sean B. Holden and Bernard F.
                  Buxton},
  title        = {Performance Degradation in Boosting.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {11-21},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960011.htm},
}

@INPROCEEDINGS{Windridge01Classifier,
  author       = {David Windridge and Josef Kittler},
  title        = {Classifier Combination as a Tomographic Process.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2001,
  pages        = {248-258},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2096/20960248.htm},
}



#MCS02

@INPROCEEDINGS{Altincay02Post,
  author       = {Hakan Altin\c{c}ay and M{\"u}beccel Demirekler},
  title        = {Post-processing of Classifier Outputs in Multiple Classifier
                  Systems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {159-168},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640159.htm},
}

@INPROCEEDINGS{Benfenati02Combining,
  author       = {Emilio Benfenati and Paolo Mazzatorta and Daniel Neagu and
                  Giuseppina C. Gini},
  title        = {Combining Classifiers of Pesticides Toxicity through a
                  Neuro-fuzzy Approach.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {293-303},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640293.htm},
}

@INPROCEEDINGS{Caprile02Highlighting,
  author       = {Bruno Caprile and Cesare Furlanello and Stefano Merler},
  title        = {Highlighting Hard Patterns via AdaBoost Weights Evolution.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {72-80},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640072.htm},
}

@INPROCEEDINGS{Chawla02Distributed,
  author       = {Nitesh V. Chawla and Lawrence O. Hall and Kevin W. Bowyer and
                  Thomas E. Moore and W. Philip Kegelmeyer},
  title        = {Distributed Pasting of Small Votes.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {52-61},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640052.htm},
}

@INPROCEEDINGS{Cohen02Forward,
  author       = {Shimon Cohen and Nathan Intrator},
  title        = {Forward and Backward Selection in Regression Hybrid Network.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {98-107},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640098.htm},
}

@INPROCEEDINGS{Cordella02Multi,
  author       = {Luigi P. Cordella and Massimo De Santo and Gennaro
                  Percannella and Carlo Sansone and Mario Vento},
  title        = {A Multi-expert System for Movie Segmentation.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {304-313},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640304.htm},
}

@INPROCEEDINGS{Dzeroski02Stacking,
  author       = {Saso Dzeroski and Bernard Zenko},
  title        = {Stacking with Multi-response Model Trees.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {201-211},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640201.htm},
}

@INPROCEEDINGS{Ghosh02Multiclassifier,
  author       = {Joydeep Ghosh},
  title        = {Multiclassifier Systems: Back to the Future.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {1-15},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640001.htm},
}

@INPROCEEDINGS{Guenter02Generating,
  author       = {Simon G{\"u}nter and Horst Bunke},
  title        = {Generating Classifier Ensembles from Multiple Prototypes and
                  Its Application to Handwriting Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {179-188},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640179.htm},
}

@INPROCEEDINGS{Janeliunas02Reduction,
  author       = {Arunas Janeliunas and Sarunas Raudys},
  title        = {Reduction of the Boasting Bias of Linear Experts.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {242-251},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640242.htm},
}

@INPROCEEDINGS{Kittler02Decision,
  author       = {Josef Kittler and Marco Ballette and Jacek Czyz and Fabio
                  Roli and Luc Vandendorpe},
  title        = {Decision Level Fusion of Intramodal Personal Identity
                  Verification Experts.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {314-324},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640314.htm},
}

@INPROCEEDINGS{Kuncheva02Using,
  author       = {Ludmila I. Kuncheva and Christopher J. Whitaker},
  title        = {Using Diversity with Three Variants of Boosting: Aggressive,
                  Conservative, and Inverse.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {81-90},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640081.htm},
}

@INPROCEEDINGS{Lai02Combining,
  author       = {Carmen Lai and David M. J. Tax and Robert P. W. Duin and
                  Elzbieta Pekalska and Pavel Pacl\'{\i}k},
  title        = {On Combining One-Class Classifiers for Image Database
                  Retrieval.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {212-221},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640212.htm},
}

@INPROCEEDINGS{Masulli02Boosting,
  author       = {Francesco Masulli and Matteo Pardo and Giorgio Sberveglieri
                  and Giorgio Valentini},
  title        = {Boosting and Classification of Electronic Nose Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {262-271},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640262.htm},
}

@INPROCEEDINGS{Minguillon02Classifier,
  author       = {Juli{\`a} Minguill{\'o}n and Anne Rosemary Tate and Carles
                  Ar{\'u}s and John R. Griffiths},
  title        = {Classifier Combination for In Vivo Magnetic Resonance Spectra
                  of Brain Tumours.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {282-292},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640282.htm},
}

@INPROCEEDINGS{Mola02Discriminant,
  author       = {Francesco Mola and Roberta Siciliano},
  title        = {Discriminant Analysis and Factorial Multiple Splits in
                  Recursive Partitioning for Data Mining.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {118-126},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640118.htm},
}

@INPROCEEDINGS{Morgan02Adaptive,
  author       = {Joseph T. Morgan and Alex Henneguelle and Melba M. Crawford
                  and Joydeep Ghosh and Amy Neuenschwander},
  title        = {Adaptive Feature Spaces for Land Cover Classification with
                  Limited Ground Truth Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {189-200},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640189.htm},
}

@INPROCEEDINGS{Pekalska02Discussion,
  author       = {Elzbieta Pekalska and Robert P. W. Duin and Marina
                  Skurichina},
  title        = {A Discussion on the Classifier Projection Space for
                  Classifier Combining.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {137-148},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640137.htm},
}

@INPROCEEDINGS{Raudys02Multiple,
  author       = {Sarunas Raudys},
  title        = {Multiple Classification Systems in the Context of Feature
                  Extraction and Selection.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {27-41},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640027.htm},
}

@INPROCEEDINGS{Roli02Analysis,
  author       = {Fabio Roli and Giorgio Fumera},
  title        = {Analysis of Linear and Order Statistics Combiners for Fusion
                  of Imbalanced Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {252-261},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640252.htm},
}

@PROCEEDINGS{Roli02Multiple,
  editor       = {Fabio Roli and Josef Kittler},
  title        = {Multiple Classifier Systems, Third International Workshop,
                  MCS 2002, Cagliari, Italy, June 24-26, 2002, Proceedings},
  booktitle    = {Multiple Classifier Systems},
  publisher    = {Springer},
  series       = {Lecture Notes in Computer Science},
  volume       = 2364,
  year         = 2002,
  isbn         = {3-540-43818-1},
}

@INPROCEEDINGS{Roli02Multimodal,
  author       = {Fabio Roli and Josef Kittler and Giorgio Fumera and Daniele
                  Muntoni},
  title        = {An Experimental Comparison of Classifier Fusion Rules for
                  Multimodal Personal Identity Verification Systems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {325-336},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640325.htm},
}

@INPROCEEDINGS{Roli02Crisp,
  author       = {Fabio Roli and Sarunas Raudys and Gian Luca Marcialis},
  title        = {An Experimental Comparison of Fixed and Trained Fusion Rules
                  for Crisp Classifier Outputs.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {232-241},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640232.htm},
}

@INPROCEEDINGS{Ruta02Measure,
  author       = {Dymitr Ruta and Bogdan Gabrys},
  title        = {New Measure of Classifier Dependency in Multiple Classifier
                  Systems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {127-136},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640127.htm},
}

@INPROCEEDINGS{Schettini02Content,
  author       = {Raimondo Schettini and Carla Brambilla and Claudio Cusano},
  title        = {Content-Based Classification of Digital Photos.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {272-281},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640272.htm},
}

@INPROCEEDINGS{Sharkey02Types,
  author       = {Amanda J. C. Sharkey},
  title        = {Types of Multinet System.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {108-117},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640108.htm},
}

@INPROCEEDINGS{Sirlantzis02Trainable,
  author       = {Konstantinos Sirlantzis and Sanaul Hoque and Michael C.
                  Fairhurst},
  title        = {Trainable Multiple Classifier Schemes for Handwritten
                  Character Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {169-178},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640169.htm},
}

@INPROCEEDINGS{Skurichina02Bagging,
  author       = {Marina Skurichina and Ludmila Kuncheva and Robert P. W. Duin},
  title        = {Bagging and Boosting for the Nearest Mean Classifier: Effects
                  of Sample Size on Diversity and Accuracy.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {62-71},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640062.htm},
}

@INPROCEEDINGS{Valentini02Bias,
  author       = {Giorgio Valentini and Thomas G. Dietterich},
  title        = {Bias-Variance Analysis and Ensembles of SVM.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {222-231},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640222.htm},
}

@INPROCEEDINGS{Windeatt02Boosted,
  author       = {Terry Windeatt and Gholamreza Ardeshir},
  title        = {Boosted Tree Ensembles for Solving Multiclass Problems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {42-51},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640042.htm},
}

@INPROCEEDINGS{Windridge02General,
  author       = {David Windridge and Josef Kittler},
  title        = {On the General Application of the Tomographic Classifier
                  Fusion Methodology.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {149-158},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640149.htm},
}

@INPROCEEDINGS{Yang02Multistage,
  author       = {Shuang Yang and Antony Browne and Phil D. Picton},
  title        = {Multistage Neural Network Ensembles.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {91-97},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640091.htm},
}

@INPROCEEDINGS{Zhu02Support,
  author       = {Ji Zhu and Trevor Hastie},
  title        = {Support Vector Machines, Kernel Logistic Regression and
                  Boosting.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2002,
  pages        = {16-26},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2364/23640016.htm},
}



#MCS03

@INPROCEEDINGS{AhmadCVS03,
  author       = {Khurshid Ahmad and
                  Matthew Casey and
                  Bogdan Vrusias and
                  Panagiotis Saragiotis},
  title        = {Combining Multiple Modes of Information Using Unsupervised
                  Neural Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {236-245},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090236.htm},
}

@INPROCEEDINGS{Aksela03,
  author       = {Matti Aksela},
  title        = {Comparison of Classifier Selection Methods for Improving
                  Committee Performance.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {84-93},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090084.htm},
}

@INPROCEEDINGS{AlamRT03,
  author       = {Hassan Alam and
                  Ahmad Fuad Rezaur Rahman and
                  Yuliya Tarnikova},
  title        = {Solving Problems Two at a Time: Classification of Web Pages
                  Using a Generic Pair-Wise Multiple Classifier System.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {385-394},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090385.htm},
}

@INPROCEEDINGS{Arenas-GarciaFS03,
  author       = {J. Arenas-Garc\'{\i}a and
                  An\'{\i}bal R. Figueiras-Vidal and
                  Amanda J. C. Sharkey},
  title        = {The Beneficial Effects of Using Multi-net Systems That Focus
                  on Hard Patterns.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {45-54},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090045.htm},
}

@INPROCEEDINGS{AsdornwisedJ03,
  author       = {Widhyakorn Asdornwised and
                  Somchai Jitapunkul},
  title        = {Automatic Target Recognition Using Multiple Description
                  Coding Models for Multiple Classifier Systems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {336-345},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090336.htm},
}

@INPROCEEDINGS{AyadK03,
  author       = {Hanan Ayad and
                  Mohamed S. Kamel},
  title        = {Finding Natural Clusters Using Multi-clusterer Combiner
                  Based on Shared Nearest Neighbors.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {166-175},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090166.htm},
}

@INPROCEEDINGS{BanfieldHBK03,
  author       = {Robert E. Banfield and
                  Lawrence O. Hall and
                  Kevin W. Bowyer and
                  W. Philip Kegelmeyer},
  title        = {A New Ensemble Diversity Measure Applied to Thinning Ensembles.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {306-316},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090306.htm},
}

@INPROCEEDINGS{BaykutE03,
  author       = {Alper Baykut and
                  Ayt{\"u}l Er\c{c}il},
  title        = {Towards Automated Classifier Combination for Pattern Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {94-105},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090094.htm},
}

@INPROCEEDINGS{BrownW03,
  author       = {Gavin Brown and
                  Jeremy L. Wyatt},
  title        = {Negative Correlation Learning and the Ambiguity Family of
                  Ensemble Methods.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {266-275},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090266.htm},
}

@INPROCEEDINGS{Christensen03,
  author       = {Stefan W. Christensen},
  title        = {Ensemble Construction via Designed Output Distortion.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {286-295},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090286.htm},
}

@INPROCEEDINGS{CohenI03,
  author       = {Shimon Cohen and
                  Nathan Intrator},
  title        = {A Study of Ensemble of Hybrid Networks with Strong Regularization.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {227-235},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090227.htm},
}

@INPROCEEDINGS{Cutzu03,
  author       = {Florin Cutzu},
  title        = {Polychotomous Classification with Pairwise Classifiers:
                  A New Voting Principle.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {115-124},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090115.htm},
}

@INPROCEEDINGS{DuanKCSP03,
  author       = {Kaibo Duan and
                  S. Sathiya Keerthi and
                  Wei Chu and
                  Shirish Krishnaj Shevade and
                  Aun Neow Poo},
  title        = {Multi-category Classification by Soft-Max Combination of
                  Binary Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {125-134},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090125.htm},
}

@INPROCEEDINGS{EstruchFHR03,
  author       = {Vicent Estruch and
                  C{\'e}sar Ferri and
                  Jos{\'e} Hern{\'a}ndez-Orallo and
                  M. Jos{\'e} Ram\'{\i}rez-Quintana},
  title        = {Beam Search Extraction and Forgetting Strategies on Shared
                  Ensembles.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {206-216},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090206.htm},
}

@INPROCEEDINGS{FumeraR03,
  author       = {Giorgio Fumera and
                  Fabio Roli},
  title        = {Linear Combiners for Classifier Fusion: Some Theoretical
                  and Experimental Results.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {74-83},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090074.htm},
}

@INPROCEEDINGS{GiacintoRD03,
  author       = {Giorgio Giacinto and
                  Fabio Roli and
                  Luca Didaci},
  title        = {A Modular Multiple Classifier System for the Detection of
                  Intrusions in Computer Networks.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {346-355},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090346.htm},
}

@INPROCEEDINGS{GunterB03,
  author       = {Simon G{\"u}nter and
                  Horst Bunke},
  title        = {New Boosting Algorithms for Classification Problems with
                  Large Number of Classes Applied to a Handwritten Word Recognition
                  Task.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {326-335},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090326.htm},
}

@INPROCEEDINGS{InoueN03,
  author       = {Hirotaka Inoue and
                  Hiroyuki Narihisa},
  title        = {Improving Performance of a Multiple Classifier System Using
                  Self-generating Neural Networks.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {256-265},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090256.htm},
}

@INPROCEEDINGS{JaserKC03,
  author       = {Edward Jaser and
                  Josef Kittler and
                  William J. Christmas},
  title        = {Building Classifier Ensembles for Automatic Sports Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {366-374},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090366.htm},
}

@INPROCEEDINGS{KamelW03,
  author       = {Mohamed S. Kamel and
                  Nayer M. Wanas},
  title        = {Data Dependence in Combining Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {1-14},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090001.htm},
}

@INPROCEEDINGS{KittlerAW03,
  author       = {Josef Kittler and
                  Alireza Ahmadyfard and
                  David Windridge},
  title        = {Serial Multiple Classifier Systems Exploiting a Coarse to
                  Fine Output Coding.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {106-114},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090106.htm},
}

@INPROCEEDINGS{KoB03,
  author       = {Jaepil Ko and
                  Hyeran Byun},
  title        = {Binary Classifier Fusion Based on the Basic Decomposition
                  Methods.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {146-155},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090146.htm},
}

@INPROCEEDINGS{Kuncheva03,
  author       = {Ludmila I. Kuncheva},
  title        = {Error Bounds for Aggressive and Conservative AdaBoost.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {25-34},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090025.htm},
}

@INPROCEEDINGS{LewittP03,
  author       = {Michael Lewitt and
                  Robi Polikar},
  title        = {An Ensemble Approach for Data Fusion with Learn++.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {176-185},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090176.htm},
}

@INPROCEEDINGS{Luttrell03,
  author       = {Stephen P. Luttrell},
  title        = {A Markov Chain Approach to Multiple Classifier Fusion.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {217-226},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090217.htm},
}

@INPROCEEDINGS{Magee03,
  author       = {Derek R. Magee},
  title        = {A Sequential Scheduling Approach to Combining Multiple Object
                  Classifiers Using Cross-Entropy.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {135-145},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090135.htm},
}

@INPROCEEDINGS{McDonaldHE03,
  author       = {Ross A. McDonald and
                  David J. Hand and
                  Idris A. Eckley},
  title        = {An Empirical Comparison of Three Boosting Algorithms on
                  Real Data Sets with Artificial Class Noise.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {35-44},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090035.htm},
}

@INPROCEEDINGS{Oza03,
  author       = {Nikunj C. Oza},
  title        = {Boosting with Averaged Weight Vectors.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {15-24},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090015.htm},
}

@INPROCEEDINGS{OzaTTH03,
  author       = {Nikunj C. Oza and
                  Kagan Tumer and
                  Irem Y. Tumer and
                  Edward M. Huff},
  title        = {Classification of Aircraft Maneuvers for Fault Detection.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {375-384},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090375.htm},
}

@INPROCEEDINGS{RaudysR03,
  author       = {Sarunas Raudys and
                  Fabio Roli},
  title        = {The Behavior Knowledge Space Fusion Method: Analysis of
                  Generalization Error and Strategies for Performance Improvement.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {55-64},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090055.htm},
}

@INPROCEEDINGS{RaudysSB03,
  author       = {Sarunas Raudys and
                  Ray L. Somorjai and
                  Richard Baumgartner},
  title        = {Reducing the Overconfidence of Base Classifiers when Combining
                  Their Decisions.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {65-73},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090065.htm},
}

@INPROCEEDINGS{SantosVAF03,
  author       = {Rafael Valle dos Santos and
                  Marley B. R. Vellasco and
                  Fredy Artola and
                  S{\'e}rgio da Fontoura},
  title        = {Neural Net Ensembles for Lithology Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {246-255},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090246.htm},
}

@INPROCEEDINGS{SirlantzisHF03,
  author       = {Konstantinos Sirlantzis and
                  Sanaul Hoque and
                  Michael C. Fairhurst},
  title        = {Input Space Transformations for Multi-classifier Systems
                  Based on n-tuple Classifiers with Application to Handwriting
                  Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {356-365},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090356.htm},
}

@INPROCEEDINGS{TapiaGG03,
  author       = {Elizabeth Tapia and
                  Jos{\'e} Carlos Gonz{\'a}lez and
                  L. Javier Garc\'{\i}a-Villalba},
  title        = {Good Error Correcting Output Codes for Adaptive Multiclass
                  Learning.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {156-165},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090156.htm},
}

@INPROCEEDINGS{VelekJN03,
  author       = {Ondrej Velek and
                  Stefan J{\"a}ger and
                  Masaki Nakagawa},
  title        = {Accumulated-Recognition-Rate Normalization for Combining
                  Multiple On/Off-Line Japanese Character Classifiers Tested
                  on a Large Database.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {196-205},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090196.htm},
}

@INPROCEEDINGS{VerbaetenA03,
  author       = {Sofie Verbaeten and
                  Anneleen Van Assche},
  title        = {Ensemble Methods for Noise Elimination in Classification
                  Problems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {317-325},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090317.htm},
}

@INPROCEEDINGS{WilczokL03,
  author       = {Elke Wilczok and
                  Wolfgang Lellmann},
  title        = {Design and Evaluation of an Adaptive Combination Framework
                  for OCR Result Strings.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {395-404},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090395.htm},
}

@INPROCEEDINGS{WindeattGA03,
  author       = {Terry Windeatt and
                  Reza Ghaderi and
                  Gholamreza Ardeshir},
  title        = {Spectral Coefficients and Classifier Correlation.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {276-285},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090276.htm},
}

@PROCEEDINGS{mcs2003,
  editor       = {Terry Windeatt and
                  Fabio Roli},
  title        = {Multiple Classifier Systems, 4th International Workshop,
                  MCS 2003, Guilford, UK, June 11-13, 2003, Proceedings},
  booktitle    = {Multiple Classifier Systems},
  publisher    = {Springer},
  series       = {Lecture Notes in Computer Science},
  volume       = 2709,
  year         = 2003,
  isbn         = {3-540-40369-8},
}

@INPROCEEDINGS{WindridgeK03,
  author       = {David Windridge and
                  Josef Kittler},
  title        = {The Practical Performance Characteristics of Tomographically
                  Filtered Multiple Classifier Fusion.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {186-195},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090186.htm},
}

@INPROCEEDINGS{ZouariHLA03,
  author       = {H{\'e}la Zouari and
                  Laurent Heutte and
                  Yves Lecourtier and
                  Adel M. Alimi},
  title        = {Simulating Classifier Outputs for Evaluating Parallel Combination
                  Methods.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2003,
  pages        = {296-305},
  ee           = {http://link.springer.de/link/service/series/0558/bibs/2709/27090296.htm},
}



#MCS04

@INPROCEEDINGS{AyadBK04,
  author       = {Hanan Ayad and
                  Otman A. Basir and
                  Mohamed Kamel},
  title        = {A Probabilistic Model Using Information Theoretic Measures
                  for Cluster Ensembles.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {144-153},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=144},
}

@INPROCEEDINGS{BanfieldHBBKE04,
  author       = {Robert E. Banfield and
                  Lawrence O. Hall and
                  Kevin W. Bowyer and
                  Divya Bhadoria and
                  W. Philip Kegelmeyer and
                  Steven Eschrich},
  title        = {A Comparison of Ensemble Creation Techniques.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {223-232},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=223},
}

@INPROCEEDINGS{BonissoneGY04,
  author       = {Piero P. Bonissone and
                  Kai Goebel and
                  Weizhong Yan},
  title        = {Classifier Fusion Using Triangular Norms.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {154-163},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=154},
}

@INPROCEEDINGS{CaprileMFJ04,
  author       = {Bruno Caprile and
                  Stefano Merler and
                  Cesare Furlanello and
                  Giuseppe Jurman},
  title        = {Exact Bagging with k-Nearest Neighbour Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {72-81},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=72},
}

@INPROCEEDINGS{ChenKJ04,
  author       = {Lei Chen and
                  Mohamed Kamel and
                  Ju Jiang},
  title        = {A Modular System for the Classification of Time Series Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {134-143},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=134},
}

@INPROCEEDINGS{CordellaLS04,
  author       = {Luigi P. Cordella and
                  Alessandro Limongiello and
                  Carlo Sansone},
  title        = {Network Intrusion Detection by a Multi-stage Classification
                  System.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {324-333},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=324},
}

@INPROCEEDINGS{DaraK04,
  author       = {Rozita A. Dara and
                  Mohamed S. Kamel},
  title        = {Sharing Training Patterns among Multiple Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {243-252},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=243},
}

@INPROCEEDINGS{DidaciG04,
  author       = {Luca Didaci and
                  Giorgio Giacinto},
  title        = {Dynamic Classifier Selection by Adaptive k-Nearest-Neighbourhood
                  Rule.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {174-183},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=174},
}

@INPROCEEDINGS{DiegoMM04,
  author       = {Isaac Mart\'{\i}n de Diego and
                  Javier M. Moguerza and
                  Alberto Mu{\~n}oz},
  title        = {Combining Kernel Information for Support Vector Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {102-111},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=102},
}

@INPROCEEDINGS{EstruchFHR04,
  author       = {Vicent Estruch and
                  C{\'e}sar Ferri and
                  Jos{\'e} Hern{\'a}ndez-Orallo and
                  M. Jos{\'e} Ram\'{\i}rez-Quintana},
  title        = {Bagging Decision Multi-trees.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {41-51},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=41},
}

@INPROCEEDINGS{GunterB04,
  author       = {Simon G{\"u}nter and
                  Horst Bunke},
  title        = {Ensembles of Classifiers Derived from Multiple Prototypes
                  and Their Application to Handwriting Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {314-323},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=314},
}

@INPROCEEDINGS{Hernandez-EspinosaFT04,
  author       = {Carlos Hern{\'a}ndez-Espinosa and
                  Mercedes Fern{\'a}ndez-Redondo and
                  Joaqu\'{\i}n Torres-Sospedra},
  title        = {First Experiments on Ensembles of Radial Basis Functions.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {253-262},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=253},
}

@INPROCEEDINGS{JuszczakD04,
  author       = {Piotr Juszczak and
                  Robert P. W. Duin},
  title        = {Combining One-Class Classifiers to Classify Missing Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {92-101},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=92},
}

@INPROCEEDINGS{Kang04,
  author       = {Hee-Joong Kang},
  title        = {Combining Classifiers Using Dependency-Based Product Approximation
                  with Bayes Error Rate.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {112-121},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=112},
}

@INPROCEEDINGS{KittlerS04,
  author       = {Josef Kittler and
                  Mohammad Sadeghi},
  title        = {Physics-Based Decorrelation of Image Data for Decision Level
                  Fusion in Face Verification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {354-363},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=354},
}

@INPROCEEDINGS{Kuncheva04,
  author       = {Ludmila I. Kuncheva},
  title        = {Classifier Ensembles for Changing Environments.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {1-15},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=1},
}

@INPROCEEDINGS{MarcialisR04,
  author       = {Gian Luca Marcialis and
                  Fabio Roli},
  title        = {High Security Fingerprint Verification by Perceptron-Based
                  Fusion of Multiple Matchers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {364-373},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=364},
}

@INPROCEEDINGS{MarroccoT04,
  author       = {Claudio Marrocco and
                  Francesco Tortorella},
  title        = {A Method for Designing Cost-Sensitive ECOC.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {204-213},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=204},
}

@INPROCEEDINGS{MelvilleSMM04,
  author       = {Prem Melville and
                  Nishit Shah and
                  Lilyana Mihalkova and
                  Raymond J. Mooney},
  title        = {Experiments on Ensembles with Missing and Noisy Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {293-302},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=293},
}

@INPROCEEDINGS{MuhlbaierTP04,
  author       = {Michael Muhlbaier and
                  Apostolos Topalis and
                  Robi Polikar},
  title        = {Learn++.MT: A New Approach to Incremental Learning.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {52-61},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=52},
}

@INPROCEEDINGS{Oza04,
  author       = {Nikunj C. Oza},
  title        = {AveBoost2: Boosting for Noisy Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {31-40},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=31},
}

@INPROCEEDINGS{PekalskaSD04,
  author       = {Elzbieta Pekalska and
                  Marina Skurichina and
                  Robert P. W. Duin},
  title        = {Combining Dissimilarity-Based One-Class Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {122-133},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=122},
}

@INPROCEEDINGS{RahmanTKA04,
  author       = {Fuad Rahman and
                  Yuliya Tarnikova and
                  Aman Kumar and
                  Hassan Alam},
  title        = {Second Guessing a Commercial 'Black Box' Classifier by an
                  'In House' Classifier: Serial Classifier Combination in
                  a Speech Recognition Application.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {374-383},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=374},
}

@INPROCEEDINGS{RajanG04,
  author       = {Suju Rajan and
                  Joydeep Ghosh},
  title        = {An Empirical Comparison of Hierarchical vs. Two-Level Approaches
                  to Multiclass Problems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {283-292},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=283},
}

@INPROCEEDINGS{Rao04,
  author       = {Nageswara S. V. Rao},
  title        = {A Generic Sensor Fusion Problem: Classification and Function
                  Estimation.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {16-30},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=16},
}

@INPROCEEDINGS{RaudysI04,
  author       = {Sarunas Raudys and
                  Masakazu Iwamura},
  title        = {Multiple Classifiers System for Reducing Influences of Atypical
                  Observations.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {233-242},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=233},
}

@PROCEEDINGS{mcs2004,
  editor       = {Fabio Roli and
                  Josef Kittler and
                  Terry Windeatt},
  title        = {Multiple Classifier Systems, 5th International Workshop,
                  MCS 2004, Cagliari, Italy, June 9-11, 2004, Proceedings},
  booktitle    = {Multiple Classifier Systems},
  publisher    = {Springer},
  series       = {Lecture Notes in Computer Science},
  volume       = 3077,
  year         = 2004,
  isbn         = {3-540-22144-1},
  bibsource    = {DBLP, http://dblp.uni-trier.de},
}

@INPROCEEDINGS{RooneyPAT04,
  author       = {Niall Rooney and
                  David W. Patterson and
                  Sarab S. Anand and
                  Alexey Tsymbal},
  title        = {Dynamic Integration of Regression Models.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {164-173},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=164},
}

@INPROCEEDINGS{SaerensF04,
  author       = {Marco Saerens and
                  Francois Fouss},
  title        = {Yet Another Method for Combining Classifiers Outputs: A
                  Maximum Entropy Approach.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {82-91},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=82},
}

@INPROCEEDINGS{SchenkerBLK04,
  author       = {Adam Schenker and
                  Horst Bunke and
                  Mark Last and
                  Abraham Kandel},
  title        = {Building Graph-Based Classifier Ensembles by Random Node
                  Selection.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {214-222},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=214},
}

@INPROCEEDINGS{SvetnikLTW04,
  author       = {Vladimir Svetnik and
                  Andy Liaw and
                  Christopher Tong and
                  Ting Wang},
  title        = {Application of Breiman's Random Forest to Modeling Structure-Activity
                  Relationships of Pharmaceutical Molecules.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {334-343},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=334},
}

@INPROCEEDINGS{TapiaGHG04,
  author       = {Elizabeth Tapia and
                  Jos{\'e} Carlos Gonz{\'a}lez and
                  Alexander H{\"u}termann and
                  L. Javier Garc\'{\i}a-Villalba},
  title        = {Beyond Boosting: Recursive ECOC Learning Machines.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {62-71},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=62},
}

@INPROCEEDINGS{Valentini04,
  author       = {Giorgio Valentini},
  title        = {Random Aggregated and Bagged Ensembles of SVMs: An Empirical
                  Bias?Variance Analysis.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {263-272},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=263},
}

@INPROCEEDINGS{WangT04,
  author       = {Xiaogang Wang and
                  Xiaoou Tang},
  title        = {Experimental Study on Multiple LDA Classifier Combination
                  for High Dimensional Data Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {344-353},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=344},
}

@INPROCEEDINGS{Windeatt04,
  author       = {Terry Windeatt},
  title        = {Spectral Measure for Multi-class Problems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {184-193},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=184},
}

@INPROCEEDINGS{WindridgeB04,
  author       = {David Windridge and
                  Richard Bowden},
  title        = {Induced Decision Fusion in Automated Sign Language Interpretation:
                  Using ICA to Isolate the Underlying Components of Sign.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {303-313},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=303},
}

@INPROCEEDINGS{WindridgePK04,
  author       = {David Windridge and
                  Robin Patenall and
                  Josef Kittler},
  title        = {The Relationship between Classifier Factorisation and Performance
                  in Stochastic Vector Quantisation.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {194-203},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=194},
}

@INPROCEEDINGS{ZouariHLA04,
  author       = {H{\'e}la Zouari and
                  Laurent Heutte and
                  Yves Lecourtier and
                  Adel M. Alimi},
  title        = {Building Diverse Classifier Outputs to Evaluate the Behavior
                  of Combination Methods: The Case of Two Classifiers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2004,
  pages        = {273-282},
  ee           = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=3077{\&}spage=273},
}



#MCS05

@INPROCEEDINGS{AyadK05,
  author       = {Hanan Ayad and
                  Mohamed S. Kamel},
  title        = {Cluster-Based Cumulative Ensembles.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {236-245},
  ee           = {http://dx.doi.org/10.1007/11494683_24},
}

@INPROCEEDINGS{BanfieldHBK05,
  author       = {Robert E. Banfield and
                  Lawrence O. Hall and
                  Kevin W. Bowyer and
                  W. Philip Kegelmeyer},
  title        = {Ensembles of Classifiers from Spatially Disjoint Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {196-205},
  ee           = {http://dx.doi.org/10.1007/11494683_20},
}

@INPROCEEDINGS{BonissoneEG05,
  author       = {Piero P. Bonissone and
                  Neil Eklund and
                  Kai Goebel},
  title        = {Using an Ensemble of Classifiers to Audit a Production Classifier.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {376-386},
  ee           = {http://dx.doi.org/10.1007/11494683_38},
}

@INPROCEEDINGS{BrownWS05,
  author       = {Gavin Brown and
                  Jeremy L. Wyatt and
                  Ping Sun},
  title        = {Between Two Extremes: Examining Decompositions of the Ensemble
                  Objective Function.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {296-305},
  ee           = {http://dx.doi.org/10.1007/11494683_30},
}

@INPROCEEDINGS{ChawlaB05,
  author       = {Nitesh V. Chawla and
                  Kevin W. Bowyer},
  title        = {Designing Multiple Classifier Systems for Face Recognition.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {407-416},
  ee           = {http://dx.doi.org/10.1007/11494683_41},
}

@INPROCEEDINGS{ChenK05,
  author       = {Lei Chen and
                  Mohamed S. Kamel},
  title        = {Design of Multiple Classifier Systems for Time Series Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {216-225},
  ee           = {http://dx.doi.org/10.1007/11494683_22},
}

@INPROCEEDINGS{ChindaroSF05,
  author       = {Samuel Chindaro and
                  Konstantinos Sirlantzis and
                  Michael C. Fairhurst},
  title        = {Analysis and Modelling of Diversity Contribution to Ensemble-Based
                  Texture Recognition Performance.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {387-396},
  ee           = {http://dx.doi.org/10.1007/11494683_39},
}

@INPROCEEDINGS{DaraMK05,
  author       = {Rozita A. Dara and
                  Masoud Makrehchi and
                  Mohamed S. Kamel},
  title        = {Data Partitioning Evaluation Measures for Classifier Ensembles.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {306-315},
  ee           = {http://dx.doi.org/10.1007/11494683_31},
}

@INPROCEEDINGS{DuanK05,
  author       = {Kai-Bo Duan and
                  S. Sathiya Keerthi},
  title        = {Which Is the Best Multiclass SVM Method? An Empirical Study.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {278-285},
  ee           = {http://dx.doi.org/10.1007/11494683_28},
}

@INPROCEEDINGS{ErdemPGY05,
  author       = {Zeki Erdem and
                  Robi Polikar and
                  Fikret S. G{\"u}rgen and
                  Nejat Yumusak},
  title        = {Ensemble of SVMs for Incremental Learning.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {246-256},
  ee           = {http://dx.doi.org/10.1007/11494683_25},
}

@INPROCEEDINGS{ErdoganEEBEKA05,
  author       = {Hakan Erdogan and
                  Aytal Er\c{c}il and
                  Hazim Kemal Ekenel and
                  S. Y. Bilgin and
                  Ibrahim Eden and
                  Meltem Kiris\c{c}i and
                  Huseyin Abut},
  title        = {Multi-modal Person Recognition for Vehicular Applications.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {366-375},
  ee           = {http://dx.doi.org/10.1007/11494683_37},
}

@INPROCEEDINGS{Fierrez-AguilarGOG05,
  author       = {Julian Fi{\'e}rrez-Aguilar and
                  Daniel Garcia-Romero and
                  Javier Ortega-Garcia and
                  Joaquin Gonzalez-Rodriguez},
  title        = {Speaker Verification Using Adapted User-Dependent Multilevel
                  Fusion.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {356-365},
  ee           = {http://dx.doi.org/10.1007/11494683_36},
}

@INPROCEEDINGS{FumeraRS05,
  author       = {Giorgio Fumera and
                  Fabio Roli and
                  Alessandra Serrau},
  title        = {Dynamics of Variance Reduction in Bagging and Other Techniques
                  Based on Randomisation.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {316-325},
  ee           = {http://dx.doi.org/10.1007/11494683_32},
}

@INPROCEEDINGS{Gal-OrMS05,
  author       = {Mordechai Gal-Or and
                  Jerrold H. May and
                  William E. Spangler},
  title        = {Using Decision Tree Models and Diversity Measures in the
                  Selection of Ensemble Classification Models.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {186-195},
  ee           = {http://dx.doi.org/10.1007/11494683_19},
}

@INPROCEEDINGS{HessKK05,
  author       = {Andreas He{\ss} and
                  Rinat Khoussainov and
                  Nicholas Kushmerick},
  title        = {Ensemble Learning with Biased Classifiers: The Triskel Algorithm.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {226-235},
  ee           = {http://dx.doi.org/10.1007/11494683_23},
}

@INPROCEEDINGS{KapoorAP05,
  author       = {Ashish Kapoor and
                  Hyungil Ahn and
                  Rosalind W. Picard},
  title        = {Mixture of Gaussian Processes for Combining Multiple Modalities.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {86-96},
  ee           = {http://dx.doi.org/10.1007/11494683_9},
}

@INPROCEEDINGS{KimK05,
  author       = {Eunju Kim and
                  Jaepil Ko},
  title        = {Dynamic Classifier Integration Method.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {97-107},
  ee           = {http://dx.doi.org/10.1007/11494683_10},
}

@INPROCEEDINGS{LandgrebePTD05,
  author       = {Thomas Landgrebe and
                  Pavel Pacl\'{\i}k and
                  David M. J. Tax and
                  Robert P. W. Duin},
  title        = {Optimising Two-Stage Recognition Systems.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {206-215},
  ee           = {http://dx.doi.org/10.1007/11494683_21},
}

@INPROCEEDINGS{LeiG05,
  author       = {Hansheng Lei and
                  Venu Govindaraju},
  title        = {Half-Against-Half Multi-class Support Vector Machines.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {156-164},
  ee           = {http://dx.doi.org/10.1007/11494683_16},
}

@INPROCEEDINGS{LiCFK05,
  author       = {Peng Li and
                  Kap Luk Chan and
                  Sheng Fu and
                  Shankar Muthu Krishnan},
  title        = {An Abnormal ECG Beat Detection Approach for Long-Term Monitoring
                  of Heart Patients Based on Hybrid Kernel Machine Ensemble.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {346-355},
  ee           = {http://dx.doi.org/10.1007/11494683_35},
}

@inproceedings{MelnikVZ05,
  author       = {Ofer Melnik and
                  Yehuda Vardi and
                  Cun-Hui Zhang},
  title        = {A Probability Model for Combining Ranks.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {64-73},
  ee           = {http://dx.doi.org/10.1007/11494683_7},
}

@INPROCEEDINGS{MuhlbaierTP05,
  author       = {Michael Muhlbaier and
                  Apostolos Topalis and
                  Robi Polikar},
  title        = {Ensemble Confidence Estimates Posterior Probability.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {326-335},
  ee           = {http://dx.doi.org/10.1007/11494683_33},
}

@INPROCEEDINGS{Narasimhamurthy05,
  author       = {Anand M. Narasimhamurthy},
  title        = {Evaluation of Diversity Measures for Binary Classifier Ensembles.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {267-277},
  ee           = {http://dx.doi.org/10.1007/11494683_27},
}

@INPROCEEDINGS{NishidaK05,
  author       = {Kenji Nishida and
                  Takio Kurita},
  title        = {Boosting Soft-Margin SVM with Feature Selection for Pedestrian
                  Detection.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {22-31},
  ee           = {http://dx.doi.org/10.1007/11494683_3},
}

@INPROCEEDINGS{NishidaYO05,
  author       = {Kyosuke Nishida and
                  Koichiro Yamauchi and
                  Takashi Omori},
  title        = {ACE: Adaptive Classifiers-Ensemble System for Concept-Drifting
                  Environments.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {176-185},
  ee           = {http://dx.doi.org/10.1007/11494683_18},
}

@INPROCEEDINGS{PaclikLTD05,
  author       = {Pavel Pacl\'{\i}k and
                  Thomas Landgrebe and
                  David M. J. Tax and
                  Robert P. W. Duin},
  title        = {On Deriving the Second-Stage Training Set for Trainable
                  Combiners.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {136-146},
  ee           = {http://dx.doi.org/10.1007/11494683_14},
}

@INPROCEEDINGS{PatenallWK05,
  author       = {Robin Patenall and
                  David Windridge and
                  Josef Kittler},
  title        = {Multiple Classifier Fusion Performance in Networked Stochastic
                  Vector Quantisers.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {128-135},
  ee           = {http://dx.doi.org/10.1007/11494683_13},
}

@INPROCEEDINGS{PohB05,
  author       = {Norman Poh and
                  Samy Bengio},
  title        = {EER of Fixed and Trainable Fusion Classifiers: A Theoretical
                  Study with Application to Biometric Authentication Tasks.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {74-85},
  ee           = {http://dx.doi.org/10.1007/11494683_8},
}

@INPROCEEDINGS{PranckevicieneBS05,
  author       = {Erinija Pranckeviciene and
                  Richard Baumgartner and
                  Ray L. Somorjai},
  title        = {Using Domain Knowledge for in the Random Subspace Method:
                  Application: Application to the Classification of Biomedical
                  Spectra.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {336-345},
  ee           = {http://dx.doi.org/10.1007/11494683_34},
}

@INPROCEEDINGS{PriorW05,
  author       = {Matthew Prior and
                  Terry Windeatt},
  title        = {Over-Fitting in Ensembles of Neural Network Classifiers
                  Within ECOC Frameworks.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {286-295},
  ee           = {http://dx.doi.org/10.1007/11494683_29},
}

@INPROCEEDINGS{RajanG05,
  author       = {Suju Rajan and
                  Joydeep Ghosh},
  title        = {Exploiting Class Hierarchies for Knowledge Transfer in Hyperspectral
                  Data.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {417-427},
  ee           = {http://dx.doi.org/10.1007/11494683_42},
}

@INPROCEEDINGS{RedpathL05,
  author       = {D. B. Redpath and
                  K. Lebart},
  title        = {Observations on Boosting Feature Selection.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {32-41},
  ee           = {http://dx.doi.org/10.1007/11494683_4},
}

@INPROCEEDINGS{Roli05,
  author       = {Fabio Roli},
  title        = {Semi-supervised Multiple Classifier Systems: Background
                  and Research Directions.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {1-11},
  ee           = {http://dx.doi.org/10.1007/11494683_1},
}

@INPROCEEDINGS{SantoPSV05,
  author       = {Massimo De Santo and
                  Gennaro Percannella and
                  Carlo Sansone and
                  Mario Vento},
  title        = {Combining Audio-Based and Video-Based Shot Classification
                  Systems for News Videos Segmentation.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {397-406},
  ee           = {http://dx.doi.org/10.1007/11494683_40},
}

@INPROCEEDINGS{SkurichinaD05,
  author       = {Marina Skurichina and
                  Robert P. W. Duin},
  title        = {Combining Feature Subsets in Feature Selection.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {165-175},
  ee           = {http://dx.doi.org/10.1007/11494683_17},
}

@INPROCEEDINGS{SmithW05,
  author       = {R. S. Smith and
                  Terry Windeatt},
  title        = {Decoding Rules for Error Correcting Output Code Ensembles.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {53-63},
  ee           = {http://dx.doi.org/10.1007/11494683_6},
}

@INPROCEEDINGS{TapiaSG05,
  author       = {Elizabeth Tapia and
                  Esteban Serra and
                  Jos{\'e} Carlos Gonz{\'a}lez},
  title        = {Recursive ECOC for Microarray Data Classification.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {108-117},
  ee           = {http://dx.doi.org/10.1007/11494683_11},
}

@INPROCEEDINGS{ThielSP05,
  author       = {Christian Thiel and
                  Friedhelm Schwenker and
                  G{\"u}nther Palm},
  title        = {Using Dempster-Shafer Theory in MCF Systems to Reject Samples.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {118-127},
  ee           = {http://dx.doi.org/10.1007/11494683_12},
}

@INPROCEEDINGS{TianYH05,
  author       = {Qi Tian and
                  Jie Yu and
                  Thomas S. Huang},
  title        = {Boosting Multiple Classifiers Constructed by Hybrid Discriminant
                  Analysis.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {42-52},
  ee           = {http://dx.doi.org/10.1007/11494683_5},
}

@INPROCEEDINGS{TulyakovG05,
  author       = {Sergey Tulyakov and
                  Venu Govindaraju},
  title        = {Using Independence Assumption to Improve Multimodal Biometric
                  Fusion.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {147-155},
  ee           = {http://dx.doi.org/10.1007/11494683_15},
}

@INPROCEEDINGS{WangZL05,
  author       = {Fei Wang and
                  Changshui Zhang and
                  Naijiang Lu},
  title        = {Boosting GMM and Its Two Applications.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {12-21},
  ee           = {http://dx.doi.org/10.1007/11494683_2},
}

@INPROCEEDINGS{YangQ05,
  author       = {Li-ying Yang and
                  Zheng Qin},
  title        = {Design of a New Classifier Simulator.},
  booktitle    = {Multiple Classifier Systems},
  year         = 2005,
  pages        = {257-266},
  ee           = {http://dx.doi.org/10.1007/11494683_26},
}


#MCS07


@inproceedings{mottl04combining,
  author =	 "Vadim Mottl and Alexander Tatarchuk and Valentina Sulimova and Olga Krasotkina and Oleg Seredin",
  title =	 "Combining Pattern Recognition Modalities at the Sensor Level Via Kernel Fusion",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "1-12",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{windridge04neutral,
  author =	 "David Windridge and Vadim Mottl and Alexander Tatarchuk and Andrey Eliseyev",
  title =	 "The Neutral Point Method for Kernel-Based Combination of Disjoint Training Data in Multi-modal Pattern Recognition",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "13-21",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{lee04kernel,
  author =	 "Wan-Jui Lee and Sergey Verzakov and Robert P. W. Duin",
  title =	 "Kernel Combination Versus Classifier Combination",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "22-31",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{merler04deriving,
  author =	 "Stefano Merler and Giuseppe Jurman and Cesare Furlanello",
  title =	 "Deriving the Kernel from Training Data",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "32-41",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{lienemann04application,
  author =	 "Kai Lienemann and Thomas Pltz and Gernot A. Fink",
  title =	 "On the Application of SVM-Ensembles Based on Adapted Random Subspace Sampling for Automatic Classification of NMR Data",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "42-51",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{ko04new,
  author =	 "Albert Hung-Ren Ko and Robert Sabourin and Alceu de Souza Britto Jr.",
  title =	 "A New HMM-Based Ensemble Generation Method for Numeral Recognition",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "52-61",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{raudys04classifiers,
  author =	 "Sarunas Raudys and mer Kaan Baykan and Ahmet Babalik and Vitalij Denisov and Antanas Andrius Bielskis",
  title =	 "Classifiers Fusion in Recognition of Wheat Varieties",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "62-71",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{bertolami04multiple,
  author =	 "Roman Bertolami and Horst Bunke",
  title =	 "Multiple Classifier Methods for Offline Handwritten Text Line Recognition",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "72-81",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{christensen04applying,
  author =	 "Hans Ulrich Christensen and Daniel Ortiz Arroyo",
  title =	 "Applying Data Fusion Methods to Passage Retrieval in QAS",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "82-92",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{mohamed04co-training,
  author =	 "Tawfik A. Mohamed and Neamat El Gayar and Amir F. Atiya",
  title =	 "A Co-training Approach for Time Series Prediction with Missing Data",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "93-102",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{sun04improved,
  author =	 "Shiliang Sun",
  title =	 "An Improved Random Subspace Method and Its Application to EEG Signal Classification",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "103-112",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{sun04ensemble,
  author =	 "Shiliang Sun",
  title =	 "Ensemble Learning Methods for Classifying EEG Signals",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "113-120",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{sadeghi04confidence,
  author =	 "Mohammad Sadeghi and Samaneh Khoshrou and Josef Kittler",
  title =	 "Confidence Based Gating of Colour Features for Face Authentication",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "121-130",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{ebrahimpour04view-based,
  author =	 "Reza Ebrahimpour and Ehsanollah Kabir and Mohammad Reza Yousefi",
  title =	 "View-Based Eigenspaces with Mixture of Experts for View-Independent Face Recognition",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "131-140",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{serrano04fusion,
  author =	 "ngel Serrano and Isaac Martn de Diego and Cristina Conde and Enrique Cabello and Li Bai and LinLin Shen",
  title =	 "Fusion of Support Vector Classifiers for Parallel Gabor Methods Applied to Face Verification",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "141-150",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{marcialis04serial,
  author =	 "Gian Luca Marcialis and Fabio Roli",
  title =	 "Serial Fusion of Fingerprint and Face Matchers",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "151-160",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{hall04boosting,
  author =	 "Lawrence O. Hall and Robert E. Banfield and Kevin W. Bowyer and W. Philip Kegelmeyer",
  title =	 "Boosting Lite - Handling Larger Datasets and Slower Base Classifiers",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "161-170",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{meynet04information,
  author =	 "Julien Meynet and Jean-Philippe Thiran",
  title =	 "Information Theoretic Combination of Classifiers with Application to AdaBoost",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "171-179",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{lu04interactive,
  author =	 "Yijuan Lu and Qi Tian and Thomas S. Huang",
  title =	 "Interactive Boosting for Image Classification",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "180-189",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{bicego04group-induced,
  author =	 "Manuele Bicego and Elzbieta Pekalska and Robert P. W. Duin",
  title =	 "Group-Induced Vector Spaces",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "190-199",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{hadjitodorov04selecting,
  author =	 "Stefan Todorov Hadjitodorov and Ludmila I. Kuncheva",
  title =	 "Selecting Diversifying Heuristics for Cluster Ensembles",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "200-209",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{haindl04unsupervised,
  author =	 "Michal Haindl and Stanislav Mikes",
  title =	 "Unsupervised Texture Segmentation Using Multiple Segmenters Strategy",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "210-219",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{riesen04classifier,
  author =	 "Kaspar Riesen and Horst Bunke",
  title =	 "Classifier Ensembles for Vector Space Embedding of Graphs",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "220-230",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{maudes04cascading,
  author =	 "Jess Maudes and Juan J. Rodrguez and Cesar Garca-Osorio",
  title =	 "Cascading for Nominal Data",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "231-240",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{kudo04combination,
  author =	 "Mineichi Kudo and Satoshi Shirai and Hiroshi Tenmoto",
  title =	 "A Combination of Sample Subsets and Feature Subsets in One-Against-Other Classifiers",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "241-250",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{depasquale04random,
  author =	 "Joseph DePasquale and Robi Polikar",
  title =	 "Random Feature Subset Selection for Ensemble Based Classification of Data with Missing Features",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "251-260",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{silva04feature,
  author =	 "Hugo Silva and Ana L. N. Fred",
  title =	 "Feature Subspace Ensembles: A Parallel Classifier Combination Scheme Using Feature Selection",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "261-270",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{windeatt04stopping,
  author =	 "Terry Windeatt and Matthew Prior",
  title =	 "Stopping Criteria for Ensemble-Based Feature Selection",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "271-281",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{foggia04rejecting,
  author =	 "Pasquale Foggia and Gennaro Percannella and Carlo Sansone and Mario Vento",
  title =	 "On Rejecting Unreliably Classified Patterns",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "282-291",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{biggio04bayesian,
  author =	 "Battista Biggio and Giorgio Fumera and Fabio Roli",
  title =	 "Bayesian Analysis of Linear Combiners",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "292-301",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{ko04applying,
  author =	 "Albert Hung-Ren Ko and Robert Sabourin and Alceu de Souza Britto Jr.",
  title =	 "Applying Pairwise Fusion Matrix on Fusion Functions for Classifier Combination",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "302-311",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{chindaro04modelling,
  author =	 "Samuel Chindaro and Konstantinos Sirlantzis and Michael C. Fairhurst",
  title =	 "Modelling Multiple-Classifier Relationships Using Bayesian Belief Networks",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "312-321",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{li04classifier,
  author =	 "Shoushan Li and Chengqing Zong",
  title =	 "Classifier Combining Rules Under Independence Assumptions",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "322-332",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{marrocco04embedding,
  author =	 "Claudio Marrocco and Paolo Simeone and Francesco Tortorella",
  title =	 "Embedding Reject Option in ECOC Through LDPC Codes",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "333-343",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{poh04combination,
  author =	 "Norman Poh and Guillaume Heusch and Josef Kittler",
  title =	 "On Combination of Face Authentication Experts by a Mixture of Quality Dependent Fusion Classifiers",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "344-356",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{tronci04index,
  author =	 "Roberto Tronci and Giorgio Giacinto and Fabio Roli",
  title =	 "Index Driven Combination of Multiple Biometric Experts for AUC Maximisation",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "357-366",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{kryszczuk04q-stack:,
  author =	 "Krzysztof Kryszczuk and Andrzej Drygajlo",
  title =	 "Q-stack: Uni- and Multimodal Classifier Stacking with Quality Measures",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "367-376",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{richiardi04reliability-based,
  author =	 "Jonas Richiardi and Andrzej Drygajlo",
  title =	 "Reliability-Based Voting Schemes Using Modality-Independent Features in Multi-classifier Biometric Authentication",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "377-386",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{tulyakov04optimal,
  author =	 "Sergey Tulyakov and Venu Govindaraju and Chaohong Wu",
  title =	 "Optimal Classifier Combination Rules for Verification and Identification Systems",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "387-396",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{chawla04exploiting,
  author =	 "Nitesh V. Chawla and Jared Sylvester",
  title =	 "Exploiting Diversity in Ensembles: Improving the Performance on Unbalanced Datasets",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "397-406",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{chung04diversity-performance,
  author =	 "Yun Sheng Chung and D. Frank Hsu and Chuan Yi Tang",
  title =	 "On the Diversity-Performance Relationship for Majority Voting in Classifier Ensembles",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "407-420",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{cecotti04hierarchical,
  author =	 "Hubert Cecotti and Abdel Belad",
  title =	 "Hierarchical Behavior Knowledge Space",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "421-430",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{ko04new,
  author =	 "Albert Hung-Ren Ko and Robert Sabourin and Alceu de Souza Britto Jr.",
  title =	 "A New Dynamic Ensemble Selection Method for Numeral Recognition",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "431-439",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{zanda04ensemble,
  author =	 "Manuela Zanda and Gavin Brown and Giorgio Fumera and Fabio Roli",
  title =	 "Ensemble Learning in Linearly Combined Classifiers Via Negative Correlation",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "440-449",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{rodrguez04nave,
  author =	 "Juan J. Rodrguez and Ludmila I. Kuncheva",
  title =	 "Nave Bayes Ensembles with a Random Oracle",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "450-458",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{kuncheva04experimental,
  author =	 "Ludmila I. Kuncheva and Juan J. Rodrguez",
  title =	 "An Experimental Study on Rotation Forest Ensembles",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "459-468",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{kanevskiy04cooperative,
  author =	 "Daniel Kanevskiy and Konstantin Vorontsov",
  title =	 "Cooperative Coevolutionary Ensemble Learning",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "469-478",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{berkman04robust,
  author =	 "Omer Berkman and Nathan Intrator",
  title =	 "Robust Inference in Bayesian Networks with Application to Gene Expression Temporal Data",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "479-489",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{muhlbaier04ensemble,
  author =	 "Michael Muhlbaier and Robi Polikar",
  title =	 "An Ensemble Approach for Incremental Learning in Nonstationary Environments",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "490-500",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{benediktsson04multiple,
  author =	 "Jon Atli Benediktsson and Jocelyn Chanussot and Mathieu Fauvel",
  title =	 "Multiple Classifier Systems in Remote Sensing: From Basics to Recent Developments",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "501-512",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


@inproceedings{bengio04biometric,
  author =	 "Samy Bengio and Johnny Marithoz",
  title =	 "Biometric Person Authentication Is a Multiple Classifier Problem",
  editor =	 "M. Haindl, J. Kittler, F. Roli",
  booktitle =	 "Proc. Int. Workshop on Multiple Classifier Systems (LNCS 4472)",
  publisher =	 "Springer",
  month =	 "May 23-25",
  pages =	 "513-522",
  year =	 "2007",
  address =	 "Prague, Czech Republic"
}


#POST2000NONMCSPAPERS

@article{martinez06using,
  author = {Gonzalo Mart{\'{\i}}nez-Mu{\~n}oz and Alberto Su{\'a}rez},
  title = {Using Boosting to Prune Bagging Ensembles},
  journal = {Pattern Recognition Letters},
  pages = {156--165},
  volume = {28},
  year = {2007},
  number = {1},
  abstract = {Boosting is used to determine the order in which classifiers
    are aggregated in a bagging ensemble. Early stopping in the aggregation
    of the classifiers in the ordered bagging ensemble allows the identification
    of subensembles that require less memory for storage, classify faster and
    can improve the generalization accuracy of the original bagging ensemble.
    In all the classification problems investigated pruned ensembles with 20\%
    of the original classifiers show statistically significant improvements
    over bagging. In problems where boosting is superior to bagging, these
    improvements are not sufficient to reach the accuracy of the corresponding
    boosting ensembles. However, ensemble pruning preserves the performance of
    bagging in noisy classification tasks, where boosting often has larger
    generalization errors. Therefore, pruned bagging should generally be
    preferred to complete bagging and, if no information about the level of
    noise is available, it is a robust alternative to AdaBoost.}
}

@article{martinez05switching,
  author = {Martinez-Munoz, G. and Suarez, A.},
  title = {Switching class labels to generate classification ensembles},
  journal = {Pattern Recognition},
  year = {2005},
  volume = {38},
  pages = {1483--1494},
  number = {10},
  abstract = {Ensembles that combine the decisions of classifiers generated by using
    perturbed versions of the training set where the classes of the training
    examples are randomly switched can produce a significant error reduction,
    provided that large numbers of units and high class switching rates
    are used. The classifiers generated by this procedure have statistically
    uncorrelated errors in the training set. Hence, the ensembles they
    form exhibit a similar dependence of the training error on ensemble
    size, independently of the classification problem. In particular,
    for binary classification problems, the classification performance
    of the ensemble on the training data can be analysed in terms of
    a Bernoulli process. Experiments on several UCI datasets demonstrate
    the improvements in classification accuracy that can be obtained
    using these class-switching ensembles. (c) 2005 Pattern Recognition
    Society. Published by Elsevier Ltd. All rights reserved.}
}

@article{martinez04using,
  author = {Gonzalo Mart{\'{\i}}nez-Mu{\~n}oz and Alberto Su{\'a}rez},
  title = {Using All Data to Generate Decision Tree Ensembles},
  journal = {IEEE Transactions on Systems, Man and Cybernetics part {C}},
  year = {2004},
  volume = {34},
  pages = {393--397},
  number = {4}
}

@inproceedings{hernandez06pruning,
  author    = {Daniel Hern{\'a}ndez-Lobato and
               Jos{\'e} Miguel Hern{\'a}ndez-Lobato and
               Rub{\'e}n Ruiz-Torrubiano and
               {\'A}ngel Valle},
  title     = {Pruning Adaptive Boosting Ensembles by Means of a Genetic
               Algorithm.},
  booktitle = {IDEAL},
  year      = {2006},
  pages     = {322--329}
}

@inproceedings{martinez06building,
  author = {Gonzalo Mart\'{\i}nez-Mu\~noz and
            Aitor S\'anchez-Mart\'{\i}nez and
            Daniel Hern\'andez-Lobato and
            Alberto Su\'arez}
  title = {Building Ensembles of Neural Networks with Class-Switching},
  booktitle = {Artificial Neural Networks - ICANN 2006},
  pages = {178--187},
  year  = {2006}
}

@inproceedings{martinez06pruning,
  author = {Gonzalo Mart\'{\i}nez-Mu\~{n}oz and Alberto Su\'{a}rez},
  title = {Pruning in ordered bagging ensembles},
  booktitle = {ICML '06: Proceedings of the 23rd international conference on Machine learning},
  year = {2006},
  isbn = {1-59593-383-2},
  pages = {609--616},
  location = {Pittsburgh, Pennsylvania},
  doi = {http://doi.acm.org/10.1145/1143844.1143921},
  publisher = {ACM Press},
  address = {New York, NY, USA},
  abstract = {We present a novel ensemble pruning method based on reordering
    the classifiers obtained from bagging and then selecting a subset for
    aggregation. Ordering the classifiers generated in bagging makes it possible
    to build subensembles of increasing size by including first those classifiers
    that are expected to perform best when aggregated. Ensemble pruning is achieved
    by halting the aggregation process before all the classifiers generated are
    included into the ensemble. Pruned subensembles containing between 15\% and 30\% of
    the initial pool of classifiers, besides being smaller, improve the generalization
    performance of the full bagging ensemble in the classification problems investigated.}
}

@inproceedings{hernandez06pruning,
  title = {Pruning in Ordered Regression Bagging Ensembles},
  author = {Hern\'andez-Lobato, D.; Mart\'{\i}nez-Mu\~noz, G.; Su\'arez, A.},
  booktitle = {Neural Networks, 2006. IJCNN '06. International Joint Conference on},
  year = {2006},
  pages = {1266--1273},
  abstract = {An efficient procedure for pruning regression ensembles is introduced.
    Starting from a bagging ensemble, pruning proceeds by ordering the regressors in
    the original ensemble and then selecting a subset for aggregation. Ensembles of
    increasing size are built by including first the regressors that perform best
    when aggregated. This strategy gives an approximate solution to the problem of
    extracting from the original ensemble the minimum error subensemble, which we
    prove to be NP-hard. Experiments show that pruned ensembles with only 20\% of the
    initial regressors achieve better generalization accuracies than the complete
    bagging ensembles. The performance of pruned ensembles is analyzed by means of
    the bias-variance decomposition of the error.}
}


@inproceedings{martinez04aggregation,
  author = {Gonzalo Mart{\'{\i}}nez-Mu{\~n}oz and Alberto Su{\'a}rez},
  title = {Aggregation Ordering in Bagging},
  booktitle = {Proc. of the {IASTED} International Conference on Artificial Intelligence
    and Applications},
  year = {2004},
  pages = {258--263},
  publisher = {Acta Press}
}

@inproceedings{Dzeroski02Combining,
  author = {Sa{\v{s}}o D{\v{z}}eroski and Bernard {\v{Z}}enko},
  title = {Is Combining Classifiers Better than Selecting the Best One},
  booktitle = {ICML '02: Proceedings of the Nineteenth International Conference on Machine Learning},
  year = {2002},
  isbn = {1-55860-873-7},
  pages = {123--130},
  publisher = {Morgan Kaufmann},
  address = {San Francisco, CA, USA},
  url = {http://www-ai.ijs.si/SasoDzeroski/files/2002_DZ_CombiningClassifiersBetter.pdf},
}

@inproceedings{Zenko02Stacking,
  author = {Bernard {\v{Z}}enko and Sa{\v{s}}o D{\v{z}}eroski},
  title = {{Stacking with an Extended Set of Meta-level Attributes and MLR}},
  booktitle = {ECML '02: Proceedings of the 13th European Conference on Machine Learning},
  year = {2002},
  isbn = {3-540-44036-4},
  pages = {493--504},
  publisher = {Springer},
  address = {Berlin, Germany},
  url = {http://www.springerlink.com/content/qnr9tdtae9n27g3v/},
}

@article{Dzeroski04Combining,
  author = {Sa{\v{s}}o D{\v{z}}eroski and Bernard {\v{Z}}enko},
  title = {{Is Combining Classifiers with Stacking Better than Selecting the Best One?}},
  journal = {Machine Learning},
  volume = {54},
  number = {3},
  year = {2004},
  issn = {0885-6125},
  pages = {255--273},
  doi = {http://dx.doi.org/10.1023/B:MACH.0000015881.36452.6e},
  publisher = {Kluwer},
  address = {Hingham, MA, USA},
}



@article{rodriguez2006rfn,
  title={{Rotation forest: A new classifier ensemble method}},
  author={RODRIGUEZ, J.J. and KUNCHEVA, L.I. and ALONSO, C.J.},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={28},
  number={10},
  pages={1619--1630},
  year={2006},
  publisher={Institute of Electrical and Electronics Engineers}
}

@article{windeatt2006ada,
  title={{Accuracy/Diversity and Ensemble MLP Classifier Design}},
  author={Windeatt, T.},
  journal={Neural Networks, IEEE Transactions on},
  volume={17},
  number={5},
  pages={1194--1211},
  year={2006}
}

@article{ko2006nof,
  title={{A New Objective Function for Ensemble Selection in Random Subspaces}},
  author={Ko, A.H.R. and Sabourin, R. and de Souza Britto Jr, A.},
  journal={Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)-Volume 04},
  pages={185--188},
  year={2006},
  publisher={IEEE Computer Society Washington, DC, USA}
}

@article{tang2006adm,
  title={{An analysis of diversity measures}},
  author={Tang, E.K. and Suganthan, P.N. and Yao, X.},
  journal={Machine Learning},
  volume={65},
  number={1},
  pages={247--271},
  year={2006},
  publisher={Springer}
}

@article{canuto2006uwd,
  title={{Using weighted dynamic classifier selection methods in ensembles with different levels of diversity}},
  author={Canuto, A.M.P. and Fagundes, D. and Abreu, M.C.C. and Junior, J.C.X.},
  journal={International Journal of Hybrid Intelligent Systems},
  volume={3},
  number={3},
  pages={147--158},
  year={2006},
  publisher={IOS Press}
}

@article{kotsiantis2006lah,
  title={{Local averaging of heterogeneous regression models}},
  author={Kotsiantis, SB},
  journal={International Journal of Hybrid Intelligent Systems},
  volume={3},
  number={2},
  pages={99--107},
  year={2006},
  publisher={IOS Press}
}

@article{zhang2006epv,
  title={{Ensemble Pruning Via Semi-definite Programming}},
  author={Zhang, Y. and Burer, S. and Street, W.N.},
  journal={Journal of Machine Learning Research},
  volume={7},
  pages={1315--1338},
  year={2006}
}

@article{prior2006ptu,
  title={{Parameter Tuning using the Out-of-Bootstrap Generalisation Error Estimate for Stochastic Discrimination and Random Forests}},
  author={Prior, M. and Windeatt, T.},
  journal={Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)-Volume 02},
  pages={498--501},
  year={2006},
  publisher={IEEE Computer Society Washington, DC, USA}
}

@article{lefaucheur2006rme,
  title={{Robust Multiclass Ensemble Classifiers via Symmetric Functions}},
  author={Lefaucheur, P. and Nock, R.},
  journal={Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)-Volume 04},
  pages={136--139},
  year={2006},
  publisher={IEEE Computer Society Washington, DC, USA}
}

@article{ko2006eec,
  title={{Evolving ensemble of classifiers in random subspace}},
  author={Ko, A.H.R. and Sabourin, R. and de Souza Britto Jr, A.},
  journal={Proceedings of the 8th annual conference on Genetic and evolutionary computation},
  pages={1473--1480},
  year={2006},
  publisher={ACM Press New York, NY, USA}
}

@article{kim2006oec,
  title={{Optimal ensemble construction via meta-evolutionary ensembles}},
  author={Kim, Y.S. and Street, W.N. and Menczer, F.},
  journal={Expert Systems With Applications},
  volume={30},
  number={4},
  pages={705--714},
  year={2006},
  publisher={Elsevier}
}

@article{karmaker2006bar,
  title={{A boosting approach to remove class label noise}},
  author={Karmaker, A. and Kwek, S.},
  journal={International Journal of Hybrid Intelligent Systems},
  volume={3},
  number={3},
  pages={169--177},
  year={2006},
  publisher={IOS Press}
}

@article{chandra2006elu,
  title={{Ensemble Learning Using Multi-Objective Evolutionary Algorithms}},
  author={Chandra, A. and Yao, X.},
  journal={Journal of Mathematical Modelling and Algorithms},
  volume={5},
  number={4},
  pages={417--445},
  year={2006},
  publisher={Springer}
}

@article{hadjitodorov2006mdb,
  title={{Moderate diversity for better cluster ensembles}},
  author={Hadjitodorov, S.T. and Kuncheva, L.I. and Todorova, L.P.},
  journal={Information Fusion},
  volume={7},
  number={3},
  pages={264--275},
  year={2006},
  publisher={Elsevier}
}

@ARTICLE{atikorale03hong,
  author       = {A. S. Atukorale, T. Downs and P. N. Suganthan},
  title        = {Boosting HONG Networks},
  journal      = {Neurocomputing},
  year         = 2003,
  volume       = 51,
  pages        = {75-86},
  month        = {April},
}

@INPROCEEDINGS{ahn01speciated,
  author       = {Joon-Hyun Ahn and Sung-Bae Cho},
  title        = {Speciated Neural Networks Evolved with Fitness
                  Sharing Technique},
  booktitle    = {Proceedings of the Congress on Evolutionary
                  Computation},
  address      = {Seoul, Korea},
  month        = {May 27-30},
  pages        = {390--396},
  year         = 2001,
}

@ARTICLE{anthony04lists,
  author       = {Martin Anthony},
  title        = {Generalisation Error Bounds for Threshold Decision
                  Lists},
  journal      = {Journal of {M}achine {L}earning {R}esearch},
  year         = 2004,
  volume       = 5,
  pages        = {189--217},
  abstract     = {In this paper we consider the generalization
                  accuracy of classification methods based on the
                  iterative use of linear classifiers. The resulting
                  classifiers, which we call threshold decision lists
                  act as follows. Some points of the data set to be
                  classified are given a particular classification
                  according to a linear threshold function (or
                  hyperplane). These are then removed from
                  consideration, and the procedure is iterated until
                  all points are classified. Geometrically, we can
                  imagine that at each stage, points of the same
                  classification are successively chopped off from the
                  data set by a hyperplane. We analyse theoretically
                  the generalization properties of data classification
                  techniques that are based on the use of threshold
                  decision lists and on the special subclass of
                  multilevel threshold functions. We present bounds on
                  the generalization error in a standard probabilistic
                  learning framework. The primary focus in this paper
                  is on obtaining generalization error bounds that
                  depend on the levels of separation---or
                  margins---achieved by the successive linear
                  classifiers. We also improve and extend previously
                  published theoretical bounds on the generalization
                  ability of perceptron decision trees.},
}

@INPROCEEDINGS{bahler00,
  author       = {Dennis Bahler and Laura Navarro},
  title        = {Methods for Combining Heterogeneous Sets of
                  Classifiers},
  booktitle    = {17th Natl. Conf. on Artificial Intelligence (AAAI),
                  Workshop on New Research Problems for Machine
                  Learning},
  year         = 2000,
}

@article{banfield05thinning,
  author    = {Robert E. Banfield and
               Lawrence O. Hall and
               Kevin W. Bowyer and
               W. Philip Kegelmeyer},
  title     = {Ensemble diversity measures and their application to thinning.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {49-62}
}

@ARTICLE{baldridge06active,
  title        = {{Active Learning and Logarithmic Opinion Pools for HPSG Parse Selection}},
  author       = {Baldridge, J. and Osborne, M.},
  journal      = {Natural Language Engineering (in press)},
}

@ARTICLE{baram04choice,
  author       = {Y. Baram and R. El-Yaniv and K. Luz},
  title        = {Online Choice of Active Learning Algorithms},
  journal      = {Journal of Machine Learning Research},
  year         = 2004,
  volume       = 5,
  pages        = {255--291},
  month        = {March},
}

@ARTICLE{brameier01evolving,
  author       = {Markus Brameier and Wolfgang Banzhaf},
  title        = {Evolving Teams of Predictors with Linear Genetic
                  Programming},
  journal      = {Genetic Programming and Evolvable Machines},
  volume       = 2,
  number       = 4,
  pages        = {381--407},
  year         = 2001,
}

@TECHREPORT{breiman00infinite,
  author       = {L.~Breiman},
  title        = {Some Infinite Theory for Predictor Ensembles},
  institution  = {Statistics Department, UC Berkeley},
  year         = 2000,
  number       = 577,
  month        = {August},
  pdf          = {http://www.boosting.org/papers/some_theory2001.pdf},
}

@PHDTHESIS{brown04thesis,
  author       = {G. Brown},
  title        = {Diversity in Neural Network Ensembles},
  school       = {School of Computer Science, University of
                  Birmingham},
  year         = 2004,
}

@INPROCEEDINGS{brownwyatt03ambiguity,
  author       = {G. Brown and J.L. Wyatt},
  title        = {The Use of the Ambiguity Decomposition in Neural
                  Network Ensemble Learning Methods},
  booktitle    = {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year         = 2003,
  editor       = {Tom Fawcett and Nina Mishra},
  month        = {August},
  address      = {Washington DC, USA},
}

@ARTICLE{brown04survey,
  author       = {G. Brown and J.L. Wyatt and R. Harris and X. Yao},
  title        = {Diversity Creation Methods: A Survey and
                  Categorisation},
  journal      = {Journal of Information Fusion},
  volume       = 6,
  number       = 1,
  month        = {March},
  pages        = {5--20},
  year         = 2005,
}

@ARTICLE{brownwyatt05jmlr,
  title        = {Managing Diversity in Regression Ensembles},
  author       = {G. Brown and J.L. Wyatt and P. Tino},
  journal      = {Journal of Machine Learning Research},
  volume       = 6,
  pages        = {1621--1650},
  year         = 2005,
}

@INPROCEEDINGS{brownyao01:ukci,
  author       = {Gavin Brown and Xin Yao},
  title        = {On The {E}ffectiveness of {N}egative {C}orrelation
                  {L}earning},
  booktitle    = {Proceedings of First UK Workshop on Computational
                  Intelligence},
  note         = {Edinburgh, Scotland},
  pages        = {57-62},
  year         = 2001,
}

@INPROCEEDINGS{brownyao02exploiting,
  author       = {Gavin Brown and Xin Yao and Jeremy Wyatt and Heiko
                  Wersing and Bernhard Sendhoff},
  title        = {Exploiting Ensemble Diversity for Automatic Feature
                  Extraction},
  booktitle    = {Proc. of the 9th International Conference on Neural
                  Information Processing (ICONIP'02)},
  pages        = {1786-1790},
  year         = 2002,
  month        = {November},
}

@TECHREPORT{buhlmann00explaining,
  author       = {Peter Buhlmann and Bin Yu},
  title        = {Explaining Bagging},
  institution  = {ETH Zurich, Seminar Fur Statistik},
  month        = {May},
  year         = 2000,
  number       = 92,
  url          = {ftp://ftp.stat.math.ethz.ch/Research-Reports/92.html},
}

@INPROCEEDINGS{caruana06icml,
  author       = {Rich Caruana and Alexandru Niculescu-Mizil},
  title        = {An Empirical Comparison of Supervised Learning Algorithms},
  booktitle    = {International Conference on Machine Learning},
  year         = 2006,
  organization = {Department of Computer Science, Cornell University},
  annote       = {Large empirical comparison of ML methods. Boosting and Bagging come top.},
}

@INPROCEEDINGS{chawla01small,
  author       = {Nitesh Chawla and Thomas Moore and Kevin Bowyer and
                  Lawrence Hall and Clayton Springer and Philip
                  Kegelmeyer},
  title        = {Bagging is a Small-Data-Set Phenomenon},
  booktitle    = {International Conference on Computer Vision and
                  Pattern Recognition (CVPR)},
  year         = 2001,
}

@INPROCEEDINGS{cheung03radial,
  author       = {Y.M. Cheung and R.B. Huang},
  title        = {An Advance on Divide-and-Conquer Based Radial Basis
                  Function Networks},
  booktitle    = {Proceedings of Fourth International Conference on
                  Intelligent Data Engineering and Automated Learning},
  address      = {Hong Kong},
  month        = {March},
  year         = 2003,
}

@ARTICLE{cohenIntrator01,
  author       = {S. Cohen and N. Intrator},
  year         = 2002,
  title        = {A hybrid projection based and radial basis function
                  architecture: Initial values and global
                  optimization},
  journal      = {Pattern Anal. Appl. (Special issue on Fusion of
                  Multiple Classifiers)},
  volume       = 5,
  number       = 2,
  pages        = {113--120},
}

@ARTICLE{cohenIntratorFusion01,
  author       = {S. Cohen and N. Intrator},
  title        = {Automatic model selection in a hybrid
                  perceptron/radial network},
  journal      = {Information Fusion},
  volume       = 3,
  number       = 4,
  pages        = {259--266},
  year         = 2002,
}

@INPROCEEDINGS{cunningham00diversity,
  author       = {Padraig Cunningham and John Carney},
  title        = {Diversity versus Quality in Classification Ensembles
                  Based on Feature Selection},
  booktitle    = {LNCS - European Conference on Machine Learning},
  volume       = 1810,
  publisher    = {Springer, Berlin},
  pages        = {109--116},
  year         = 2000,
}

@INPROCEEDINGS{davidson04stable,
  author       = {Ian Davidson},
  title        = {An Ensemble Technique for Stable Learners with
                  Performance Bounds},
  booktitle    = {Proceedings of the Nineteenth National Conference on
                  Artificial Intelligence},
  year         = 2004,
  publisher    = {AAAI Press},
  pages        = {300-335},
}

@ARTICLE{dietterich00experimental,
  author       = {Thomas G. Dietterich},
  title        = {An Experimental Comparison of Three Methods for
                  Constructing Ensembles of Decision Trees: Bagging,
                  Boosting, and Randomization},
  journal      = {Machine Learning},
  volume       = 40,
  number       = 2,
  pages        = {139-157},
  year         = 2000,
}

@INPROCEEDINGS{domingos00unified,
  author       = {Pedro Domingos},
  title        = {A Unified Bias-Variance Decomposition and its
                  Applications},
  booktitle    = {Proc. 17th International Conf. on Machine Learning},
  publisher    = {Morgan Kaufmann, San Francisco, CA},
  pages        = {231--238},
  year         = 2000,
  url          = {http://citeseer.nj.nec.com/domingos00unified.html},
}

@INPROCEEDINGS{domingos00unified-AAAI,
  author       = {Pedro Domingos},
  title        = {A Unified Bias-Variance Decomposition for Zero-One
                  and Squared Loss},
  booktitle    = {{AAAI}/{IAAI}},
  pages        = {564-569},
  year         = 2000,
}

@TECHREPORT{drug06formal,
  author       = {Jan Drugowitsch and Alwyn M Barry},
  title        = {A Formal Framework and Extensions for Function Approximation in Learning Classifier Systems},
  institution  = {University of Bath},
  year         = 2006,
  number       = {CSBU2006-01},
}

@INPROCEEDINGS{duchitert03undemocratic,
  author       = {Wlodzislaw Duch and Lukasz Itert},
  title        = {Committees of Undemocratic Competent Models},
  booktitle    = {International Conference on Artificial Neural
                  Networks (ICANN) and International Conference on
                  Neural Information Processing (ICONIP)},
  year         = 2003,
  address      = {Istanbul, Turkey},
  month        = June,
  pages        = {33--36},
  url          = {http://www.phys.uni.torun.pl/publications/kmk/03-C-Ensambles-s.html},
}

@BOOK{duda01book,
  author       = {Richard Duda and Peter Hart and David Stork},
  title        = {Pattern Classification},
  publisher    = {John Wiley and Sons},
  year         = 2001,
  note         = {0-471-05669-3},
}

@ARTICLE{fern03onlinebranch,
  author       = {Alan Fern and
                  Robert Givan},
  title        = {Online Ensemble Learning: An Empirical Study.},
  journal      = {Machine Learning},
  volume       = 53,
  number       = {1-2},
  year         = 2003,
  pages        = {71-109},
  abstract     = {Applied online bagging and boosting to branch prediction},
}

@ARTICLE{fern2003rph,
  title        = {{Random projection for high dimensional data clustering: A cluster ensemble approach}},
  author       = {X. Fern and C. Brodley},
  journal      = {Proceedings of the Twentieth International Conference on Machine Learning},
  pages        = {186--193},
  year         = 2003,
}

@ARTICLE{fleuret04cmi,
  title        = {{Fast Binary Feature Selection with Conditional Mutual Information}},
  author       = {Fleuret, F.},
  journal      = {The Journal of Machine Learning Research},
  volume       = 5,
  pages        = {1531-1555},
  year         = 2004,
  publisher    = {MIT Press Cambridge, MA, USA},
}

@INPROCEEDINGS{freund01averaging,
  title        = {Why Averaging Classifiers can Protect Against
                  Overfitting},
  author       = {Yoav Freund and Yishay Mansour and Robert Schapire},
  booktitle    = {Eighth International Workshop on Artificial
                  Intelligence and Statistics},
  year         = 2001,
}

@TECHREPORT{friedman03isle,
  author       = {J.H. Friedman and B. Popescu},
  title        = {Importance Sampling Learning Ensembles},
  institution  = {Stanford University},
  year         = 2003,
  month        = {September},
  url          = {http://www-stat.stanford.edu/\~{}jhf/ftp/isle.pdf},
}

@ARTICLE{gencay01pricing,
  author       = {R. Gencay and Min Qi},
  title        = {Pricing and hedging derivative securities with
                  neural networks: Bayesian regularization, early
                  stopping, and bagging},
  journal      = {IEEE Transactions on Neural Networks},
  year         = 2001,
  volume       = 12,
  issue        = 4,
  month        = {July},
  pages        = {726--734},
}

@INPROCEEDINGS{grandvalet01bagging,
  author       = {Y.~Grandvalet},
  title        = {Bagging can stabilize without reducing variance},
  booktitle    = {ICANN'01},
  series       = {Lecture Notes in Computer Science},
  publisher    = {Springer},
  year         = 2001,
}

@INPROCEEDINGS{navone2000,
  author       = {H.D.Navone and P.F.Verdes and P.M.Granitto and
                  H.A.Ceccatto},
  title        = {A New Algorithm for Selecting Diverse Members of a
                  Neural Network Ensemble},
  booktitle    = {6th International Congress on Information
                  Engineering},
  year         = 2000,
  note         = {Buenos Aires, Argentina},
}

@PHDTHESIS{hansen00thesis,
  author       = {J.V. Hansen},
  title        = {Combining Predictors: Meta Machine Learning Methods
                  and Bias/Variance and Ambiguity Decompositions},
  school       = {Aarhus Universitet, Datalogisk Institut},
  year         = 2000,
}

@ARTICLE{hayashi02medical,
  author       = {Y. Hayashi and R. Setiono},
  title        = {Combining neural network predictions for medical
                  diagnosis},
  journal      = {Computers in Biology and Medicine},
  year         = 2002,
  volume       = 32,
  number       = 4,
  pages        = {237--246},
}

@ARTICLE{islam03constructive,
  author       = {Md. M. Islam and X. Yao and K. Murase},
  number       = 4,
  year         = 2003 ,
  journal      = {IEEE Transactions on Neural Networks},
  title        = {A constructive algorithm for training cooperative
                  neural network ensembles},
  month        = {July},
  pages        = {820--834},
  volume       = 14,
}

@ARTICLE{itqon01,
  author       = {Itqon and Shun'ichi Kaneko and Satoru Igarashi},
  title        = {Combining Multiple k-Nearest Neighbor Classifiers
                  Using Feature Combinations},
  journal      = {Journal of IECI (Indonesian Society on Electrical
                  Electronics, Communication and Information)},
  year         = 2000,
  volume       = 2,
  number       = 3,
  pages        = {23--31},
}

@INPROCEEDINGS{wuzhouchen02:ensembling,
  author       = {J.X.Wu and Z.H.Zhou and Z.Q.Chen},
  title        = "Ensemble of {GA} based selective neural network,
                  ensembles" ,
  booktitle    = {8th International Conference on Neural Information
                  Processing (ICONIP)},
  volume       = 3,
  pages        = {1477-1482},
  year         = 2002,
}

@INBOOK{jaakkola00variational,
  author       = {Jaakkola, T.},
  editor       = {D. Saad and M. Opper},
  title        = {Advanced Mean Field methods - Theory and Practice},
  chapter      = {Tutorial on Variational Approximation Methods},
  publisher    = {MIT Press},
  year         = 2000,
}

@ARTICLE{james03bv,
  author       = {Gareth James},
  year         = 2003,
  title        = {Variance and Bias for General Loss Functions},
  journal      = {Machine Learning},
  volume       = 51,
  pages        = {115--135},
}

@INPROCEEDINGS{jin03regularizer,
  author       = {Rong Jin and Yan Liu and Luo Si and Jaime Carbonell
                  and Alexander Hauptmann},
  title        = {A New Boosting Algorithm using Input-Dependent
                  Regularizer},
  booktitle    = {20th International Conference on Machine Learning},
  year         = 2003,
}

@INPROCEEDINGS{joshi01evaluating,
  author       = {Mahesh V. Joshi and Vipin Kumar and Ramesh
                  C. Agarwal},
  title        = {Evaluating Boosting Algorithms to Classify Rare
                  Classes: Comparison and Improvements},
  booktitle    = {{ICDM}},
  pages        = {257-264},
  year         = 2001,
  url          = {http://citeseer.nj.nec.com/joshi01evaluating.html},
}

@INPROCEEDINGS{kanamori02mixture,
  author       = {Takafumi Kanamori},
  title        = {A New Sequential Algorithm for Regression Problems
                  by using Mixture Distribution},
  booktitle    = {Int. Conf. Artif. Neur. Netw. ICANN},
  pages        = {535--540},
  year         = 2002,
  publisher    = {Springer},
  url          = {http://citeseer.nj.nec.com/573689.html},
}

@INPROCEEDINGS{khareyao02,
  author       = {Vineet Khare and Xin Yao},
  title        = {Artificial Speciation of Neural Network Ensembles},
  booktitle    = {Proc. of the 2002 UK Workshop on Computational
                  Intelligence (UKCI'02)},
  pages        = {96--103},
  year         = 2002,
  editor       = {J.A.Bullinaria},
  month        = {September},
  organization = {University of Birmingham, UK},
}

@ARTICLE{kleinberg00algorithmic,
  author       = {Eugene M. Kleinberg},
  title        = {On the Algorithmic Implementation of Stochastic
                  Discrimination},
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  volume       = 22,
  number       = 5,
  pages        = {473-490},
  year         = 2000,
}

@INPROCEEDINGS{kuncheva00independence,
  author       = {L. Kuncheva and C. Whitaker and C. Shipp and
                  R. Duin},
  title        = {Is independence good for combining classifiers},
  booktitle    = {Proceedings of the 15th International Conference on
                  Pattern Recognition, Barcelona, Spain},
  year         = 2000,
  pages        = {168-171},
}

@ARTICLE{kuncheva03limits,
  author       = {L. Kuncheva and C. Whitaker and C. Shipp and
                  R. Duin},
  title        = {Limits on the majority vote accuracy in classifier
                  fusion},
  journal      = {Pattern {A}nalysis and {A}pplications},
  year         = 2003,
  month        = {April},
  volume       = 6,
  number       = 1,
  pages        = {22--31},
}

@ARTICLE{kuncheva02sixstrategies,
  author       = {Kuncheva, L.I.},
  title        = {A Theoretical Study on Six Classifier Fusion
                  Strategies},
  journal      = {PAMI},
  volume       = 24,
  year         = 2002,
  number       = 2,
  month        = {February},
  pages        = {281-286},
}

@ARTICLE{kuncheva02switching,
  author       = {L.I. Kuncheva},
  title        = {Switching between selection and fusion in combining
                  classifiers: An experiment},
  journal      = {IEEE Transactions On Systems Man And Cybernetics},
  volume       = 32,
  number       = 2,
  pages        = {146--156},
  year         = 2002,
}

@INPROCEEDINGS{kuncheva03elusive,
  author       = {Kuncheva, L.I.},
  title        = {That {E}lusive {D}iversity in {C}lassifier
                  {E}nsembles},
  booktitle    = {First Iberian Conference on Pattern Recognition and
                  Image Analysis (IbPRIA), available as LNCS volume
                  2652},
  pages        = {1126--1138},
  year         = 2003,
}

@ARTICLE{kuncheva02generating,
  author       = {L.I. Kuncheva and R.K. Kountchev},
  title        = {Generating Classifier Outputs of Fixed Accuracy and
                  Diversity},
  journal      = {Pattern Recognition Letters},
  year         = 2002,
  number       = 23,
  pages        = {593-600},
}

@ARTICLE{kunchevawhitaker03,
  author       = {L.I. Kuncheva and C. Whitaker},
  title        = {Measures of Diversity in Classifier Ensembles},
  journal      = {Machine Learning},
  year         = 2003,
  number       = 51,
  pages        = {181--207},
}

@ARTICLE{kuncheva2004udc,
  title        = {{Using diversity in cluster ensembles}},
  author       = {Kuncheva, LI and Hadjitodorov, ST},
  journal      = {Systems, Man and Cybernetics, 2004 IEEE International Conference on},
  volume       = 2,
  year         = 2004,
}

@ARTICLE{hadjitodorov2005mdb,
  title        = {{Moderate Diversity for Better Cluster Ensembles}},
  author       = {Hadjitodorov, S.T. and Kuncheva, L.I. and Todorova, L.P.},
  journal      = {Information Fusion},
  volume       = 7,
  number       = 3,
  year         = 2006,
}

@INCOLLECTION{kuncheva01:tenmeasures,
  author       = {L.I. Kuncheva and C.J. Whitaker},
  title        = {Ten Measures of Diversity in Classifier Ensembles:
                  Limits for Two Classifiers},
  booktitle    = {IEE Workshop on Intelligent Sensor Processing},
  publisher    = {IEE},
  year         = 2001,
  month        = {February},
}

@BOOK{kunchevabook,
  author       = {Ludmila Kuncheva},
  title        = {Combining Pattern Classifiers: Methods and
                  Algorithms},
  publisher    = {Wiley Press},
  year         = 2004,
  note         = {ISBN 0-471-21078-1},
}

@TECHREPORT{kutin01stability,
  author       = {Samuel Kutin and Partha Niyogi},
  title        = {The Interaction of Stability and Weakness in
                  AdaBoost},
  institution  = {The University of Chicago},
  year         = 2001,
  number       = {TR-2001-30},
  url          = {http://www.cs.uchicago.edu/research/publications/techreports/TR-2001-30},
}

@TECHREPORT{langdon:2001:edf,
  author       = {W. B. Langdon},
  title        = {Evolutionary Data Fusion},
  institution  = {University College, London},
  year         = 2001,
  number       = {RN/01/19},
  address      = {UK},
  month        = {3 April},
  keywords     = {genetic algorithms, genetic programming, ROC},
  url          = {http://www.cs.ucl.ac.uk/staff/W.Langdon/datafusion.html},
  url          = {http://www.cs.ucl.ac.uk/staff/W.Langdon/roc},
  size         = {12 pages},
  notes        = {Distributed at 25 April 2001 Faraday meeting
                  http://www.npl.co.uk/intersect/ },
}

@INPROCEEDINGS{langdon:2001:wsc6,
  author       = {W. B. Langdon and S. J. Barrett and B. F. Buxton},
  title        = {Genetic Programming for Combining Neural Networks
                  for Drug Discovery},
  booktitle    = {Soft Computing and Industry Recent Applications},
  year         = 2001,
  editor       = {Rajkumar Roy and Mario K\"oppen and Seppo Ovaska and
                  Takeshi Furuhashi and Frank Hoffmann},
  pages        = {597--608},
  month        = {10--24 September},
  publisher    = {Springer-Verlag},
  note         = {Published 2002},
  keywords     = {genetic algorithms, genetic programming, data
                  fusion, data mining, knowledge discovery, Receiver
                  Operating Characteristics, ensemble of classifiers,
                  size fair crossover},
  isbn         = {1-85233-539-4},
  url          = {http://www.springer.de/cgi/svcat/search_book.pl?isbn=1-85233-539-4},
}

@INPROCEEDINGS{langdon03gp
  title        = {Comparison of AdaBoost and Genetic Programming for
                  combining Neural Networks for Drug Discovery},
  author       = {W. B. Langdon and S. J. Barrett and B. F. Buxton},
  booktitle    = {Applications of Evolutionary Computing,
                  EvoWorkshops2003: Evo{BIO}, Evo{COP}, Evo{IASP},
                  Evo{MUSART}, Evo{ROB}, Evo{STIM}},
  editor       = {G\"unther R.~Raidl and Stefano Cagnoni and Juan
                  Jes\'us Romero Cardalda and David W.~Corne and Jens
                  Gottlieb and Agn\`es Guillot and Emma Hart and Colin
                  G.~Johnson and Elena Marchiori and Jean-Arcady Meyer
                  and Martin Middendorf},
  volume       = 2611,
  series       = {LNCS},
  pages        = {87--98},
  address      = {University of Essex, UK},
  publisher    = {Springer-Verlag},
  publisher_address ={Berlin},
  month        = {14-16 April},
  organisation = {EvoNet},
  year         = 2003,
  keywords     = {genetic algorithms, genetic programming, adaboost,
                  drug design, Receiver Operating Characteristics
                  (ROC), ensemble of classifiers, data fusion,
                  artificial neural networks, clementine, high through
                  put screening (HTS)},
  size         = {12 pages},
  notes        = {EvoWorkshops2003},
}

@INPROCEEDINGS{langdon:2001:gROC,
  title        = {Genetic Programming for Combining Classifiers},
  author       = {W. B. Langdon and B. F. Buxton},
  pages        = {66--73},
  year         = 2001,
  publisher    = {Morgan Kaufmann},
  booktitle    = {Proceedings of the Genetic and Evolutionary
                  Computation Conference (GECCO-2001)},
  editor       = {Lee Spector and Erik D. Goodman and Annie Wu and
                  W.B. Langdon and Hans-Michael Voigt and Mitsuo Gen
                  and Sandip Sen and Marco Dorigo and Shahram Pezeshk
                  and Max H. Garzon and Edmund Burke},
  address      = {San Francisco, California, USA},
  publisher_address ={San Francisco, CA 94104, USA},
  month        = {7-11 July},
  keywords     = {genetic algorithms, genetic programming, data
                  fusion, data mining, knowledge discovery, Receiver
                  Operating Characteristics, ensemble of classifiers,
                  size fair crossover},
  isbn         = {1-55860-774-9},
  url          = {ftp://cs.ucl.ac.uk/genetic/papers/WBL_gecco2001_roc.ps.gz},
  url          = {ftp://cs.ucl.ac.uk/genetic/papers/WBL_gecco2001_roc.pdf},
  size         = {8 pages},
  abstract     = {Genetic programming (GP) can automatically fuse
                  given classifiers to produce a combined classifier
                  whose Receiver Operating Characteristics (ROC) are
                  better than scott:1998:BMVC ``Maximum Realisable
                  Receiver Operating Characteristics''
                  (MRROC). I.e. better than their convex hull. This is
                  demonstrated on artificial, medical and satellite
                  image processing bench marks.},
  notes        = {GECCO-2001 A joint meeting of the tenth
                  International Conference on Genetic Algorithms
                  (ICGA-2001) and the sixth Annual Genetic Programming
                  Conference (GP-2001) Part of spector:2001:GECCO},
}

@MISC{langdon:2002:kdmdd,
  author       = {W. B. Langdon and B. F. Buxton and S. J. Barrett},
  title        = {Combining Machine Learning techniques to Predict
                  Compounds' Cytochrome P450 High Throughput Screening
                  Inhibition},
  howpublished = {Knowledge Discovery meets Drug Discovery},
  year         = 2002,
  month        = {23 October},
  note         = {poster},
  keywords     = {genetic algorithms, genetic programming},
  url          = {ftp://cs.ucl.ac.uk/genetic/papers/wbl_kdmdd2002.pdf},
  notes,
                  {http://www.kdnet.org/workshop_overview1_bioinfoLeuven02.htm},
}

@INPROCEEDINGS{langdon02gp,
  year         = 2002,
  title        = {A Hybrid Genetic Programming Neural Network
                  Classifier for Use in Drug Discovery},
  institution  = {Department of Computer Science -- University College
                  London -- UK},
  booktitle    = {Soft Computing Systems - Design, Management and
                  Applications},
  author       = {William B. Langdon},
  abstract     = {We have shown genetic programming (GP) can
                  automatically fuse given classifiers of diverse
                  types to produce a hybrid classifier. Combinations
                  of neural networks, decision trees and Bayes
                  classifier shave been formed. On a range of
                  benchmarks the evolved multiple classifier system is
                  better than all of its components. Indeed its
                  Receiver Operating Characteristics (ROC) are better
                  than [Scott et al., 1998]s "Maximum Realisable
                  Receiver Operating Characteristics" MRROC (convex
                  hull) An important component in the drug discovery
                  is testing potential drugs for activity with P450
                  cell membrane molecules. Our technique has been used
                  in a blind trial where artificial neural networks
                  are trained by Clementine on P450 pharmaceutical
                  data. Using just the trained networks, GP
                  automatically evolves a composite classifier. Recent
                  experiments with boosting the networks will be
                  compared with genetic programming.},
}

@INPROCEEDINGS{langdon:2002:EuroGP,
  title        = {Combining Decision Trees and Neural Networks for
                  Drug Discovery},
  author       = {William B. Langdon and S. J. Barrett and
                  B. F. Buxton},
  booktitle    = {Genetic Programming, Proceedings of the 5th European
                  Conference, EuroGP 2002},
  pages        = {60--70},
  address      = {Kinsale, Ireland},
  publisher_address ={Berlin},
  month        = {3-5 April},
  year         = 2002,
  keywords     = {genetic algorithms, genetic programming, drug
                  design, Receiver Operating Characteristics (ROC),
                  ensemble of classifiers, data fusion, artificial
                  neural networks, clementine, decision trees C4.5,
                  high through put screening (HTS)},
  isbn         = {3-540-43378-3},
  size         = {10 pages},
  abstract     = {Genetic programming (GP) offers a generic method of
                  automatically fusing together classifiers using
                  their receiver operating characteristics (ROC) to
                  yield superior ensembles. We combine decision trees
                  (C4.5) and artificial neural networks (ANN) on a
                  difficult pharmaceutical data mining (KDD) drug
                  discovery application. Specifically predicting
                  inhibition of a P450 enzyme. Training data came from
                  high throughput screening (HTS) runs. The evolved
                  model may be used to predict behaviour of virtual
                  (i.e. yet to be manufactured) chemicals. Measures to
                  reduce over fitting are also described. },
  notes        = {EuroGP'2002, part of lutton:2002:GP},
}

@INPROCEEDINGS{langdon:2001:eROC,
  author       = {William B. Langdon and Bernard F. Buxton},
  title        = {Evolving Receiver Operating Characteristics for Data
                  Fusion},
  booktitle    = {Genetic Programming, Proceedings of EuroGP'2001},
  year         = 2001,
  volume       = 2038,
  pages        = {87--96},
  address      = {Lake Como, Italy},
  publisher_address ={Berlin},
  month        = {18-20 April},
  organisation = {EvoNET},
  publisher    = {Springer-Verlag},
  keywords     = {genetic algorithms, genetic programming, Data
                  Fusion, Data Mining, Knowledge Discovery, Receiver
                  Operating Characteristics, ROC, Combining
                  Classifiers},
  isbn         = {3-540-41899-7},
  url          = {http://evonet.dcs.napier.ac.uk/eurogp2001/},
  url          = {ftp://cs.ucl.ac.uk/genetic/papers/wbl_egp2001.ps.gz},
  size         = {10 pages},
  abstract     = {It has been suggested that the ``Maximum Realisable
                  Receiver Operating Characteristics'' for a
                  combination of classifiers is the convex hull of
                  their individual ROCs [Scott et al., 1998]. As
                  expected in at least some cases better ROCs can be
                  produced. We show genetic programming (GP) can
                  automatically produce a combination of classifiers
                  whose ROC is better than the convex hull of the
                  supplied classifier's ROCs.},
  notes        = {EuroGP'2001, part of miller:2001:gp},
}

@MISC{lappalainen00ensemble,
  author       = {H. Lappalainen and J. Miskin},
  title        = {Ensemble Learning},
  text         = {H. Lappalainen and J. Miskin, Ensemble Learning, in
                  M. Girolami (Ed.), Advances in Independent Component
                  Analysis, Springer, Berlin, 2000 (in press).},
  year         = 2000,
}

@ARTICLE{lee2004lob,
  title        = {{Lossless Online Bayesian Bagging}},
  author       = {Lee, H.K.H. and Clyde, M.A.},
  journal      = {Journal of Machine Learning Research},
  volume       = 5,
  pages        = {143-151},
  year         = 2004,
  publisher    = {MIT Press Cambridge, MA, USA},
}

@ARTICLE{liu00evolutionary,
  author       = {Y. Liu and X. Yao and T. Higuchi},
  title        = {Evolutionary Ensembles with Negative Correlation
                  Learning},
  journal      = {IEEE Transactions on Evolutionary Computation},
  volume       = 4,
  number       = 4,
  month        = {November},
  year         = 2000,
  url          = {http://citeseer.nj.nec.com/article/liu00evolutionary.html},
}

@INPROCEEDINGS{liuyao02decision,
  author       = {Y. Liu and X. Yao and Q. Zhao and T. Higuchi},
  title        = {An experimental comparison of neural network
                  ensemble learning methods on decision boundaries},
  booktitle    = {Proceedings of the 2002 International Joint
                  Conference on Neural Networks (IJCNN'02)},
  pages        = {221-226},
  month        = {May},
  year         = 2002,
  publisher    = {IEEE Press, Piscataway, NJ, USA},
}

@INPROCEEDINGS{liu01mutual,
  author       = {Yong Liu and Xin Yao and Qiangfu Zhao and Tetsuya
                  Higuchi},
  title        = {Evolving a Cooperative Population of Neural Networks
                  by Minimizing Mutual Information},
  booktitle    = {Proceedings of the 2001 Congress on Evolutionary
                  Computation},
  pages        = {384-389},
  year         = 2001,
  month        = {May},
  publisher    = {IEEE Press},
}

@INPROCEEDINGS{luochen02b,
  author       = {Dingsheng Luo and Ke Chen},
  title        = {On the use of statistical ensemble methods for
                  telephone-line speaker identification},
  booktitle    = {Proceedings of International Joint Conference on
                  Communications, Circuits and Systems (ICCCAS'2002)},
  year         = 2002,
  publisher    = {IEEE Press},
  address      = {Chengdu, China},
  pages        = {II904-II908},
  month        = {July},
}

@INPROCEEDINGS{luochen02a,
  author       = {Dingsheng Luo and Ke Chen},
  title        = {A comparative study of statistical ensemble methods
                  on mismatch conditions},
  booktitle    = {Proceedings of World Congress on Computational
                  Intelligence: International Joint Conference on
                  Neural Networks (WCCI 2002 and IJCNN 2002)},
  year         = 2002,
  address      = {Honolulu USA},
  publisher    = {IEEE Press},
  pages        = {59--64},
  month        = {November},
}

@ARTICLE{malzahn03approximate,
  author       = {Dorthe Malzahn and Manfred Opper},
  title        = {An Approximate Analytical Approach to Resampling
                  Averages},
  journal      = {Journal of Machine Learning Research},
  year         = 2003,
  volume       = 4,
  pages        = {1151--1173},
  month        = {December},
  url          = {http://www.jmlr.org},
}

@INBOOK{mason00functional,
  author       = {L. Mason and J. Baxter and P. L. Bartlett and
                  M. Frean},
  editor       = {A.J. Smola, P. L. Bartlett, B. Scholkopf, and
                  D. Schuurmans},
  title        = {Advances in Large Margin Classifiers : Functional
                  gradient techniques for combining hypotheses},
  publisher    = {MIT Press},
  year         = 2000,
  address      = {Cambridge, MA},
  pages        = {221--246},
}

@INPROCEEDINGS{mckay00sharing,
  author       = {Bob McKay},
  title        = {Fitness Sharing in Genetic Programming},
  pages        = {435--442},
  year         = 2000,
  publisher    = {Morgan Kaufmann},
  booktitle    = {Proceedings of the Genetic and Evolutionary
                  Computation Conference (GECCO-2000)},
  editor       = {Darrell Whitley and David Goldberg and Erick
                  Cantu-Paz and Lee Spector and Ian Parmee and
                  Hans-Georg Beyer},
  address      = {Las Vegas, Nevada, USA},
  publisher_address ="San Francisco, CA 94104, USA",
  month        = "10-12 " # jul,
  keywords     = {genetic algorithms, genetic programming},
  isbn         = {1-55860-708-0},
  notes        = {A joint meeting of the ninth International
                  Conference on Genetic Algorithms (ICGA-2000) and the
                  fifth Annual Genetic Programming Conference
                  (GP-2000) Part of whitley:2000:GECCO},
}

@INPROCEEDINGS{mckayabbass01analyzing,
  author       = {R. McKay and H. Abbass},
  title        = {Analyzing Anticorrelation in Ensemble Learning},
  booktitle    = {Proceedings of 2001 Conference on Artificial Neural
                  Networks and Expert Systems},
  year         = 2001,
  pages        = {22--27},
  address      = {Otago, New Zealand},
}

@INPROCEEDINGS{mckayabbass01rtqrt,
  author       = {Robert McKay and Hussein Abbass},
  title        = {Anticorrelation Measures in Genetic Programming},
  booktitle    = {Australasia-Japan Workshop on Intelligent and
                  Evolutionary Systems},
  pages        = {45--51},
  year         = 2001,
}

@INPROCEEDINGS{meir00localized,
  author       = {Ron Meir and Ran El-{Y}aniv and Shai Ben-{D}avid},
  title        = {Localized Boosting},
  booktitle    = {Proc. 13th Annu. Conference on Comput. Learning
                  Theory},
  publisher    = {Morgan Kaufmann, San Francisco},
  pages        = {190--199},
  year         = 2000,
  url          = {http://citeseer.nj.nec.com/511402.html},
}

@article{melville05artificial,
  author    = {Prem Melville and
               Raymond J. Mooney},
  title     = {Creating diversity in ensembles using artificial data.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {99-111}
}

@phdthesis{melville05thesis,
  author      = {Prem Melville},
  title       = {Creating Ensemble Diversity to Reduce Supervision},
  institution = {University of Texas at Austin}
  year        = {2005}
}

@INPROCEEDINGS{melville03decorate,
  author       = {Prem Melville and Ray Mooney},
  title        = {Constructing Diverse Classifier Ensembles Using
                  Artificial Training Examples},
  booktitle    = {Proceedings of the Eighteenth International Joint
                  Conference on Artificial Intelligence},
  pages        = {505--510},
  year         = 2003,
  address      = {Mexico},
  month        = {August},
}

@article{galor05assessing,
  author    = {Mordechai Gal-Or and
               Jerrold H. May and
               William E. Spangler},
  title     = {Assessing the predictive accuracy of diversity measures
               with domain-dependent, asymmetric misclassification costs.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {37-48}
}

@ARTICLE{rooney2006pes,
  title        = {{Pruning extensions to stacking}},
  author       = {Rooney, N. and Patterson, D. and Nugent, C.},
  journal      = {Intelligent Data Analysis},
  volume       = 10,
  number       = 1,
  pages        = {47-66},
  year         = 2006,
  publisher    = {IOS Press},
  abstract     = {In this paper we investigate an algorithmic extension to the technique of Stacking for regression
                  that prunes the ensemble set before application based on a consideration of the training accuracy and diversity
                  of the ensemble members. We evaluate two variants of this approach in comparison to the standard Stacking
                  algorithm, one of which is a static approach that prunes back the ensemble to the same constant size; the
                  other of which is a variable approach prunes the ensemble to an appropriate level based on measures of
                  accuracy and diversity of the ensemble members. We show that on average both techniques are robust in
                  performance to their non-pruned counterpart, while having the advantage of producing smaller and less
                  complex ensembles. In the latter respect, the static approach proved more effective, but we show that
                  the variable approach lends itself better for further optimization.},
}

@ARTICLE{dettling2003btc,
  title        = {{Boosting for tumor classification with gene expression data}},
  author       = {Dettling, M. and B{\"u}hlmann, P.},
  journal      = {Bioinformatics},
  volume       = 19,
  number       = 9,
  pages        = {1061-1069},
  year         = 2003,
}

@ARTICLE{eibl06multi,
  title        = {{Multiclass Boosting for Weak Classifiers}},
  author       = {Eibl, G. and Pfeiffer, K.P.},
  journal      = {The Journal of Machine Learning Research},
  volume       = 6,
  pages        = {189-210},
  year         = 2005,
  publisher    = {MIT Press Cambridge, MA, USA},
}

@ARTICLE{friedman2000sip,
  title        = {{Special Invited Paper. Additive Logistic Regression: A Statistical View of Boosting}},
  author       = {Friedman, J. and Hastie, T. and Tibshirani, R.},
  journal      = {The Annals of Statistics},
  volume       = 28,
  number       = 2,
  pages        = {337-374},
  year         = 2000,
  publisher    = {JSTOR},
}

@ARTICLE{torralba2004sfe,
  title        = {{Sharing features: efficient boosting procedures for multiclass object detection}},
  author       = {Torralba, A. and Murphy, KP and Freeman, WT},
  journal      = {Computer Vision and Pattern Recognition, 2004},
  volume       = 2,
}

@ARTICLE{folino2006icg,
  title        = {{Improving cooperative GP ensemble with clustering and pruning for pattern classification}},
  author       = {Folino, G. and Pizzuti, C. and Spezzano, G.},
  journal      = {Proceedings of the 8th annual conference on Genetic and evolutionary computation},
  pages        = {791-798},
  year         = 2006,
  publisher    = {ACM Press New York, NY, USA},
}

@ARTICLE{bi2006eae,
  title        = {{An evidential approach in ensembles}},
  author       = {Bi, Y. and Dubitzky, W.},
  journal      = {Proceedings of the 2006 ACM symposium on Applied computing},
  pages        = {1-6},
  year         = 2006,
  publisher    = {ACM Press New York, NY, USA},
  abstract     = {In this paper, we describe an approach to modeling the general process of combining
                  decisions involved in ensembles of classifiers as an evidential reasoning process. This work proposes
                  a novel structure, theoretical properties and manipulation mechanisms for representing classifier
                  decisions as pieces of evidence. The advantage of the representation formalism is that it not only
                  facilitates the distinguishing of trivial focal elements from important ones, resulting in the
                  improvement of the ensemble performance, but it also effectively reduces the computation-time from
                  exponential (as required in the conventional process of combining multiple pieces of evidence) to linear.
                  We have conducted a comparative analysis on the effectiveness of the proposed evidence representation
                  formalism in the text categorization domain. By comparing this method with majority voting and the previous
                  results, we also demonstrate the advantage of this novel approach in combining classifiers.},
}

@ARTICLE{mashao2006ccd,
  title        = {{Combining Classifier Decisions for Robust Speaker Identification}},
  author       = {MASHAO, D.J. and SKOSAN, M.},
  journal      = {Pattern recognition},
  volume       = 39,
  number       = 1,
  pages        = {147-155},
  year         = 2006,
  publisher    = {Elsevier Science},
}

@ARTICLE{murua02upperbound,
  author       = {Murua, A.},
  title        = {Upper Bounds for Error Rates of Linear Combinations
                  of Classifiers},
  journal      = {PAMI},
  volume       = 24,
  year         = 2002,
  number       = 5,
  month        = {May},
  pages        = {591-602},
}

@ARTICLE{nanni2006ekl,
  title        = {{An ensemble of K-local hyperplanes for predicting protein-protein interactions}},
  author       = {Nanni, L. and Lumini, A.},
  journal      = {Bioinformatics},
  volume       = 22,
  number       = 10,
  pages        = 1207,
  year         = 2006,
}

@ARTICLE{nair:jmlr02,
  title        = {Some greedy learning algorithms for sparse
                  regression and classification with mercer kernels},
  author       = {P. B. Nair and A. Choudhury and A. J. Keane},
  journal      = {Journal of Machine Learning Research},
  volume       = 3,
  pages        = {781-801},
  year         = 2002,
}

@PHDTHESIS{oza2001thesis,
  title        = {{Online Ensemble Learning}},
  author       = {Oza, N.C.},
  year         = 2001,
  school       = {UNIVERSITY of CALIFORNIA},
}

@ARTICLE{oza2001eco,
  title        = {{Experimental comparisons of online and batch versions of bagging and boosting}},
  author       = {Oza, N.C. and Russell, S.},
  journal      = {Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages        = {359-364},
  year         = 2001,
  publisher    = {ACM Press New York, NY, USA},
}

@ARTICLE{oza2005oba,
  title        = {{Online bagging and boosting}},
  author       = {Oza, NC},
  journal      = {Systems, Man and Cybernetics, 2005 IEEE International Conference on},
  volume       = 3,
  year         = 2005,
}

@INPROCEEDINGS{pennock00normative,
  author       = {David M. Pennock and Pedrito {Maynard-Reid {II}} and
                  C. Lee Giles and Eric Horvitz},
  title        = {A Normative Examination of Ensemble Learning
                  Algorithms},
  booktitle    = {Proc. 17th International Conf. on Machine Learning},
  publisher    = {Morgan Kaufmann, San Francisco, CA},
  pages        = {735--742},
  year         = 2000,
}

@TECHREPORT{poggio02bagging,
  author       = {Tomaso Poggio and Ryan Rifkin and Sayan Mukherjee
                  and Alex Rakhlin},
  title        = {Bagging Regularizes},
  institution  = {MIT AI Lab},
  year         = 2002,
  number       = {AI Memo 2002-003, CBCL Memo 214},
}

@MISC{roli02notes,
  author       = {Fabio Roli},
  title        = {Lecture Notes: Linear Combiners for Fusion of
                  Pattern Classifiers},
  url          = {http://www.disi.unige.it/person/MasulliF/ricerca/school2002/contributions/vietri02-lect-roli.pdf},
}

@article{ruta05selection,
  author    = {Dymitr Ruta and
               Bogdan Gabrys},
  title     = {Classifier selection for majority voting.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {63-81}
}

@ARTICLE{ruta03sets,
  author       = {Dymitr Ruta and Bogdan Gabrys},
  title        = {Set Analysis of Coincident Errors and Its
                  Applications for Combining Classifiers},
  journal      = {Combinatorial Optimisation},
  volume       = 13,
  publisher    = {Kluwer Academic},
  year         = 2003,
}

@ARTICLE{schapiresinger00boostexter,
  author       = {R.E.~Schapire and Y.~Singer},
  title        = {BoosTexter: A boosting-based system for text
                  categorization},
  journal      = {Machine Learning},
  year         = 2000,
  volume       = 39,
  number       = {2/3},
  pages        = {135-168},
}

@INPROCEEDINGS{shan05cmi,
  author       = {Shan, C. and Gong, S. and McOwan, P.W.},
  title        = {{Conditional Mutual Information Based Boosting for Facial Expression Recognition}},
  booktitle    = {BMVC},
  year         = 2005
}

@INPROCEEDINGS{shih03bundling,
  author       = {Lawrence Shih and Jason Rennie and Yu-Han Chang and
                  David Karger},
  title        = {Text Bundling : Statistics Based Data Reduction},
  booktitle    = {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year         = 2003,
  editor       = {Tom Fawcett and Nina Mishra},
  month        = {August},
  address      = {Washington DC, USA},
}

@ARTICLE{shippkuncheva02,
  author       = {C.A. Shipp and L.I. Kuncheva},
  title        = {Relationships between combination methods and
                  measures of diversity in combining classifiers},
  journal      = {Information Fusion},
  year         = 2002,
  number       = 3,
  pages        = {135-148},
}

@INPROCEEDINGS{skurichina00role,
  author       = {M. Skurichina and R.P.W. Duin},
  title        = {The Role of Combining Rules in Bagging and Boosting},
  booktitle    = {Advances in Pattern Recognition, Proc. Joint IAPR
                  International Workshops SSPR2000 and SPR2000 Lecture
                  Notes in Computer Science, vol. 1876},
  publisher    = {Springer},
  month        = {June},
  pages        = {236-245},
  editors      = {F.J. Ferri, J.M. Inesta, A. Amin, P. Pudil},
  year         = 2000,
  address      = {Alicante, Spain},
}

@PHDTHESIS{schubert2005qca,
  title        = {{Quantifying correlation and its effects on system performance in classifier fusion}},
  author       = {Schubert, C.M.},
  year         = 2005,
  school       = {AIR FORCE INSTITUTE OF TECHNOLOGY},
  url          = {http://gradworks.umi.com/31/91/3191355.html}
}

@ARTICLE{skurichina2002bba,
  title        = {{Bagging, Boosting and the Random Subspace Method for Linear Classifiers}},
  author       = {Skurichina, M.W. and Duin, R.P.W.W.},
  journal      = {Pattern Analysis \& Applications},
  volume       = 5,
  number       = 2,
  pages        = {121-135},
  year         = 2002,
  publisher    = {Springer},
}

@INPROCEEDINGS{stainvas00,
  title        = {Blurred Face Recognition via a Hybrid Network
                  Architecture},
  author       = {I. Stainvas and N. Intrator},
  booktitle    = {ICPR},
  year         = 2000,
  volume       = 2,
  pages        = {809--812},
}

@INPROCEEDINGS{stephenson03compiler,
  author       = {Mark Stephenson and Una-May O'Reilly and Martin
                  C. Martin and Saman P. Amarasinghe},
  title        = {Genetic Programming Applied to Compiler Heuristic
                  Optimization.},
  booktitle    = {EuroGP},
  year         = 2003,
  pages        = {238-253},
}

@ARTICLE{strehl2003cek,
  title        = {{Cluster ensembles- A knowledge reuse framework for combining multiple partitions.}},
  author       = {Strehl, A. and Ghosh, J.},
  journal      = {Journal of Machine Learning Research},
  volume       = 3,
  number       = 3,
  pages        = {583-617},
  year         = 2003,
  publisher    = {MIT Press},
}

@ARTICLE{suganthan01hierarchical,
  author       = {P. N. Suganthan},
  title        = {Pattern classification using multiple hierarchical
                  overlapped self-organising maps},
  journal      = {Pattern Recognition},
  year         = 2001,
  volume       = 34,
  number       = 11,
  pages        = {2173-2179},
  month        = {November},
}

@ARTICLE{tetko02associative,
  author       = {Tetko, I. V.},
  title        = {Associative neural network},
  journal      = {Neural Processing Letters},
  volume       = 16,
  number       = 2,
  pages        = {187-199},
  abstract     = {An associative neural network (ASNN) is a
                  combination of an ensemble of the feed-forward
                  neural networks and the K-nearest neighbor
                  technique. The introduced network uses correlation
                  between ensemble responses as a measure of distance
                  among the analyzed cases for the nearest neighbor
                  technique and provides an improved prediction by the
                  bias correction of the neural network ensemble both
                  for function approximation and
                  classification. Actually, the proposed method
                  corrects a bias of a global model for a considered
                  data case by analyzing the biases of its nearest
                  neighbors determined in the space of calculated
                  models. An associative neural network has a memory
                  that can coincide with the training set. If new data
                  become available the network can provide a
                  reasonable approximation of such data without a need
                  to retrain the neural network ensemble. Applications
                  of ASNN for prediction of lipophilicity of chemical
                  compounds and classification of UCI letter and
                  satellite data set are presented. The developed
                  algorithm is available on-line at
                  http://www.virtuallaboratory.org/lab/asnn.},
  year         = 2002,
  url          = {http://cogprints.ecs.soton.ac.uk/archive/00001441/},
}

@ARTICLE{tetko02associative2,
  author       = {Tetko, I. V.},
  title        = {Neural network studies. 4. Introduction to
                  associative neural networks},
  journal      = {J Chem Inf Comput Sci},
  volume       = 42,
  number       = 3,
  pages        = {717-28.},
  keywords     = {*Neural Networks (Computer) Support,
                  Non-U.S. Gov\'t},
  abstract     = {Associative neural network (ASNN) represents a
                  combination of an ensemble of feed-forward neural
                  networks and the k-nearest neighbor technique. This
                  method uses the correlation between ensemble
                  response,, as a measure of distance amid the
                  analyzed cases for the nearest neighbor
                  technique. This provides an improved prediction by
                  the bias correction of the neural network
                  ensemble. An associative neural network has a memory
                  that can coincide with the training set. If new data
                  becomes available, the network further improves its
                  predictive ability and provides a reasonable
                  approximation of the unknown function without a need
                  to retrain the neural network ensemble, This feature
                  of the method dramatically improves its predictive
                  ability over traditional neural networks and
                  k-nearest neighbor techniques, as demonstrated using
                  several artificial data sets and a program to
                  predict lipophilicity of chemical compounds. Another
                  important feature of ASNN is the possibility to
                  interpret neural network results by analysis of
                  correlations between data cases in the space of
                  models, It is shown that analysis of such
                  correlations makes it possible to provide
                  \"property-targeted\" clustering of data. The
                  possible applications and importance of ASNN in drug
                  design and medicinal and combinatorial chemistry are
                  discussed. The method is available on-line at
                  http://www.vcclab.org/lab/asnn.},
  year         = 2002,
}

@ARTICLE{tetko01volume,
  author       = {Tetko, I. V. and Kovalishyn, V. V. and Livingstone,
                  D. J.},
  title        = {Volume learning algorithm artificial neural networks
                  for 3D QSAR studies},
  journal      = {J Med Chem},
  volume       = 44,
  number       = 15,
  pages        = {2411-20.},
  abstract     = {The current study introduces a new method, the
                  volume learning algorithm (VLA), for the
                  investigation of three-dimensional quantitative
                  structure-activity relationships (QSAR) of chemical
                  compounds. This method incorporates the advantages
                  of comparative molecular field analysis (CoMFA) and
                  artificial neural network approaches. VLA is a
                  combination of supervised and unsupervised neural
                  networks applied to solve the same problem. The
                  supervised algorithm is a feed-forward neural
                  network trained with a back-propagation algorithm
                  while the unsupervised network is a self-organizing
                  map of Kohonen. The use of both of these algorithms
                  makes it possible to cluster the input CoMFA field
                  variables and to use only a small number of the most
                  relevant parameters to correlate spatial properties
                  of the molecules with their activity. The
                  statistical coefficients calculated by the proposed
                  algorithm for cannabimimetic aminoalkyl indoles were
                  comparable to, or improved, in comparison to the
                  original study using the partial least squares
                  algorithm. The results of the algorithm can be
                  visualized and easily interpreted. Thus, VLA is a
                  new convenient tool for three-dimensional QSAR
                  studies.},
  year         = 2001,
}

@ARTICLE{tetko02asnnapp,
  author       = {Tetko, I. V. and Tanchuk, V. Y.},
  title        = {Application of associative neural networks for
                  prediction of lipophilicity in ALOGPS 2.1 program},
  journal      = {J Chem Inf Comput Sci},
  volume       = 42,
  number       = 5,
  pages        = {1136-45.},
  abstract     = {This article provides a systematic study of several
                  important parameters of the Associative Neural
                  Network (ASNN), such as the number of networks in
                  the ensemble, distance measures, neighbor functions,
                  selection of smoothing parameters, and strategies
                  for the user-training feature of the algorithm. The
                  performance of the different methods is assessed
                  with several training/test sets used to predict
                  lipophilicity of chemical compounds. The Spearman
                  rank-order correlation coefficient and Parzen-window
                  regression methods provide the best performance of
                  the algorithm. If additional user data is available,
                  an improved prediction of lipophilicity of chemicals
                  up to 2-5 times can be calculated when the
                  appropriate smoothing parameters for the neural
                  network are selected. The detected best combinations
                  of parameters and strategies are implemented in the
                  ALOGPS 2.1 program that is publicly available at
                  http://www.vcclab.org/lab/alogps.},
  year         = 2002,
}

@ARTICLE{tetko01estimation,
  author       = {Tetko, I. V. and Tanchuk, V. Y. and Kasheva,
                  T. N. and Villa, A. E.},
  title        = {Estimation of aqueous solubility of chemical
                  compounds using E-state indices},
  journal      = {J Chem Inf Comput Sci},
  volume       = 41,
  number       = 6,
  pages        = {1488-93.},
  abstract     = {The molecular weight and electrotopological E-state
                  indices were used to estimate by Artificial Neural
                  Networks aqueous solubility for a diverse set of
                  1291 organic compounds. The neural network with
                  33-4-1 neurons provided highly predictive results
                  with r(2) = 0.91 and RMS = 0.62. The used parameters
                  included several combinations of E-state indices
                  with similar properties. The calculated results were
                  similar to those published for these data by
                  Huuskonen (2000). However, in the current study only
                  E-state indices were used without need of additional
                  indices (the molecular connectivity, shape,
                  flexibility and indicator indices) also considered
                  in the previous study. In addition, the present
                  neural network contained three times less hidden
                  neurons. Smaller neural networks and use of one
                  homogeneous set of parameters provides a more robust
                  model for prediction of aqueous solubility of
                  chemical compounds. Limitations of the developed
                  method for prediction of large compounds are
                  discussed, The developed approach is available
                  online at http://www.vcclab.org/lab/alogps},
  year         = 2001,
}

@ARTICLE{tetko01prediction,
  author       = {Tetko, I. V. and Tanchuk, V. Y. and Villa, A. E.},
  title        = {Prediction of n-octanol/water partition coefficients
                  from PHYSPROP database using artificial neural
                  networks and E-state indices},
  journal      = {J Chem Inf Comput Sci},
  volume       = 41,
  number       = 5,
  pages        = {1407-21.},
  abstract     = {The molecular weight and electrotopological E-state
                  indices were used to estimate by Artificial Neural
                  Networks aqueous solubility for a diverse set of
                  1291 organic compounds. The neural network with
                  33-4-1 neurons provided highly predictive results
                  with r(2) = 0.91 and RMS = 0.62. The used parameters
                  included several combinations of E-state indices
                  with similar properties. The calculated results were
                  similar to those published for these data by
                  Huuskonen (2000). However, in the current study only
                  E-state indices were used without need of additional
                  indices (the molecular connectivity, shape,
                  flexibility and indicator indices) also considered
                  in the previous study. In addition, the present
                  neural network contained three times less hidden
                  neurons. Smaller neural networks and use of one
                  homogeneous set of parameters provides a more robust
                  model for prediction of aqueous solubility of
                  chemical compounds. Limitations of the developed
                  method for prediction of large compounds are
                  discussed. The developed approach is available
                  online at http://www.vcclab.org/lab/alogps.},
  year         = 2001,
}

@article{tsymbal05search,
  author    = {Alexey Tsymbal and
               Mykola Pechenizkiy and
               Padraig Cunningham},
  title     = {Diversity in search strategies for ensemble feature selection.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {83-98}
}

@ARTICLE{tino04nonlinear,
  author       = {P. Tino and I. Nabney and B.S. Williams and J. Losel
                  and Y. Sun},
  title        = {Non-linear Prediction of Quantitative
                  Structure-Activity Relationships},
  journal      = {Journal of Chemical Information and Computer
                  Sciences},
  year         = 2004,
  volume       = 44,
  number       = 5,
  pages        = {1647--1653},
}

@ARTICLE{topchy2004acp,
  title        = {{Analysis of consensus partition in cluster ensemble}},
  author       = {Topchy, AP and Law, MHC and Jain, AK and Fred, AL},
  journal      = {Data Mining, 2004. ICDM 2004. Proceedings. Fourth IEEE International Conference on},
  pages        = {225-232},
  year         = 2004,
}

@INPROCEEDINGS{tresp00generalizedbayescommittee,
  author       = {Volker Tresp},
  title        = {The Generalized Bayesian Committee Machine},
  booktitle    = {Proceedings of the Sixth ACM SIGKDD International
                  Conference on Knowledge Discovery and Data Mining,
                  KDD-2000},
  year         = 2000,
  pages        = {130-139},
  abstract     = { In this paper we introduce the Generalized Bayesian
                  Committee Machine (GBCM) for applications with large
                  data sets. In particular, the GBCM can be used in
                  the context of kernel based systems such as
                  smoothing splines, kriging, regularization networks
                  and Gaussian process regression which ---for
                  computational reasons--- are otherwise limited to
                  rather small data sets. The GBCM provides a novel
                  and principled way of combining estimators trained
                  for regression, classification, the prediction of
                  counts, the prediction of lifetimes and other
                  applications which can be derived from the
                  exponential family of distributions. We describe an
                  online version of the GBCM which only requires one
                  pass through the data set and only requires the
                  storage of a matrix of the dimension of the number
                  of query or test points. After training, the
                  prediction at additional test points only requires
                  resources dependent on the number of query points
                  but is independent of the number of training
                  data. We confirm the good scaling behavior using
                  real and experimental data sets. },
  url          = {http://www.boosting.org/papers/upload_7240_kddpaper2.ps},
}

@ARTICLE{tresp00bayescommittee,
  author       = {Volker Tresp},
  title        = {A Bayesian Committee Machine},
  journal      = {Neural Computation},
  year         = 2000,
  pages        = {2719-2741},
  volume       = 12,
  number       = 11,
  abstract     = { The Bayesian committee machine (BCM) is a novel
                  approach to combining estimators which were trained
                  on different data sets. Although the BCM can be
                  applied to the combination of any kind of estimators
                  the main foci are Gaussian process regression and
                  related systems such as regularization networks and
                  smoothing splines for which the degrees of freedom
                  increase with the number of training data. Somewhat
                  surprisingly, we find that the performance of the
                  BCM improves if several test points are queried at
                  the same time and is optimal if the number of test
                  points is at least as large as the degrees of
                  freedom of the estimator. The BCM also provides a
                  new solution for online learning with potential
                  applications to data mining. We apply the BCM to
                  systems with fixed basis functions and discuss its
                  relationship to Gaussian process
                  regression. Finally, we also show how the ideas
                  behind the BCM can be applied in a non-Bayesian
                  setting to extend the input dependent combination of
                  estimators.},
  url          = {http://www.boosting.org/papers/upload_7235_bcm5.ps},
}

@INBOOK{tresp01committeemachines,
  author       = {Volker Tresp},
  editor       = {Yu Hen Hu and Jenq-Nen Hwang},
  title        = {Handbook for Neural Network Signal Processing},
  chapter      = {Committee machines},
  publisher    = {CRC Press},
  year         = 2001,
}

@ARTICLE{tumerghosh00robust,
  author       = {K Tumer and J Ghosh},
  title        = {Robust Combining of Disparate Classifiers through
                  Order Statistics},
  journal      = {To appear in Pattern Analysis and Applications,
                  Special issue on Fusion of Multiple Classifiers},
  year         = 2002,
  number       = 2,
  volume       = 5,
  url          = {http://citeseer.nj.nec.com/592387.html},
}

@ARTICLE{ueda00optimal,
  author       = {N Ueda},
  title        = {Optimal linear combination of neural networks for
                  improving classification Performance },
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year         = 2000,
  volume       = 22,
  number       = 2,
  pages        = {207--215},
}

@INPROCEEDINGS{valentini03baggedsvm,
  author       = {Giorgio Valentini and Thomas G. Dietterich},
  title        = {Low Bias Bagged Support Vector Machines},
  booktitle    = {20th {I}nternational {C}onference on {M}achine
                  {L}earning (ICML'03)},
  year         = 2003,
  editor       = {Tom Fawcett and Nina Mishra},
  month        = {August},
  address      = {Washington DC, USA},
}

@ARTICLE{valev01multi,
  author       = {V. Valev and A. Asaithambi},
  title        = {Multidimensional pattern recognition problems and
                  combining classifiers},
  journal      = {Pattern Recognition Letters},
  volume       = 22,
  year         = 2001,
  number       = 12,
  month        = {October},
  pages        = {1291-1297},
}

@ARTICLE{viola01boosting,
  title        = {{Rapid object detection using a boosted cascade of simple features}},
  author       = {Viola, P. and Jones, M.},
  journal      = {Proc. CVPR},
  volume       = 1,
  pages        = {511-518},
  year         = 2001,
}

@article{wang2003mcd,
  title        = {{Mining concept-drifting data streams using ensemble classifiers}},
  author       = {Wang, H. and Fan, W. and Yu, P.S. and Han, J.},
  journal      = {Proceedings of the ninth ACM SIGKDD international conference on
                  Knowledge discovery and data mining},
  pages        = {226-235},
  year         = {2003},
  publisher    = {ACM Press New York, NY, USA}
}

@INPROCEEDINGS{wang01diversity,
  author       = {Wenjia Wang and Derek Partridge and John
                  Etherington},
  title        = {Hybrid Ensembles and Coincident-Failure Diversity},
  booktitle    = {Proceedings of the International Joint Conference on
                  Neural Networks},
  publisher    = {IEEE Press},
  month        = {July},
  pages        = {2376--2381},
  volume       = 4,
  year         = 2001,
  address      = {Washington, USA},
}

@ARTICLE{webb05nsn,
  title        = {{Not So Naive Bayes: Aggregating One-Dependence Estimators}},
  author       = {Webb, G.I. and Boughton, J.R. and Wang, Z.},
  journal      = {Machine Learning},
  volume       = 58,
  number       = 1,
  pages        = {5-24},
  year         = 2005,
  publisher    = {Springer},
  annote       = {Combining probabilistic models - similar to naive bayes.},
}

@ARTICLE{webb00multiboosting,
  author       = {Geoffrey I. Webb},
  title        = {Multi{B}oosting: {A} Technique for Combining
                  {B}oosting and {W}agging},
  journal      = {Machine Learning},
  volume       = 40,
  number       = 2,
  publisher    = {Kluwer Academic Publishers, Boston},
  pages        = {159--196},
  year         = 2000,
  url          = {http://citeseer.nj.nec.com/webb98multiboosting.html},
}

@INPROCEEDINGS{wersing02a,
  author       = {H. Wersing and E. K{\"o}rner},
  title        = {Unsupervised Learning of Combination Features for
                  Hierarchical Recognition Models},
  booktitle    = {Int. Conf. Artif. Neur. Netw. ICANN},
  year         = 2002,
  note         = {accepted},
}

@ARTICLE{wersingkorner02,
  author       = {Heiko Wersing and Edgar Korner},
  title        = {Learning Optimized Features for Hierarchical Models
                  of Invariant Object Recognition},
  journal      = {Neural Computation (to appear)},
  year         = 2002,
}

@INPROCEEDINGS{wezel00nonconformist,
  author       = {M.C. van Wezel and M.D. Out and W.A. Kosters},
  title        = {Ensembles of nonconformist neural networks},
  booktitle    = {Proceedings of the Twelfth Belgium-Netherlands
                  Artificial Intelligence Conference},
  editors      = {H. Weigand and A. van den Bosch},
  pages        = {165-172},
  year         = 2000,
}

@TECHREPORT{whitaker03examining,
  author       = {Christopher Whitaker and Ludmila Kuncheva},
  title        = {Examining the relationship between majority vote
                  accuracy and diversity in bagging and boosting},
  institution  = {School of Informatics, University of Wales, Bangor},
  year         = 2003,
  type         = {Technical Report},
  url          = {http://www.informatics.bangor.ac.uk/~kuncheva/papers/lkcw_tr.pdf},
}

@article{windeatt05measures,
  author    = {Terry Windeatt},
  title     = {Diversity measures for multiple classifier system analysis
               and design.},
  journal   = {Information Fusion},
  volume    = {6},
  number    = {1},
  year      = {2005},
  pages     = {21-36}
}

@INPROCEEDINGS{yaobrown01telecoms,
  author       = {X. Yao and M. Fischer and G. Brown},
  title        = {Neural Network Ensembles and their Application to
                  Traffic Flow Prediction in Telecommunications
                  Networks},
  booktitle    = {Proceedings of International Joint Conference on
                  Neural Networks},
  note         = {Washington DC},
  pages        = {693-698},
  publisher    = {IEEE Press},
  year         = 2001,
}

@INPROCEEDINGS{yang06select,
  author       = {Ying Yang and Geoff Webb and Jesus Cerquides and
                  Kevin Korb and Janice Boughton and Kai Ming Ting},
  title        = {To Select or To Weigh: A Comparative Study of Model
                  Selection and Model Weighing for SPODE Ensembles},
  booktitle    = {17th European Conference on Machine Learning (ECML)},
  year         = 2006,
}

@INPROCEEDINGS{yang05ensemble,
  author       = {Ying Yang and Kevin Korb and Kai Ming Ting and Geoff Webb},
  title        = {Ensemble Selection for SuperParent-One-Dependence Estimators},
  booktitle    = {18th Australian Joint Conference on Artificial Intelligence},
  year         = 2005,
}

@ARTICLE{zhouwutang02ensembling,
  author       = {Z.-H. Zhou, J. Wu, and W. Tang},
  title        = {Ensembling neural networks: Many could be better
                  than all},
  journal      = {Artificial Intelligence},
  volume       = 137,
  number       = {1-2},
  pages        = {239-263},
  year         = 2002,
}

@ARTICLE{zhou02extracting,
  author       = {Z.-H. Zhou, Y. Jiang, and S.-F. Chen},
  title        = {Extracting Symbolic Rules from Trained Neural
                  Network Ensembles},
  journal      = {AI Communications, 2003, 16(1): 3-15},
  volume       = 16,
  number       = 1,
  pages        = {3-15},
  year         = 2003,
  url          = {http://citeseer.nj.nec.com/zhou03extracting.html},
}

@ARTICLE{zenobi01using,
  author       = {Gabriele Zenobi and P{\'a}draig Cunningham},
  title        = {Using Diversity in Preparing Ensembles of
                  Classifiers Based on Different Feature Subsets to
                  Minimize Generalization Error},
  journal      = {Lecture Notes in Computer Science},
  volume       = 2167,
  pages        = {576--587},
  year         = 2001,
}

@ARTICLE{zhang2004onb,
  title        = {{The optimality of naive Bayes}},
  author       = {Zhang, H.},
  journal      = {17th International FLAIRS conference, Miami Beach, May},
  pages        = {17-19},
  year         = 2004,
}

@INPROCEEDINGS{zhou02ensembling,
  author       = {Z.H. Zhou and J.Wu and W.Tang and Z.Q. Chen},
  title        = "Selectively ensembling neural classifiers" ,
  booktitle    = {International Joint Conference on Neural Networks},
  volume       = 2,
  page         = {1411-1415},
  year         = 2002,
}

@ARTICLE{zhu2006ecn,
  title        = {{Effective classification of noisy data streams with attribute-oriented dynamic classifier selection}},
  author       = {Zhu, X. and Wu, X. and Yang, Y.},
  journal      = {Knowledge and Information Systems},
  volume       = 9,
  number       = 3,
  pages        = {339-363},
  year         = 2006,
  publisher    = {Springer},
}



#PRE2000ENSEMBLEPAPERS

@TECHREPORT{ali95comparison,
  author       = {K. Ali},
  year         = 1995,
  title        = {A comparison of methods for learning and combining
                  evidence from multiple models},
  institution  = {University of California, Irvine, Dept. of
                  Information and Computer Sciences},
  number       = {UCI TR \#95-47},
  abstract     = {Most previous work on multiple models has been done
                  on a few domains. We present a comparsion of three
                  ways of learning multiple models on 29 data sets
                  from the UCI repository. The methods are bagging,
                  $k$-fold partition learning and stochastic
                  search. By using 29 data sets of various kinds -
                  artificial data sets, artificial data sets with
                  noise, molecular-biology and real-world noisy data
                  sets - we are able to draw robust experimental
                  conclusions about the kinds of data sets for which
                  each learning method works best. We also compare
                  four evidence combination methods (Uniform Voting,
                  Bayesian Combination, Distribution Summation and
                  Likelihood Combination) and characterize the kinds
                  of data sets for which each method works best.},
}

@ARTICLE{avnimelech99boostedregression,
  author       = {R.~Avnimelech and N.~Intrator},
  title        = {Boosting Regression Estimators},
  journal      = {Neural Computation},
  year         = 1999,
  volume       = 11,
  pages        = {491--513},
}

@ARTICLE{avnimelechintrator99boosted,
  pages        = {475-490},
  author       = {Ran Avnimelech and Nathan Intrator},
  year         = 1999,
  journal      = {Neural Computation},
  volume       = 11,
  title        = {Boosted Mixtures of Experts: An Ensemble Learning
                  Scheme},
}

@ARTICLE{parmanto96,
  author       = {B.Parmanto and P.W.Munro and H.R.Doyle},
  title        = {Improving Committee Diagnosis with Resampling
                  Techniques},
  editor       = {D.S.Touretzky and M.C.Mozer and M.E.Hesselmo},
  volume       = 8,
  journal      = {Advances in Neural Information Processing Systems},
  year         = 1996,
  pages        = {882--888},
  publisher    = {The {MIT} Press},
}

@ARTICLE{back93evolution,
  author       = {T. B{\"{a}}ck and H.-P. Schwefel},
  title        = {An overview of evolutionary algorithms for parameter
                  optimization},
  journal      = {Evolutionary Computation},
  year         = 1993,
  volume       = 1,
  number       = 1,
  pages        = {1-23},
}

@ARTICLE{batesgranger69,
  author       = {J. M. Bates and C. W. J. Granger},
  title        = {The combination of forecasts},
  journal      = {Operations Research Quarterly},
  year         = 1969,
  number       = 20,
  pages        = {451-468},
  annote       = {The use of combination techniques in financial forecasting.}
}

@ARTICLE{battiti1994democracy,
  author       = {Roberto Battiti and Anna Maria Colla},
  title        = {Democracy in Neural Nets: Voting Schemes for
                  Classification},
  journal      = {Neural Networks},
  year         = 1994,
  volume       = 7,
  number       = 4,
  pages        = {691--707},
  abstract     = {Discusses some possible ways to combine the outputs
                  of a set of neural network classifiers to reach a
                  combined decision with a higher performance in terms
                  of lower rejection rates and/or better accuracy
                  rates. The methods considered range from the
                  requirement of a complete agreement among the
                  individual classifications to election schemes based
                  on the distribution of votes collected by the
                  different classes. In addition, the rejection rules
                  based on the different output classes can be
                  complemented by rules that also consider the
                  information in the individual output vectors, with
                  the possibility of using threshold requirements and
                  that of averaging the different vectors. Although
                  the Bayesian framework and some probabilistic
                  assumptions provide useful indications about the
                  potential advantage of different combination
                  schemes, the combined performance ultimately depends
                  on the joint probability distribution of the
                  outputs, and it can be estimated by joining the
                  results of different nets on the same test set. The
                  combination methods are very flexible, they permit a
                  straightforward cooperation of neural and
                  traditional recognizers, and they are appropriate in
                  a development environment where experiments are
                  performed with different kinds of nets and features
                  for a selected application. From the authors'
                  experiments in the field of handwritten digit
                  recognition (up to a total of more than 50000
                  characters), they found that the use of a small
                  number of nets (two to three) with a sufficiently
                  large uncorrelation in their mistakes reaches a
                  combined performance that is significantly higher
                  than the best obtainable from the individual nets,
                  with a negligible effort after starting from a pool
                  of networks produced in the development phase of an
                  application. In particular, for a real-world OCR
                  application, the best accuracy increase is about
                  half the increase in the rejection rate, so that
                  accuracies of the order of 99.5% can be reached by
                  rejecting less than 5% of the patterns. This
                  performance is significant for real applications.},
}

@ARTICLE{bauer99empirical,
  author       = {E. Bauer and R. Kohavi},
  title        = {An Empirical Comparison of Voting Classification
                  Algorithms: Bagging, Boosting, and Variants},
  journal      = {Machine Learning},
  year         = 1999,
  volume       = 36,
  number       = {1,2},
}

@ARTICLE{bishop95training,
  author       = {Chris M. Bishop},
  title        = {Training with Noise is Equivalent to {T}ikhonov
                  Regularization},
  journal      = {Neural Computation},
  volume       = 7,
  number       = 1,
  pages        = {108--116},
  year         = 1995,
  url          = {http://citeseer.nj.nec.com/bishop94training.html},
}

@BOOK{bishop95book,
  author       = {Christopher M. Bishop},
  title        = {Neural Networks for Pattern Recogntion},
  publisher    = {Oxford University Press},
  year         = 1995,
  isbn         = {0-19-853864-2},
}

@MISC{uci,
  author       = {C.L. Blake and C.J. Merz},
  year         = 1998,
  title        = {{UCI} Repository of machine learning databases},
  url          = {http://www.ics.uci.edu/$\sim$mlearn/MLRepository.html},
  institution  = {University of California, Irvine, Dept. of
                  Information and Computer Sciences},
}

@INPROCEEDINGS{bottou91framework,
  author       = {L{\'e}on Bottou and Patrick Gallinari},
  title        = {A Framework for the Cooperation of Learning
                  Algorithms},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 3,
  publisher    = {Morgan Kaufmann Publishers, Inc.},
  editor       = {Richard P. Lippmann and John E. Moody and David
                  S. Touretzky},
  pages        = {781--788},
  year         = 1991,
}

@TECHREPORT{breiman96arcing,
  author       = {Leo Breiman},
  title        = {Bias, Variance, and Arcing Classifiers},
  institution  = {Statistics Department, Berkeley},
  year         = 1996,
  number       = 460,
}

@ARTICLE{breiman96bagging,
  author       = {Leo Breiman},
  title        = {Bagging Predictors},
  journal      = {Machine Learning},
  volume       = 24,
  number       = 2,
  pages        = {123-140},
  year         = 1996,
}

@TECHREPORT{breiman98randomizing,
  author       = {Leo Breiman},
  title        = {Randomizing Outputs to increase prediction accuracy},
  institution  = {Statistics Department, University of California},
  month        = {May},
  year         = 1998,
  type         = {Technical Report},
  number       = 518,
  url          = {http://www.boosting.org/papers/Bre98.pdf},
}

@TECHREPORT{breiman99random,
  author       = {Leo Breiman},
  year         = 1999,
  title        = {Random Forests Random Features},
  institution  = {University of California, Berkley (Dept of
                  Statistics)},
  number       = 567,
}

@INPROCEEDINGS{brodleylane96,
  author       = {C. Brodley and T. Lane},
  title        = {Creating and exploiting coverage and diversity},
  booktitle    = {AAAI-96 Workshop Integrating Multiple Learned
                  Models},
  year         = 1996,
}

@TECHREPORT{carneytuning99,
  author       = {John Carney and Padraig Cunningham},
  title        = {Tuning diversity in bagged neural network ensembles},
  institution  = {Trinity College Dublin},
  year         = 1999,
  number       = {TCD-CS-1999-44},
}

@INPROCEEDINGS{chan95arbiter,
  author       = {Philip K.~Chan and Salvatore J.~Stolfo},
  title        = {Learning Arbiter and Combiner Trees from Partitioned
                  Data for Scaling Machine Learning},
  booktitle    = {The first international conference on knowledge
                  discovery and data mining, KDD '95},
  year         = 1995,
  pages        = {39-45},
}

@INPROCEEDINGS{chan96local,
  author       = {Philip K.~Chan and Salvatore J.~Stolfo},
  title        = {Sharing Learned Models among Remote Database
                  Partitions by Local Meta-learning},
  booktitle    = {Proc. Second Intl. Conf. on Knowledge Discovery \&
                  Data Mining},
  year         = 1996,
  pages        = {2--7},
}

@ARTICLE{chan98accuracy,
  author       = {Philip K.~Chan and Salvatore J.~Stolfo},
  title        = {On the Accuracy of Meta-learning for Scalable Data
                  Mining,},
  journal      = {Journal of Intelligent Information Systems},
  year         = 1997,
  volume       = 8,
  pages        = {5--28},
}

@ARTICLE{clemen89forecasts,
  author       = {R. Clemen},
  year         = 1989,
  title        = {Combining forecast: A review and annotated
                  bibliography},
  journal      = {International Journal on Forecasting},
  volume       = 5,
  pages        = {559--583},
}

@INPROCEEDINGS{costa95bayesian,
  author       = {M. Costa and E. Filippi and E. Pasero},
  title        = {Artificial neural network ensembles: a Bayesian
                  standpoint},
  booktitle    = {Proceedings of the 7th Italian Workshop on Neural
                  Nets},
  pages        = {39-57},
  publisher    = {World Scientific},
  year         = 1995,
  editor       = {M. Marinaro and R. Tagliaferri},
  abstract     = {Pooled estimates naturally arise from the Bayesian
                  framework as the elected approach to both regression
                  and classification problems whenever optimality in
                  the average sense is concerned. In contrast,
                  selection of a single 'best' member appears to be a
                  somewhat crude approximation where some important
                  features of the underlying model are
                  discarded. However, the elegant and powerful full
                  fledged formalism carries a very high computational
                  complexity. Some practical implementations relevant
                  to artificial neural network ensembles are therefore
                  reviewed that make things more tractable while
                  keeping the same generality.},
}

@ARTICLE{hoeting99bma,
  author       = {D. Hoeting, J. A.and Madigan and C.T. Raftery,
                  A.E.and Volinsky},
  title        = {Bayesian model averaging: A tutorial},
  journal      = {Statistical Science},
  year         = 1999,
  volume       = 44,
  number       = 4,
  pages        = {382--417},
  url          = {http://www.stat.washington.edu/www/research/online/hoeting1999.pdf},
  url          = {http://citeseer.nj.nec.com/context/1052498/0},
}

@ARTICLE{darwenyao97,
  author       = {Paul J. Darwen and Xin Yao},
  title        = {Speciation as Automatic Categorical Modularization},
  journal      = {{IEEE} {T}rans. on {E}volutionary {C}omputation},
  volume       = 1,
  number       = 2,
  pages        = {100--108},
  year         = 1997,
}

@TECHREPORT{darwenyao95,
  year         = 1995,
  author       = {Paul Darwen and Xin Yao},
  institution  = {University of New South Wales},
  number       = {CS 8/95},
  title        = {How Good is Fitness Sharing with a Scaling Function},
  month        = {April},
}

@INCOLLECTION{darwenyao96,
  publisher    = {Springer-Verlag},
  author       = {Paul Darwen and Xin Yao},
  title        = {Every niching method has its niche: fitness sharing
                  and implicit sharing compared},
  year         = 1996 ,
  booktitle    = {Proc. of Parallel Problem Solving from Nature (PPSN)
                  IV - Lecture Notes in Computer Science 1141},
}

@ARTICLE{deb99multiobjective,
  author       = {Kalyanmoy Deb},
  title        = {Multi-objective Genetic Algorithms: Problem
                  Difficulties and Construction of Test Problems},
  journal      = {Evolutionary Computation},
  volume       = 7,
  number       = 3,
  pages        = {205-230},
  year         = 1999,
}

@ARTICLE{devroye79distribution,
  author       = {L. Devroye and T. Wagner},
  title        = {Distribution-free Performance Bounds for Potential
                  Function Rules},
  journal      = {IEEE Transactions on Information Theory},
  year         = 1979,
  volume       = 25,
  number       = 5,
  pages        = {601--604},
}

@INPROCEEDINGS{dietterichbakiri91:errorcorrecting,
  author       = {T. G. Dietterich and G. Bakiri},
  title        = {Error-correcting output codes: a general method for
                  improving multiclass inductive learning programs},
  booktitle    = {Proceedings of the Ninth {AAAI} National Conference
                  on Artificial Intelligence},
  publisher    = {AAAI Press},
  address      = {Menlo Park, CA},
  editor       = {Dean, T. L. and McKeown, K.},
  pages        = {572--577},
  year         = 1991,
}

@ARTICLE{dietterich98approximate,
  author       = {Thomas G. Dietterich},
  title        = {Approximate Statistical Test For Comparing
                  Supervised Classification Learning Algorithms},
  journal      = {Neural Computation},
  volume       = 10,
  number       = 7,
  pages        = {1895-1923},
  year         = 1998,
  url          = {citeseer.nj.nec.com/dietterich98approximate.html},
  abstract     = {This paper reviews five approximate statistical
                  tests for determining whether one learning algorithm
                  out-performs another on a particular learning
                  task. These tests are compared experimentally to
                  determine their probability of incorrectly detecting
                  a difference when no difference exists (type I
                  error). Two widely-used statistical tests are shown
                  to have high probability of Type I error in certain
                  situations and should never be used. These tests are
                  (a) a test for the difference of two proportions and
                  (b) a paired-differences t test based on taking
                  several random train/test splits. A third test, a
                  paired-differences t test based on 10-fold
                  cross-validation, exhibits somewhat elevated
                  probability of Type I error. A fourth test,
                  McNemar's test, is shown to have low Type I
                  error. The fifth test is a new test, 5x2cv, based on
                  5 iterations of 2-fold cross-validation. Experiments
                  show that this test also has acceptable Type I
                  error.},
}

@INPROCEEDINGS{domingos97,
  title        = {Why Does Bagging Work? {A} Bayesian Account and its
                  Implications},
  author       = {P.~Domingos},
  pages        = 155,
  booktitle    = {Proceedings of the Third International Conference on
                  Knowledge Discovery and Data Mining ({KDD}-97)},
  year         = 1997,
  editor       = {David Heckerman and Heikki Mannila and Daryl
                  Pregibon and Ramasamy Uthurusamy},
  publisher    = {AAAI Press},
  url          = {http://www.boosting.org/papers/Dom97.ps.gz},
  ps           = {http://www.boosting.org/papers/Dom97.ps},
  psgz         = {http://www.boosting.org/papers/Dom97.ps.gz},
  pdf          = {http://www.boosting.org/papers/Dom97.pdf},
}

@INPROCEEDINGS{domingos98occams,
  author       = {Pedro Domingos},
  title        = {Occam's Two Razors: The Sharp and the Blunt},
  booktitle    = {Proceedings of the Fourth International Conference
                  on Knowledge Discovery and Data Mining},
  publisher    = {AAAI Press},
  year         = 1998,
}

@ARTICLE{drucker94boosting,
  author       = {Harris Drucker and Corinna Cortes and L. D. Jackel
                  and Yann LeCun and Vladimir Vapnik},
  title        = {Boosting and Other Ensemble Methods},
  type         = {Letter},
  journal      = {Neural Computation},
  volume       = 6,
  number       = 6,
  pages        = {1289--1301},
  year         = 1994,
  abstract     = {Compares the performance of three types of neural
                  network-based ensemble techniques to that of a
                  single neural network. The ensemble algorithms are
                  two versions of boosting and committees of neural
                  networks trained independently. For each of the four
                  algorithms, we experimentally determine the test and
                  training error curves in an optical character
                  recognition (OCR) problem as both a function of
                  training set size and computational cost, using
                  three architectures. We show that a single machine
                  is best for small training set sizes, while for
                  large training set sizes, some version of boosting
                  is best. However, for a given computational cost,
                  boosting is always best. Furthermore, we show a
                  surprising result for the original boosting
                  algorithm, namely that as the training set size
                  increases, the training error decreases until it
                  asymptotes to the test error rate. This has
                  potential implications in the search for better
                  training algorithms. },
}

@INCOLLECTION{edelmanintrator97,
  author       = {S. Edelman and N. Intrator},
  title        = {Learning as Extraction of Low-Dimensional
                  Representations},
  booktitle    = {Mechanisms of Perceptual Learning},
  publisher    = {Academic Press},
  year         = 1997,
  editor       = {D. Medlin, R. Goldstone, P. Schyns},
}

@BOOK{efron93bootstrap,
  author       = {B. Efron and R. Tibshirani},
  title        = {An Introduction to the Bootstrap},
  publisher    = {Chapman and Hall},
  year         = 1993,
}

@ARTICLE{elkan1997ban,
  title        = {{Boosting and Naive Bayesian Learning}},
  author       = {Elkan, C.},
  journal      = {proceeding of KDD-97, New Port beach, CA},
  year         = 1997
}

@INPROCEEDINGS{fan99adacost,
  author       = {Wei Fan and Salvatore J. Stolfo and Junxin Zhang and
                  Philip K. Chan},
  title        = {Ada{C}ost: misclassification cost-sensitive
                  boosting},
  booktitle    = {Proc. 16th International Conf. on Machine Learning},
  publisher    = {Morgan Kaufmann, San Francisco, CA},
  pages        = {97--105},
  year         = 1999,
  url          = {http://citeseer.nj.nec.com/fan99adacost.html},
}

@ARTICLE{feder94entropy,
  author       = {Meir Feder and Neri Merhav},
  title        = {Relations between entropy and error probability},
  journal      = {IEEE Transactions on Information Theory},
  volume       = 40,
  number       = 1,
  year         = 1994,
  pages        = 259,
}

@INPROCEEDINGS{feraud98ensemblemodular,
  author       = {Rapha{\"e}l Feraud and Olivier Bernier},
  title        = {Ensemble and Modular Approaches for Face Detection:
                  {A} Comparison},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 10,
  year         = 1998,
  publisher    = {The {MIT} Press},
  editor       = {Michael I. Jordan and Michael J. Kearns and Sara
                  A. Solla},
}

@ARTICLE{freund99short,
  author       = {Y. Freund and R. Schapire},
  title        = {A short introduction to boosting},
  journal      = {Journal of Japanese Society for Artificial
                  Intelligence},
  year         = 1999,
  pages        = {771--780},
  volume       = 14,
  number       = 5,
  url          = {http://citeseer.nj.nec.com/freund99short.html},
}

@INPROCEEDINGS{freundschapire96experiments,
  author       = {Yoav Freund and Robert E. Schapire},
  title        = {Experiments with a new boosting algorithm},
  booktitle    = {Proceedings of the 13th International Conference on
                  Machine Learning},
  publisher    = {Morgan Kaufmann},
  year         = 1996,
  pages        = {148--156},
  abstract     = {In an earlier paper, we introduced a new 'boosting'
                  algorithm called AdaBoost which, theoretically, can
                  be used to significantly reduce the error of any
                  learning algorithm that consistently generates
                  classifiers whose performance is a little better
                  than random guessing. We also introduced the related
                  notion of a 'pseudo-loss' which is a method for
                  forcing a learning algorithm of multi-label concepts
                  to concentrate on the labels that are hardest to
                  discriminate. In this paper, we describe experiments
                  we carried out to assess how well AdaBoost with and
                  without pseudo-loss, performs on real learning
                  problems. We performed two sets of experiments. The
                  first set compared boosting to Breiman's 'bagging'
                  method when used to aggregate various classifiers
                  (including decision trees and single attribute-value
                  tests). We compared the performance of the two
                  methods on a collection of machine-learning
                  benchmarks. In the second set of experiments, we
                  studied in more detail the performance of boosting
                  using a nearest-neighbor classifier on an OCR
                  problem.},
}

@TECHREPORT{friedman99bagging,
  author       = {J. Friedman and P. Hall},
  title        = {On Bagging and Nonlinear Estimation - available
                  online at
                  http://citeseer.nj.nec.com/friedman99bagging.html},
  institution  = {Stanford University},
  year         = 1999,
  url          = {http://citeseer.nj.nec.com/friedman99bagging.html},
}

@ARTICLE{friedman91mars,
  author       = {J.H. Friedman},
  title        = {Multivariate Adaptive Regression Splines},
  journal      = {Annals of Statistics},
  year         = 1991,
  volume       = 19,
  pages        = {1--141},
}

@TECHREPORT{friedman96,
  author       = {J.H. Friedman},
  title        = {Bias, Variance, 0-1 Loss and the Curse of
                  Dimensionality},
  institution  = {Stanford University},
  year         = 1996,
}

@ARTICLE{geman92,
  author       = {S. Geman and E. Bienenstock and R. Doursat},
  title        = {Neural Networks and the Bias/Variance Dilemma},
  type         = {Letter},
  journal      = {Neural Computation},
  volume       = 4,
  number       = 1,
  pages        = {1--58},
  year         = 1992,
  abstract     = {Feedforward neural networks trained by error
                  backpropagation are examples of nonparametric
                  regression estimators. We present a tutorial on
                  nonparametric inference and its relation to neural
                  networks, and we use the statistical viewpoint to
                  highlight strengths and weaknesses of neural
                  models. We illustrate the main points with some
                  recognition. experiments involving artificial data
                  as well as handwritten numerals. In way of
                  conclusion, we suggest that current-generation
                  feedforward neural networks are largely inadequate
                  for difficult problems in machine perception and
                  machine learning, regardless of
                  parallel-versus-serial hardware or other
                  implementation issues. Furthermore, we suggest that
                  the fundamental challenges in neural modeling are
                  about representation rather than learning per
                  se. This last point is supported by additional
                  experiments with handwritten numerals.},
}

@ARTICLE{girosi95regularization,
  author       = {Federico Girosi and Michael Jones and Tomaso Poggio},
  title        = {Regularization Theory and Neural Networks
                  Architectures},
  journal      = {Neural Computation},
  volume       = 7,
  number       = 2,
  pages        = {219--269},
  year         = 1995,
  url          = {http://citeseer.nj.nec.com/girosi95regularization.html},
}

@ARTICLE{granger89combining,
  pages        = {167--174},
  author       = {C.W.J. Granger},
  year         = 1989,
  journal      = {Journal of Forecasting},
  volume       = 8,
  title        = {Combining Forecasts -- Twenty Years Later},
}

@INPROCEEDINGS{guerrasalcedo99genetic,
  author       = {Cesar Guerra-Salcedo and Darrell Whitley},
  title        = {Genetic Approach to Feature Selection for Ensemble
                  Creation},
  booktitle    = {Proceedings of the Genetic and Evolutionary
                  Computation Conference},
  volume       = 1,
  month        = {13-17},
  publisher    = {Morgan Kaufmann},
  address      = {Orlando, Florida, USA},
  editor       = {Wolfgang Banzhaf and Jason Daida and Agoston
                  E. Eiben and Max H. Garzon and Vasant Honavar and
                  Mark Jakiela and Robert E. Smith},
  isbn         = {1-55860-611-4},
  pages        = {236--243},
  year         = 1999,
  url          = {citeseer.ist.psu.edu/531553.html},
}

@ARTICLE{hansen99combining,
  author       = {J.V. Hansen},
  title        = {Combining Predictors: Comparison of Five Meta
                  Machine Learning Methods},
  journal      = {Information Sciences},
  volume       = 119,
  number       = {1-2},
  pages        = {91-105},
  year         = 1999,
}

@ARTICLE{hansensalamon90,
  volume       = 12,
  number       = 10,
  title        = {Neural Network Ensembles},
  author       = {Lars Kai Hansen and Peter Salamon},
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year         = 1990,
  pages        = {993-1001},
}

@MISC{haselsteiner99dynamic,
  author       = {E. Haselsteiner},
  title        = {Dynamic targets - adapting supervised learning to
                  time series classification},
  text         = {In Proceedings of the International Joint Conference
                  on Neural Networks IJCNN'99, Washington D.C., IEEE
                  Press.},
  year         = 1999,
}

@PHDTHESIS{hashem93thesis,
  author       = {Sherif Hashem},
  title        = {{O}ptimal {L}inear {C}ombinations of {N}eural
                  {N}etworks},
  school       = {School of Industrial Engineering, University of
                  Purdue},
  year         = 1993,
}

@ARTICLE{hashem97optimal,
  author       = {Sherif Hashem},
  title        = {{O}ptimal {L}inear {C}ombinations of {N}eural
                  {N}etworks},
  journal      = {Neural Networks},
  volume       = 10,
  number       = 4,
  month        = {August},
  pages        = {599-614},
  year         = 1997,
}

@ARTICLE{hashem95optimal,
  author       = {Sherif Hashem and Bruce Schmeiser},
  title        = {Improving Model Accuracy Using Optimal Linear
                  Combinations of Trained Neural Networks},
  journal      = {IEEE Transactions on Neural Networks},
  type         = {Letter},
  year         = 1995,
  volume       = 6,
  number       = 3,
  pages        = {792--794},
  month        = may,
  abstract     = {Neural network (NN) based modeling often requires
                  trying multiple networks with different
                  architectures and training parameters in order to
                  achieve an acceptable model accuracy. Typically,
                  only one of the trained networks is selected as
                  'best' and the rest are discarded. The authors
                  propose using optimal linear combinations (OLC's) of
                  the corresponding outputs on a set of NN's as an
                  alternative to using a single network. Modeling
                  accuracy is measured by mean squared error (MSE)
                  with respect to the distribution of random
                  inputs. Optimality is defined by minimizing the MSE,
                  with the resultant combination referred to as
                  MSE-OLC. The authors formulate the MSE-OLC problem
                  for trained NN's and derive two closed-form
                  expressions for the optimal combination-weights. An
                  example that illustrates significant improvement in
                  model accuracy as a result of using MSE-OLC's of the
                  trained networks is included.},
}

@ARTICLE{ho98subspaces,
  author       = {T.K. Ho},
  title        = {The Random Subspace Method for Constructing Decision
                  Forests},
  journal      = {IEEE Trans. on Pattern Analysis and Machine
                  Intelligence},
  year         = 1998,
  volume       = 20,
  number       = 8,
  pages        = {832--844},
  month        = {August},
}

@ARTICLE{ho94mcs,
  author       = {Tin Kam Ho and Jonathan J.~Hull and Sargur
                  N.~Srihari},
  title        = {Decision Combination in Multiple Classifier Systems},
  journal      = {Pattern Analysis and Machine Intelligence},
  year         = 1994,
  volume       = 16,
  number       = 1,
  pages        = {66--75},
  month        = {January},
}

@INPROCEEDINGS{huang94nn,
  author       = {Y S Huang and C Y Suen},
  title        = {A Method of Combining Multiple Classifiers - A
                  Neural Network Approach},
  booktitle    = {Proceedings of the 12th International Conference on
                  Pattern Recognition and Computer Vision},
  year         = 1994,
  pages        = {473-475},
  address      = {Jerusalem, Israel},
}

@ARTICLE{huang95numerals,
  author       = {Y.S.~Huang and C.Y.~Suen},
  title        = {A Method of Combining Multiple Experts for the
                  Recognition of Unconstrained Handwritten Numerals},
  journal      = {Pattern Analysis and Machine Intelligence},
  year         = 1995,
  volume       = 17,
  number       = 1,
  pages        = {90--94},
  month        = {January},
}

@ARTICLE{husmeier98overfitting,
  author       = {Husmeier D., Althoefer K.},
  title        = {Modelling conditional probabilities with network
                  committees: how overfitting can be useful},
  journal      = {Neural Network World},
  year         = 1998,
  volume       = 8,
  numver       = 4,
  pages        = {417--439},
}

@ARTICLE{intratoredelman96,
  author       = {N. Intrator and S. Edelman},
  title        = {Making a Low-Dimensional Representation Suitable for
                  Diverse Tasks},
  journal      = {Connection Science : Special Issue on Reuse of
                  Neural Networks Through Transfer},
  year         = 1996,
  volume       = 8,
  number       = 2,
  pages        = {205-224},
}

@ARTICLE{jacobsjordan91,
  author       = {R. A. Jacobs and M. I. Jordan and S. J. Nowlan and
                  G. E. Hinton},
  title        = {Adaptive Mixtures of Local Experts},
  type         = {Letter},
  journal      = {Neural Computation},
  volume       = 3,
  number       = 1,
  pages        = {79--87},
  year         = 1991,
  abstract     = {We present a new supervised learning procedure for
                  systems composed of many separate networks, each of
                  which learns to handle a subset of the complete set
                  of training cases. The new procedure can be viewed
                  either as a modular version of a multilayer
                  supervised network, or as an associative version of
                  competitive learning. It therefore provides a new
                  link beetween these two apparently different
                  approaches. We demonstrate that the learning
                  procedure divides up a voewel discrimination task
                  into appropriate subtasks, each of which can be
                  solved by a very simple expert network.},
}

@INBOOK{jacobs99mixturesofx,
  author       = {R.A. Jacobs and M.A. Tanner},
  chapter      = {Mixtures of {X}},
  editor       = {A.J. Sharkey},
  title        = {Combining Articial Neural Nets},
  publisher    = {Springer-Verlag, London},
  year         = 1999,
}

@ARTICLE{jacobs97analyses,
  pages        = {369--383},
  author       = {Robert Jacobs},
  year         = 1997,
  journal      = {Neural Computation},
  volume       = 9,
  title        = {Bias-Variance Analyses of Mixture-of-Experts
                  Architectures},
}

@ARTICLE{jacobsjordanbarto91,
  pages        = {219-250},
  title        = {Task decomposition through competition in a modular
                  connectionist architecture - the What and Where
                  vision tasks},
  volume       = 15,
  year         = 1991,
  author       = {Robert A. Jacobs and Michael I. Jordan and Andrew
                  G. Barto},
  journal      = {Cognitive Science},
}

@INPROCEEDINGS{jelonek96replicated,
  author       = {J. Jelonek},
  title        = {Generalization Capability of Homogeneous Voting
                  Classifier Based on Partially Replicated Data},
  series       = {Integrating Multiple Learned Models for Improving
                  and Scaling Machine Learning Algorithms Workshop},
  booktitle    = {AAAI'96},
  year         = 1996,
  note         = {Portland, OR},
}

@ARTICLE{jima97weak,
  author       = {C. Ji and S. Ma},
  title        = {Combinations of Weak Classifiers},
  journal      = {IEEE Trans. on Neural Networks, Special Issue on
                  Neural Networks and Pattern Recognition},
  volume       = 8,
  year         = 1997,
  month        = {January},
  pages        = {32--42},
}

@INPROCEEDINGS{jimenez98dynamically,
  author       = {D. Jimenez and N. Walsh},
  title        = {Dynamically weighted ensemble neural networks for
                  classification},
  booktitle    = {Proceedings of the International Joint Conference on
                  Neural Networks},
  year         = 1998,
  url          = {http://citeseer.nj.nec.com/jimenez98dynamically.html},
}

@ARTICLE{jordan99introduction,
  author       = {Michael I. Jordan and Zoubin Ghahramani and Tommi
                  Jaakkola and Lawrence K. Saul},
  title        = {An Introduction to Variational Methods for Graphical
                  Models},
  journal      = {Machine Learning},
  volume       = 37,
  number       = 2,
  pages        = {183-233},
  year         = 1999,
}

@ARTICLE{jordan94hierarchical,
  author       = {Michael I. Jordan and Robert A. Jacobs},
  title        = {Hierarchical Mixtures of Experts and the {EM}
                  Algorithm},
  journal      = {Neural Computation},
  year         = 1994,
  volume       = 6,
  pages        = {181--214},
  class        = {nn, learning},
  abstract     = {We present a tree-structured architecture for
                  supervised learning. The statistical model
                  underlying the architecture is a hierarchical
                  mixture model in which both the mixture coefficients
                  and the mixture components are generalized linear
                  models (GLIM). Learning is treated as a maximum
                  likelihood problem; in particular, we present an
                  Expectation-Maximization (EM) algorithm for
                  adjusting the parameters of the architecture. We
                  develop an online learning algorithm in which the
                  parameters are updated incrementally. Comparative
                  simulation results are presented in the robot
                  dynamics domain.},
}

@INPROCEEDINGS{kang95learning,
  author       = {Kukjin Kang and Jong-Hoon Oh},
  title        = {Learning by a Population of Perceptrons},
  booktitle    = {Computational Learning Theory},
  pages        = {297-300},
  year         = 1995,
  url          = {http://citeseer.nj.nec.com/kang97learning.html},
}

@INPROCEEDINGS{kang97statistical,
  author       = {Kukjin Kang and Jong-Hoon Oh},
  title        = {Statistical Mechanics of the Mixture of Experts},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 9,
  publisher    = {The {MIT} Press},
  editor       = {Michael C. Mozer and Michael I. Jordan and Thomas
                  Petsche},
  pages        = 183,
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/kang97statistical.html},
}

@ARTICLE{kleinberg90stochastic,
  author       = {E. M. Kleinberg},
  title        = {Stochastic Discrimination},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  volume       = 1,
  year         = 1990,
}

@INPROCEEDINGS{kleinberg93pattern,
  author       = {E. M. Kleinberg and T. K. Ho},
  title        = {Pattern Recognition by Stochastic Modeling},
  booktitle    = {Proc. of the 3rd Workshop on Frontiers in
                  Handwriting Recognition},
  month        = {May},
  address      = {Buffalo, New York},
  pages        = {175--183},
  year         = 1993,
}

@INPROCEEDINGS{kohavi96bias,
  author       = {Ron Kohavi and David H. Wolpert},
  title        = {Bias Plus Variance Decomposition for Zero-One Loss
                  Functions},
  booktitle    = {Machine Learning: Proceedings of the Thirteenth
                  International Conference},
  publisher    = {Morgan Kaufmann},
  editor       = {Lorenza Saitta},
  pages        = {275--283},
  year         = 1996,
  abstract     = {We present a bias-variance decomposition of expected
                  misclassification rate, the most commonly used loss
                  function in supervised classification learning. The
                  bias-variance decomposition for quadratic loss
                  functions is well known and serves as an important
                  tool for analyzing learning algorithms, yet no
                  decomposition was offered for the more commonly used
                  zero-one (misclassification) loss functions until
                  the work of Kong and Dietterich (1995) and Breiman
                  (1996). Their decomposition suffers from some major
                  shortcomings though (e.g., potentially negative
                  variance), which our decomposition avoids. We show
                  that, in practice, the naive frequency-based
                  estimation of the decomposition terms is by itself
                  biased and show how to correct for this bias. We
                  illustrate the decomposition on various algorithms
                  and datasets from the UCI repository.},
}

@INPROCEEDINGS{kongdietterich95correcting,
  author       = {E. B. Kong and T. G. Dietterich},
  title        = {Error-Correcting Output Coding Corrects Bias and
                  Variance},
  booktitle    = {Proceedings of the 12th International Conference on
                  Machine Learning},
  publisher    = {Morgan Kaufmann},
  year         = 1995,
  pages        = {313--321},
  comment      = {Tahoe Cite, CA},
  abstract     = {Previous research has shown that a technique called
                  error-correcting output coding (ECOC) can
                  dramatically improve the classification accuracy of
                  supervised learning algorithms that learn to
                  classify data points into one of k>>2 classes. This
                  paper presents an investigation of why the ECOC
                  technique works, particularly when employed with
                  decision-tree learning algorithms. It shows that the
                  ECOC method, like any form of voting or committee,
                  can reduce the variance of the learning
                  algorithm. Furthermore, unlike methods that simply
                  combine multiple runs of the same learning
                  algorithm, ECOC can correct errors caused by the
                  bias of the learning algorithm. Experiments show
                  that this bias correction ability relies on the
                  non-local behavior of C4.5.},
}

@ARTICLE{kovalishyn98cascade,
  author       = {Kovalishyn, V. V. and Tetko, I. V. and Luik,
                  A. I. and Kholodovych, V. V. and Villa, A. E. P. and
                  Livingstone, D. J.},
  title        = {Neural network studies. 3. Variable selection in the
                  cascade-correlation learning architecture},
  journal      = {Journal of Chemical Information & Computer Sciences},
  volume       = 38,
  number       = 4,
  pages        = {651-659},
  abstract     = {Pruning methods for feed-forward artificial neural
                  networks trained by the cascade-correlation learning
                  algorithm are proposed. The cascade-correlation
                  algorithm starts with a small network and
                  dynamically adds new nodes until the analyzed
                  problem has been solved. This feature of the
                  algorithm removes the requirement to predefine the
                  architecture of the neural network prior to network
                  training. The developed pruning methods are used to
                  estimate the importance of large sets of initial
                  variables for quantitative structure-activity
                  relationship studies and simulated data sets. The
                  calculated results are compared with the performance
                  of fixed-size back-propagation neural networks and
                  multiple regression analysis and are carefully
                  validated using different training/test set
                  protocols, such as leave-one-out and full
                  cross-validation procedures. The results suggest
                  that the pruning methods can be successfully used to
                  optimize the set of variables for the
                  cascade-correlation learning algorithm neural
                  networks. The use of variables selected by the
                  elaborated methods provides an improvement of neural
                  network prediction ability compared to that
                  calculated using the unpruned sets of variables.},
  year         = 1998,
}

@BOOK{koza92book,
  author       = {John R. Koza},
  title        = {Genetic Programming: On the Programming of Computers
                  by Means of Natural Selection},
  publisher    = {MIT Press},
  year         = 1992,
  isbn         = {0-262-11170-5},
}

@ARTICLE{kroghsollich97:statistical_mechanics,
  title        = {Statistical mechanics of ensemble learning},
  author       = {A. Krogh and P. Sollich},
  journal      = {Physical Review E},
  year         = 1997,
  volume       = 55,
  number       = {1PtB},
  pages        = {811--825},
  abstract     = {Within the context of learning a rule from examples,
                  we study the general characteristics of learning,
                  with ensembles. The generalization performance
                  achieved by a simple model ensemble of linear
                  students is calculated exactly in the thermodynamic
                  limit of a large number of input components and
                  shows a surprisingly rich behavior. Our main
                  findings are the following. For learning in large
                  ensembles, it is advantageous to use
                  underregularized students, which actually overfit
                  the training data. Globally optimal generalization
                  performance can be obtained by choosing the training
                  set sizes of the students optimally. For smaller
                  ensembles, optimization of the ensemble weights can
                  yield significant improvements in ensemble
                  generalization performance, in particular if the
                  individual students are subject to noise in the
                  training process. Choosing students with a wide
                  range of regularization parameters makes this
                  improvement robust against changes in the unknown
                  level of corruption of the training data.},
}

@ARTICLE{kroghvedelsby95,
  author       = {A. Krogh and J. Vedelsby},
  title        = {Neural Network Ensembles, Cross Validation, and
                  Active Learning},
  journal      = {NIPS},
  year         = 1995,
  volume       = 7,
  pages        = {231--238},
  abstract     = {The learning of continuous-valued functions using
                  neural network ensembles (committees) can give
                  improved accuracy, a reliable estimation of the
                  generalization error, and active learning. The
                  ambiguity is defined as the variation of the output
                  of ensemble members averaged over unlabeled data, so
                  it quantifies the disagreement among the
                  networks. We discuss how to use the ambiguity in
                  combination with cross-validation to give a reliable
                  estimate of the ensemble generalization error, and
                  how this type of ensemble cross-validation can
                  sometimes improve performance. We show how to
                  estimate the optimal weights of the ensemble members
                  using unlabeled data. By a generalization of
                  query-by-committee, we show how the ambiguity can be
                  used to select new training data to be labeled in an
                  active learning scheme.},
}

@BOOK{laplace1818,
  author       = {P. S. de Laplace},
  title        = {Deuxieme supplement a la theorie analytique des
                  probabilites},
  publisher    = {Paris, Gauthier-Villars},
  year         = 1818,
  note         = {Reprinted (1847) in Oeuvres Completes de Laplace,
                  vol. 7},
  annote       = {First known observation that combining probabilistic classifiers can help.},
}

@ARTICLE{leblanc96combining,
  author       = {Michael LeBlanc and Robert Tibshirani},
  title        = {Combining Estimates in Regression and
                  Classification},
  journal      = {Journal of the American Statistical Association},
  volume       = 91,
  number       = 436,
  pages        = 1641,
  year         = 1996,
  url          = {http://citeseer.nj.nec.com/leblanc93combining.html},
}

@ARTICLE{lehto95,
  volume       = 7,
  author       = {Mikko Lehtokangas and Jukka Saarinen and Kimmo Kaski
                  and Pentti Huuhtanen},
  year         = 1995,
  pages        = {983-999},
  number       = 5,
  title        = {Initializing Weights of a Multilayer Perceptron
                  Network by Using the Orthogonal Least Squares
                  Algorithm},
  journal      = {Neural Computation},
}

@ARTICLE{liaomoody99,
  author       = {Yuansong Liao and John Moody},
  title        = {Constructing Heterogeneous Committees Using Input
                  Feature Grouping},
  journal      = {Advances in Neural Information Processing Systems},
  year         = 1999,
  volume       = 12,
}

@PHDTHESIS{liu98thesis,
  author       = {Y. Liu},
  title        = {Negative Correlation Learning and Evolutionary
                  Neural Network Ensembles},
  school       = {University College, The University of New South
                  Wales, Australian Defence Force Academy, Canberra,
                  Australia},
  year         = 1998,
}

@ARTICLE{liuyao97negatively,
  title        = {Negatively Correlated Neural Networks can Produce
                  Best Ensembles},
  author       = {Y. Liu and X. Yao},
  number       = {3/4},
  year         = 1997,
  journal      = {Australian Journal of Intelligent Information
                  Processing Systems},
  pages        = {176--185},
  volume       = 4,
}

@INPROCEEDINGS{liuyao98towards,
  month        = {February},
  author       = {Yong Liu and Xin Yao},
  year         = 1998 ,
  booktitle    = {Proceedings of International Symposium on Artificial
                  Life and Robotics (AROB)},
  pages        = {265-268},
  title        = {Towards Designing Neural Network Ensembles by
                  Evolution},
}

@ARTICLE{liuyao99:ensemblelearningvia,
  author       = {Yong Liu and Xin Yao},
  title        = {Ensemble learning via negative correlation},
  journal      = {Neural Networks},
  volume       = 12,
  number       = 10,
  pages        = {1399--1404},
  year         = 1999,
}

@INPROCEEDINGS{maclinshavlik95weights,
  author       = {R. Maclin and J. W. Shavlik},
  title        = {Combining the Predictions of Multiple Classifiers:
                  Using Competitive Learning to Initialize Neural
                  Networks},
  booktitle    = {Proceedings of the 14th International Joint
                  Conference on Artificial Intelligence, Montreal,
                  Canada},
  year         = 1995,
  pages        = {524-530},
  abstract     = {The primary goal of inductive learning is to
                  generalize well-that is, induce a function that
                  accurately produces the correct output for future
                  inputs. Hansen and Salamon (1990) showed that, under
                  certain assumptions, combining the predictions of
                  several separately trained neural networks will
                  improve generalization. One of their key assumptions
                  is that the individual networks should be
                  independent in the errors they produce. In the
                  standard way of performing backpropagation this
                  assumption may be violated, because the standard
                  procedure is to initialize network weights in the
                  region of weight space near the origin. This means
                  that backpropagation's gradient-descent search may
                  only reach a small subset of the possible local
                  minima. In this paper we present an approach to
                  initializing neural networks that uses competitive
                  learning to intelligently create networks that are
                  originally located far from the origin of weight
                  space, thereby potentially increasing the set of
                  reachable local minima. We report experiments on two
                  real-world datasets where combinations of networks
                  initialized with our method generalize better than
                  combinations of networks initialized the traditional
                  way.},
}

@INPROCEEDINGS{maclin97empirical,
  author       = {Richard Maclin and David Opitz},
  title        = {An Empirical Evaluation of Bagging and Boosting},
  booktitle    = {{AAAI}/{IAAI}},
  pages        = {546-551},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/maclin97empirical.html},
}

@MISC{mak97combining,
  author       = {B. Mak},
  title        = {Combining ANNs to improve phoneme recognition},
  text         = {B. Mak. Combining ANNs to improve phoneme
                  recognition. ICASSP, 4:3253--3256, 1997.},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/mak97combining.html},
}

@INBOOK{mandler88:combining,
  author       = {E. Mandler and J. Schuermann},
  chapter      = {Combining the Classification Results of independent
                  classifiers based on the Dempster/Shafer theory of
                  evidence},
  title        = {Pattern Recognition and Artificial Intelligence},
  editor       = {E.S.Gelsema and L.N.Kanal},
  publisher    = {North Holland, Amsterdam},
  pages        = {381--393},
  year         = 1988,
}

@ARTICLE{mani91portfolio,
  author       = {G. Mani},
  title        = {Lowering variance of decisions by using artificial
                  network portfolios},
  type         = {Letter},
  journal      = {Neural Computation},
  volume       = 3,
  number       = 4,
  pages        = {484--486},
  year         = 1991,
}

@INPROCEEDINGS{margineantu97pruning,
  author       = {Dragos D. Margineantu and Thomas G. Dietterich},
  title        = {Pruning adaptive boosting},
  booktitle    = {Proc. 14th International Conference on Machine
                  Learning},
  publisher    = {Morgan Kaufmann},
  pages        = {211--218},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/margineantu97pruning.html},
}

@ARTICLE{markowitz52,
  title        = {Portfolio Selection},
  author       = {H. Markowitz},
  journal      = {Journal of Finance},
  volume       = 7,
  issue        = 1,
  month        = {March},
  year         = 1952,
  annote       = {Shows that a linear combination (ensemble)
                  of individual regressors obey the
                  bias-variance-covariance decomposition.
                  For this paper, Markowitz received the
                  1990 Nobel Prize for Economics.},
}

@BOOK{mcclellandrumelhart86,
  author       = {J. McClelland and D. Rumelhart},
  title        = {Parallel Distributed Processing},
  publisher    = {MIT Press},
  year         = 1986,
}

@ARTICLE{meir95bias,
  author       = {Ronny Meir},
  title        = {Bias, Variance and the Combination of Least Squares
                  Estimators},
  pages        = {295--302},
  editor       = {G. Tesauro and D. Touretzky and T. Leen},
  volume       = 7,
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = 1995,
  publisher    = {The {MIT} Press},
  abstract     = {We consider the effect of combining several least
                  squares estimators on the expected performance of a
                  regression problem. Computing the exact bias and
                  variance curves as a function of the sample size we
                  are able to quantitatively compare the effect of the
                  combination on the bias and variance separately, and
                  thus on the expected error which is the sum of the
                  two. Our exact calculations, demonstrate that the
                  combination of estimators is particularly useful in
                  the case where the data set is small and noisy and
                  the function to be learned is unrealizable. For
                  large data sets the single estimator produces
                  superior results. Finally, we show that by splitting
                  the data set into several independent parts and
                  training each estimator on a different subset, the
                  performance can in some cases be significantly
                  improved.},
}

@PHDTHESIS{merz98thesis,
  author       = {C.J. Merz},
  title        = {Classification and Regression by Combining Models},
  school       = {University of California, Irvine},
  year         = 1998,
  abstract     = {Two novel methods for combining predictors are
                  introduced in this thesis; one for the task of
                  regression, and the other for the task of
                  classification. The goal of combining the
                  predictions of a set of models is to form an
                  improved predictor. This dissertation demonstrates
                  how a combining scheme can rely on the stability of
                  the consensus opinion and, at the same time,
                  capitalize on the unique contributions of each
                  model. An empirical evaluation reveals that the new
                  methods consistently perform as well or better than
                  existing combining schemes for a variety of
                  prediction problems. The success of these algorithms
                  is explained empirically and analytically by
                  demonstrating how they adhere to a set of
                  theoretical and heuristic guidelines. A byproduct of
                  the empirical investigation is the evidence that
                  existing combining methods fail to satisfy one or
                  more of the guidelines defined. The new combining
                  approaches satisfy these criteria by relying upon
                  Singular Value Decomposition as a tool for filtering
                  out the redundancy and noise in the predictions of
                  the learn models, and for characterizing the areas
                  of the example space where each model is
                  superior. The SVD-based representation used in the
                  new combining methods aids in avoiding sensitivity
                  to correlated predictions without discarding any
                  learned models. Therefore, the unique contributions
                  of each model can still be discovered and
                  exploited. An added advantage of the combining
                  algorithms derived in this dissertation is that they
                  are not limited to models generated by a single
                  algorithm; they may be applied to model sets
                  generated by a diverse collection of machine
                  learning and statistical modeling methods.},
}

@ARTICLE{merz99using,
  author       = {Christopher J. Merz},
  title        = {Using Correspondence Analysis to Combine
                  Classifiers},
  journal      = {Machine Learning},
  volume       = 36,
  number       = {1-2},
  pages        = {33-58},
  year         = 1999,
}

@INPROCEEDINGS{merz97combining,
  author       = {Christopher J. Merz and Michael J. Pazzani},
  title        = {Combining Neural Network Regression Estimates with
                  Regularized Linear Weights},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 9,
  publisher    = {The {MIT} Press},
  editor       = {Michael C. Mozer and Michael I. Jordan and Thomas
                  Petsche},
  pages        = 564,
  year         = 1997,
}

@INBOOK{minsky90multiple,
  author       = {Marvin Minsky},
  editor       = {Patrick H. Winston},
  title        = {Artificial Intelligence at MIT : Expanding
                  Frontiers},
  chapter      = {Logical vs. Analogical or Symbolic vs. Connectionist
                  or Neat vs Scruffy},
  publisher    = {MIT Press},
  year         = 1990,
  volume       = 1,
  annote       = {Minsky comments on the benefits of combining multiple representations.}
}

@TECHREPORT{moerland99dynaboost,
  author       = {P.~Moerland and E.~Mayoraz},
  title        = {DynaBoost: Combining Boosted Hypotheses in a Dynamic
                  Way},
  institution  = {IDIAP},
  year         = 1999,
  number       = {RR 99-09},
  address      = {Switzerland},
  month        = {May},
  url          = {http://www.boosting.org/papers/MoeMay99.pdf},
}

@ARTICLE{montgomeryfriedman93,
  author       = {D.C. Montgomery and D.J. Friedman},
  title        = {Prediction Using Regression Models with
                  Multicollinear Predictor Variables},
  journal      = {IIE Transactions},
  year         = 1993,
  volume       = 25,
  number       = 3,
  pages        = {73--85},
  abstract     = {Linear regression models are widely used for
                  forecasting and prediction of new observations from
                  the underlying modeled process. The article explores
                  the use of regression models in this context when
                  the regressor or predictor variables exhibit
                  multicollinearity, or near-linear dependence. It is
                  shown that multicollinearity can severely impact the
                  predictive performance of a regression model and
                  that biased estimation methods can be an effective
                  countermeasure when multicollinearity is
                  present. Several biased estimation methods are
                  described and evaluated, including a new method for
                  selecting the biasing parameter in ordinary ridge
                  regression. A simulation study is performed to
                  provide some guidelines for the choice of an
                  estimation method. },
}

@INPROCEEDINGS{murata98bias,
  author       = {N. Murata},
  title        = {Bias of estimators and regularization terms},
  booktitle    = {Proceedings of Workshop on Information-Based
                  Induction Sciences},
  pages        = {87--94},
  year         = 1998,
  month        = {July},
  url          = {http://citeseer.nj.nec.com/murata98bias.html},
}

@INBOOK{murphy97exploring,
  author       = {Patrick M. Murphy and Michael J. Pazzani},
  title        = {Exploring the decision forest: an empirical
                  investigation of Occam's razor in decision tree
                  induction},
  booktitle    = {Computational Learning Theory and Natural Learning
                  Systems},
  volume       = {IV: Making Learning Systems Practical},
  publisher    = {MIT Press},
  pages        = {171--187},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/murphy94exploring.html},
}

@BOOK{nilsson65,
  author       = {N.J.Nilsson},
  title        = {Learning Machines: Foundations of Trainable
                  Pattern-Classifying Systems},
  publisher    = {McGraw-Hill},
  year         = 1965,
}

@ARTICLE{naftaly97optimal,
  author       = {Ury Naftaly and Nathan Intrator and David Horn},
  year         = 1997,
  volume       = 8,
  title        = {Optimal Ensemble Averaging of Neural Networks},
  month        = {May},
  number       = 3 ,
  pages        = {283--296},
  journal      = {Network},
}

@INBOOK{ohkang97,
  author       = {Jong-Hoon Oh and Kookjin Kang},
  editor       = {K.Y.M.Wong and D.Yan},
  title        = {Theoretical Aspects of Neural Computation: A
                  Multidisciplinary Perspective},
  chapter      = {Experts or Ensemble? A statistical Mechanics of
                  Multiple Neural Network Approaches},
  publisher    = {Springer, Heidelberg},
  pages        = {81--92},
  year         = 1997,
}

@MISC{opitz99genetic,
  author       = {D. Opitz and J. Shavlik},
  title        = {A genetic algorithm approach for creating neural
                  network ensembles},
  text         = {In A.J.C. Sharkey, editor, Combining Articial Neural
                  Nets, pages 79-99. Springer-Verlag, London, 1999.},
  year         = 1999,
}

@INPROCEEDINGS{opitz99feature,
  author       = {David Opitz},
  title        = {Feature Selection for Ensembles},
  booktitle    = {Proceedings of 16th National Conference on
                  Artificial Intelligence (AAAI)},
  pages        = {379-384},
  year         = 1999,
}

@ARTICLE{opitzmaclin99:popular,
  title        = {Popular Ensemble Methods: An Empirical Study},
  journal      = {Journal of Artificial Intelligence Research},
  volume       = 11 ,
  year         = 1999,
  pages        = {169-198},
  author       = {David Opitz and Richard Maclin},
}

@ARTICLE{opitz96:generating,
  author       = {David W. Opitz and Jude W. Shavlik},
  title        = {Generating Accurate and Diverse Members of a
                  Neural-Network Ensemble},
  pages        = {535--541},
  journal      = {NIPS},
  editor       = {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  volume       = 8,
  year         = 1996,
  publisher    = {The {MIT} Press},
  abstract     = {Neural-network ensembles have been shown to be very
                  accurate classification techniques. Previous work
                  has shown that an effective ensemble should consist
                  of networks that are not only highly correct, but
                  ones that make their errors on different parts of
                  the input space as well. Most existing techniques,
                  however, only indirectly address the problem of
                  creating such a set of networks. In this paper we
                  present a technique called ADDEMUP that uses genetic
                  algorithms to directly search for an accurate and
                  diverse set of trained networks. ADDEMUP works by
                  first creating an initial population, then uses
                  genetic operators to continually create new
                  networks, keeping the set of networks that are as
                  accurate as possible while disagreeing with each
                  other as much as possible. Experiments on three DNA
                  problems show that ADDEMUP is able to generate a set
                  of trained networks that is more accurate than
                  several existing approaches. Experiments also show
                  that ADDEMUP is able to effectively incorporate
                  prior knowledge, if available, to improve the
                  quality of its ensemble.},
}

@INPROCEEDINGS{ormoneit96improved,
  author       = {Dirk Ormoneit and Volker Tresp},
  title        = {Improved Gaussian Mixture Density Estimates Using
                  Bayesian Penalty Terms and Network Averaging},
  booktitle    = {Advances in Neural Information Processing Systems},
  volume       = 8,
  publisher    = {The {MIT} Press},
  editor       = {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  pages        = {542--548},
  year         = 1996,
}

@MISC{orr95regularisation,
  author       = {M. Orr},
  title        = {Regularisation in the Selection of Radial Basis
                  Function Centres},
  text         = {M. J. L. Orr, Regularisation in the Selection of
                  Radial Basis Function Centres, Neural Computation,
                  Vol. 7, pp. 606-623, 1995.},
  year         = 1995,
  url          = {citeseer.nj.nec.com/orr95regularisation.html},
}

@TECHREPORT{tumer99decimated,
  author       = {N. Oza and K. Tumer},
  title        = {Dimensionality Reduction through Classifier
                  Ensembles},
  institution  = {NASA Ames Labs},
  year         = 1999,
  number       = {NASA-ARC-IC-1999-126},
}

@ARTICLE{partridge96network,
  author       = {D. Partridge},
  title        = {Network Generalization Differences Quantified},
  journal      = {Neural Networks},
  volume       = 9,
  number       = 2,
  pages        = {263--271},
  year         = 1996,
  url          = {http://citeseer.nj.nec.com/partridge94network.html},
}

@ARTICLE{partridge96:engineering,
  author       = {D. Partridge and W. B. Yates},
  title        = {Engineering Multiversion Neural-Net Systems},
  journal      = {Neural Computation},
  volume       = 8,
  number       = 4,
  pages        = {869--893},
  year         = 1996,
}

@INPROCEEDINGS{nips90:Pearlmutter-Rosenfeld,
  author       = {Barak A. Pearlmutter and Ronald Rosenfeld},
  title        = {Chaitin-{K}olmogorov Complexity and Generalization
                  in Neural Networks},
  pages        = {925--931},
  url          = {http://nips.djvuzone.org/djvu/nips03/0925.djvu},
  booktitle    = {Advances in Neural Information Processing Systems 3},
  publisher    = {Morgan Kaufmann},
  year         = 1991,
}

@INCOLLECTION{perronecooper93,
  author       = {M. P. Perrone and L. N. Cooper},
  title        = {When networks disagree: Ensemble methods for hybrid
                  neural networks},
  editor       = {R. J. Mammone},
  booktitle    = {Artificial Neural Networks for Speech and Vision},
  address      = {London},
  pages        = {126--142},
  year         = 1993,
}

@PHDTHESIS{perrone93improving,
  author       = {M.P. Perrone},
  title        = {Improving Regression Estimation: Averaging Methods
                  for Variance Reduction with Extensions to General
                  Convex Measure Optimization},
  year         = 1993,
  school       = {Brown University, Institute for Brain and Neural
                  Systems},
}

@ARTICLE{prechelt98automatic,
  author       = {Lutz Prechelt},
  title        = {Automatic early stopping using cross validation:
                  quantifying the criteria},
  journal      = {Neural Networks},
  volume       = 11,
  number       = 4,
  pages        = {761--767},
  year         = 1998,
  url          = {http://citeseer.nj.nec.com/prechelt98automatic.html},
}

@INPROCEEDINGS{powalka95,
  author       = {R Powalka, N Sherkat, R Whitrow},
  title        = {Multiple recognizer combination topologies},
  booktitle    = {Proceedings of the Seventh Biannual Conference of
                  the International Graphonomics Society},
  pages        = {128--129},
  year         = 1995,
  month        = {August},
  organization = {University of Western Ontario},
  note         = {ISBN 0-921121-14-8},
}

@ARTICLE{rahman98,
  author       = {A.F.R. Rahman and M.C. Fairhurst},
  title        = {An evaluation of multi-expert configurations for the
                  recognition of handwritten numerals},
  journal      = {Pattern Recognition},
  year         = 1998,
  number       = 9,
  pages        = {1255--1273},
}

@ARTICLE{intratorraviv96:noise,
  volume       = 8,
  pages        = {355-372},
  author       = {Yuval Raviv and Nathan Intrator},
  year         = 1996,
  title        = {Bootstrapping with Noise: An Effective
                  Regularisation Technique},
  journal      = {Connection Science},
}

@ARTICLE{rogova94combining,
  author       = {Galina Rogova},
  title        = {Combining the Results of Neural Network Classifiers},
  journal      = {Neural Networks},
  year         = 1994,
  volume       = 7,
  number       = 5,
  pages        = {777--781},
  class        = {nn, learning},
}

@ARTICLE{rosen96:decorrelated,
  volume       = 8,
  number       = {3 and 4},
  title        = {Ensemble Learning using Decorrelated Neural
                  Networks},
  author       = {B.E. Rosen},
  journal      = {Connection Science - Special Issue on Combining
                  Artificial Neural Networks: Ensemble Approaches},
  year         = 1996,
  pages        = {373--384},
  abstract     = {We describe a decorrelation network training method
                  for improving the quality of regression learning in
                  'ensemble' neural networks (NNs) that are composed
                  of linear combinations of individual NNs. In this
                  method, individual networks are trained by
                  backpropagation not only to reproduce a desired
                  output, but also to have their errors linearly
                  decorrelated with the other networks. Outputs from
                  the individual networks are then linearly combined
                  to produce the output of the ensemble network. We
                  demonstrate the performances of decorrelated network
                  training on learning the 'three-parity' logic
                  function, a noisy sine function and a
                  one-dimensional non-linear function, and compare the
                  results with the ensemble networks composed of
                  independently trained individual networks (without
                  decorrelation training). Empirical results show that
                  when individual networks are forced to be
                  decorrelated with one another the resulting ensemble
                  NNs have lower mean squared errors than the ensemble
                  networks having independently trained individual
                  networks. This method is particularly applicable
                  when there is insufficient data to train each
                  individual network on disjoint subsets of training
                  patterns.},
}

@ARTICLE{ruck90bayesperceptron,
  author       = {D. W. Ruck and S. K. Rogers and M. Kabrisky and
                  M. E. Oxley and B. W. Suter},
  title        = {The multilayer perceptron ass an approximation to a
                  Bayes optimal discrimant function},
  journal      = {IEEE Transactions on Neural Networks},
  type         = {Letter},
  year         = 1990,
  volume       = 1,
  number       = 4,
  pages        = {296--298},
}

@INPROCEEDINGS{saborin93digit,
  author       = {Michael Saborin and Amar Mitiche and Danny Thomas
                  and George Nagy},
  title        = {Classifier Combination for Hand-Printed Digit
                  Recognition},
  pages        = {163--166},
  booktitle    = {Proceedings of the Second International Conference
                  on Document Analysis and Recognition},
  year         = 1993,
  organization = {IEEE},
  address      = {Japan},
}

@ARTICLE{schapiresinger99confidence,
  author       = {R.E.~Schapire and Y.~Singer},
  title        = {Improved boosting algorithms using confidence-rated
                  predictions},
  journal      = {Machine Learning},
  year         = 1999,
  volume       = 37,
  number       = 3,
  month        = dec,
  pages        = {297-336},
  abstract     = { We describe several improvements to Freund and
                  Schapire's \adaboost\ boosting algorithm,
                  particularly in a setting in which hypotheses may
                  assign confidences to each of their predictions. We
                  give a simplified analysis of AdaBoost in this
                  setting, and we show how this analysis can be used
                  to find improved parameter settings as well as a
                  refined criterion for training weak hypotheses. We
                  give a specific method for assigning confidences to
                  the predictions of decision trees, a method closely
                  related to one used by Quinlan. This method also
                  suggests a technique for growing decision trees
                  which turns out to be identical to one proposed by
                  Kearns and Mansour. We focus next on how to apply
                  the new boosting algorithms to multiclass
                  classification problems, particularly to the
                  multi-label case in which each example may belong to
                  more than one class. We give two boosting methods
                  for this problem. One of these leads to a new method
                  for handling the single-label case which is simpler
                  but as effective as techniques suggested by Freund
                  and Schapire. Finally, we give some experimental
                  results comparing a few of the algorithms discussed
                  in this paper. },
  url          = {http://www.boosting.org/papers/SchSin99b.pdf},
}

@ARTICLE{schapire90strength,
  author       = {Robert E. Schapire},
  title        = {The Strength of Weak Learnability},
  journal      = {Machine Learning},
  volume       = 5,
  pages        = {197-227},
  year         = 1990,
  url          = {http://citeseer.nj.nec.com/schapire90strength.html},
}

@INPROCEEDINGS{schapire99theoretical,
  author       = {Robert E. Schapire},
  title        = {Theoretical Views of Boosting and Applications},
  booktitle    = {Algorithmic Learning Theory, 10th International
                  Conference, {ALT} '99, Tokyo, Japan, December 1999,
                  Proceedings},
  volume       = 1720,
  publisher    = {Springer},
  pages        = {13--25},
  year         = 1999,
  url          = {http://citeseer.nj.nec.com/article/schapire99theoretical.html},
}

@INPROCEEDINGS{schapire98improved,
  author       = {Robert E. Schapire and Yoram Singer},
  title        = {Improved Boosting Algorithms using Confidence-Rated
                  Predictions},
  booktitle    = {Computational Learning Theory},
  pages        = {80-91},
  year         = 1998,
  url          = {http://citeseer.nj.nec.com/schapire99improved.html},
}

@ARTICLE{schapire99improved,
  author       = {Robert E.~Schapire and Yoram Singer},
  title        = {Improved Boosting Using Confidence-rated Predictions},
  journal      = {Machine Learning},
  volume       = 37,
  number       = 3,
  year         = 1999,
  pages        = {297--336},
}

@TECHREPORT{scott98parcel,
  author       = {M. Scott and M. Niranjan and R. Prager},
  title        = {Parcel: Feature subset selection in variable cost
                  domains},
  number       = {CUED/F-INFENG/TR 323},
  institution  = {Cambridge University},
  year         = 1998,
  url          = {http://citeseer.nj.nec.com/scott98parcel.html},
}

@ARTICLE{sharkey97combining,
  author       = {A. Sharkey and N. Sharkey},
  title        = {Combining diverse neural networks},
  journal      = {The Knowledge Engineering Review},
  year         = 1997,
  volume       = 12,
  pages        = {231-247},
  number       = 3,
}

@INBOOK{sharkey98:book,
  publisher    = {Springer-Verlag},
  pages        = {1--30},
  year         = 1999,
  author       = {Amanda Sharkey},
  chapter      = {Combining Artificial Neural Nets: Ensemble and
                  Modular Multi-Net Systems},
  title        = {Multi-Net Systems},
}

@INPROCEEDINGS{sharkey97:diversity,
  booktitle    = {Neural Networks and their Applications (NEURAP'97)},
  pages        = {205--212},
  year         = 1997 ,
  author       = {Amanda Sharkey and Noel Sharkey},
  title        = {Diversity, Selection, and Ensembles of Artificial
                  Neural Nets},
}

@ARTICLE{sharkeychandroth96,
  title        = {Diverse Neural Net Solutions to a Fault Diagnosis
                  Problem},
  year         = 1996,
  journal      = {Neural Computing and Applications},
  volume       = 4 ,
  author       = {Amanda Sharkey and Noel Sharkey and Gopinath
                  Chandroth},
  pages        = {218--227},
}

@INPROCEEDINGS{sharkey98:adapting,
  author       = {Amanda Sharkey and Noel Sharkey and Simon Cross},
  booktitle    = {ICANN `98},
  publisher    = {Springer-Verlag},
  title        = {Adapting an ensemble approach for the diagnosis of
                  breast cancer},
  year         = 1998,
  pages        = {281-286},
}

@ARTICLE{sharkey97:arm,
  author       = {Noel Sharkey},
  title        = {Artificial Neural Networks for Coordination and
                  Control: The Portability of Experimental
                  Representations},
  journal      = {Robotics and Autonomous Systems},
  year         = 1997,
  volume       = 22,
  pages        = {345-360},
}

@ARTICLE{sharkey95weight,
  author       = {Sharkey,N.E. and Neary,J. and Sharkey,A.J.C.},
  title        = {{S}earching {W}eight {S}pace for {B}ackpropagation
                  {S}olution {T}ypes},
  journal      = {{C}urrent {T}rends in {C}onnectionism: {P}roceedings
                  of the 1995 {S}wedish {C}onference on
                  {C}onnectionism},
  editor       = {Niklasson,L.F. and Boden,M.B.},
  pages        = {103--120},
  year         = 1995,
}

@ARTICLE{sierra98global,
  author       = {A. Sierra and C. Cruz},
  title        = {Global and Local Neural Network Ensembles},
  journal      = {Pattern Recognition Letters},
  volume       = 19,
  number       = 8,
  pages        = {651--655},
  year         = 1998,
  pdf          = {http://dx.doi.org/10.1016/S0167-8655(98)00042-7},
}

@ARTICLE{smith92cooperative,
  number       = 2,
  pages        = {127--149},
  title        = {Searching for Diverse Cooperative Populations with
                  Genetic Algorithms},
  volume       = 1,
  year         = 1992 ,
  journal      = {Evolutionary Computation},
  author       = {Robert E. Smith and Stephanie Forrest and Alan
                  S. Perelson},
}

@ARTICLE{sollichkrogh96:overfitting,
  author       = {P. Sollich and A. Krogh},
  title        = {Learning with ensembles: How overfitting can be
                  useful},
  pages        = {190--196},
  booktitle    = {Advances in Neural Information Processing Systems},
  editor       = {David S. Touretzky and Michael C. Mozer and Michael
                  E. Hasselmo},
  volume       = 8,
  year         = 1996,
  publisher    = {The {MIT} Press},
  abstract     = {We study the characteristics of learning with
                  ensembles. Solving exactly the simple model of an
                  ensemble of linear students, we find surprisingly
                  rich behaviour. For learning in large ensembles, it
                  is advantageous to use under-regularized students,
                  which actually over-fit the training data. Globally
                  optimal performance can be obtained by choosing the
                  training set sizes of the students
                  appropriately. For smaller ensembles, optimization
                  of the ensemble weights can yield significant
                  improvements in ensemble generalization performance,
                  in particular if the individual students are subject
                  to noise in the training process. Choosing students
                  with a wide range of regularization parameters makes
                  this improvement robust against changes in the
                  unknown level of noise in the training data.},
}

@ARTICLE{swann98:fastcommittee,
  volume       = 34,
  number       = 14,
  title        = {Fast Committee Learning: Preliminary Results},
  month        = {July},
  journal      = {Electronics Letters},
  pages        = {1408-1410},
  year         = 1998,
  author       = {A. Swann and N. Allinson},
}

@ARTICLE{taniguchi97averaging,
  author       = {Michiaki Taniguchi and Volker Tresp},
  title        = {Averaging Regularized Estimators},
  journal      = {Neural Computation},
  volume       = 9,
  number       = 5,
  pages        = {1163-1178},
  year         = 1997,
}

@ARTICLE{tetko95overfitting,
  author       = {Tetko, I. V. and Livingstone, D. J. and Luik, A. I.},
  title        = {Neural network studies. 1. Comparison of overfitting
                  and overtraining},
  journal      = {Journal of Chemical Information & Computer Sciences},
  volume       = 35,
  number       = 5,
  pages        = {826-833},
  abstract     = {The application of feed forward back propagation
                  artificial neural networks with one hidden layer
                  (ANN) to perform the equivalent of multiple linear
                  regression (MLR) has been examined using artificial
                  structured data sets and real literature data. The
                  predictive ability of the networks has been
                  estimated using a training/test set protocol. The
                  results have shown advantages of ANN over MLR
                  analysis. The ANNs do not require high order terms
                  or indicator variables to establish complex
                  structure-activity relationships. Overfitting does
                  not have any influence on network prediction ability
                  when overtraining is avoided by
                  cross-validation. Application of ANN ensembles has
                  allowed the avoidance of chance correlations and
                  satisfactory predictions of new data have been
                  obtained for a wide range of numbers of neurons in
                  the hidden layer. [References: 30]},
  keywords     = {Qsar},
  year         = 1995,
}

@ARTICLE{tetko93structure,
  author       = {Tetko, I. V. and Luik, A. I. and Poda, G. I.},
  title        = {Applications of neural networks in
                  structure-activity relationships of a small number
                  of molecules},
  journal      = {Journal of Medicinal Chemistry},
  volume       = 36,
  number       = 7,
  pages        = {811-4},
  abstract     = {We investigated the applications of back propagation
                  artificial neural networks (ANN) for a small dataset
                  analysis in the field of structure-activity
                  relationships. The derivatives of carboquinone were
                  used as an example. It\'s been found that in this
                  case the use of the same neural network results in
                  unambiguous classification of new
                  molecules. Predictions can be improved with
                  statistical analysis of independent prognosis
                  sets. We suggest that the sign criterion be used as
                  a classification rule. We also compared neural
                  networks with FALS and ALS in leave-one-out
                  prediction. ANN applied to the same dataset has
                  shown the same predictive ability as ALS but poorer
                  than FALS.},
  keywords     = {*Carbazilquinone/aa [Analogs & Derivatives]
                  Comparative Study *Mitomycins/pd [Pharmacology]
                  *Neural Networks (Computer) Structure-Activity
                  Relationship},
  year         = 1993,
}

@ARTICLE{tetko97epa,
  author       = {Tetko, I. V. and Villa, A. E.},
  title        = {An efficient partition of training data set improves
                  speed and accuracy of cascade-correlation algorithm},
  journal      = {Neural Processing Letters},
  volume       = 6,
  number       = {1-2},
  pages        = {51-59},
  abstract     = {This study extends an application of efficient
                  partition algorithm (EPA) for artificial neural
                  network ensemble trained according to Cascade
                  Correlation Algorithm. We show that EPA allows to
                  decrease the number of cases in learning and
                  validated data sets. The predictive ability of the
                  ensemble calculated using the whole data set is not
                  affected and in some cases it is even improved. It
                  is shown that a distribution of cases selected by
                  this method is proportional to the second derivative
                  of the analyzed function},
  keywords     = {algorithm, cascade correlation, early stopping,
                  efficient partition of training data set},
  year         = 1997,
}

@ARTICLE{tetko97overfitting,
  author       = {Tetko, I. V. and Villa, A. E.},
  title        = {An enhancement of generalization ability in cascade
                  correlation algorithm by avoidance of
                  overfitting/overtraining problem},
  journal      = {Neural Processing Letters},
  volume       = 6,
  number       = {1-2},
  pages        = {43-50},
  abstract     = {The current study investigates a method for
                  avoidance of an overfitting/overtraining problem in
                  Artificial Neural Network (ANN) based on a
                  combination of two algorithms: Early Stopping and
                  Ensemble averaging (ESE). We show that ESE provides
                  an improvement of the prediction ability of ANN
                  trained according to Cascade Correlation
                  Algorithm. A simple algorithm to estimate the
                  generalization ability of the method according to
                  the Leave-One-Out technique is proposed and
                  discussed. In the accompanying paper the problem of
                  optimal selection of training cases is considered
                  for accelerated learning of the ESE method},
  keywords     = {cascade correlation algorithm, early stopping,
                  overfitting, overtraining},
  year         = 1997,
}

@ARTICLE{tetko97partition,
  author       = {Tetko, I. V. and Villa, A. E.},
  title        = {Efficient Partition of Learning Data Sets for Neural
                  Network Training},
  journal      = {Neural Networks},
  volume       = 10,
  number       = 8,
  pages        = {1361-1374.},
  abstract     = {This study investigates the emerging possibilities
                  of combining unsupervised and supervised learning in
                  neural network ensembles. Such strategy is used to
                  get an efficient partition of a noisy input data set
                  in order to focus the training of neural networks on
                  the most complex and informative domains of the data
                  set and accelerate the learning phase. The proposed
                  algorithm provides a good prediction accuracy using
                  fewer cases from non-informative domains according
                  to a correlative measure of dependency between cases
                  of the training set. This measure takes into account
                  internal relationships amid analyzed data and can be
                  used to cluster neighbor cases in a multidimensional
                  space and to filter out the outliers. The possible
                  relation of the proposed algorithm to brain
                  processing occurring in the thalamo-cortical pathway
                  is discussed.},
  year         = 1997,
}

@ARTICLE{tetko96variable,
  author       = {Tetko, I. V. and Villa, A. E. and Livingstone,
                  D. J.},
  title        = {Neural network studies. 2. Variable selection},
  journal      = {Journal of Chemical Information & Computer Sciences},
  volume       = 36,
  number       = 4,
  pages        = {794-803},
  abstract     = {Quantitative structure-activity relationship (QSAR)
                  studies usually require an estimation of the
                  relevance of a very large set of initial
                  variables. Determination of the most important
                  variables allows theoretically a better
                  generalization by all pattern recognition
                  methods. This study introduces and investigates five
                  pruning algorithms designed to estimate the
                  importance of input variables in feed-forward
                  artificial neural network trained by back
                  propagation algorithm (ANN) applications and to
                  prune nonrelevant ones in a statistically reliable
                  way. The analyzed algorithms performed similar
                  variable estimations for simulated data sets, but
                  differences were detected for real QSAR
                  examples. Improvement of ANN prediction ability was
                  shown after the pruning of redundant input
                  variables. The statistical coefficients computed by
                  ANNs for QSAR examples were better than those of
                  multiple linear regression. Restrictions of the
                  proposed algorithms and the potential use of ANNs
                  are discussed.},
  keywords     = {Databases, Factual Linear Models *Neural Networks
                  (Computer) Nonlinear Dynamics Pharmaceutical
                  Preparations/ch [Chemistry] Structure-Activity
                  Relationship Support, Non-U.S. Gov\'t},
  year         = 1996,
}

@ARTICLE{tetko98pruning,
  author       = {Tetko, I. V. and Villa, A. E. P. and Aksenova,
                  T. I. and Zielinski, W. L. and Brower, J. and
                  Collantes, E. R. and Welsh, W. J.},
  title        = {Application of a pruning algorithm to optimize
                  artificial neural networks for pharmaceutical
                  fingerprinting},
  journal      = {Journal of Chemical Information & Computer Sciences},
  volume       = 38,
  number       = 4,
  pages        = {660-668},
  abstract     = {The present study investigates an application of
                  artificial neural networks (ANNs) for use in
                  pharmaceutical fingerprinting. Several pruning
                  algorithms were applied to decrease the dimension of
                  the input parameter data set. A localized
                  fingerprint region was identified within the
                  original input parameter space from which a subset
                  of input parameters was extracted leading to
                  enhanced ANN performance. The present results
                  confirm that ANNs can provide a fast, accurate, and
                  consistent methodology applicable to pharmaceutical
                  fingerprinting. [References: 26]},
  year         = 1998,
}

@BOOK{tikhonov77book,
  author       = {A. N. Tikhonov and V. Y. Arsenin},
  title        = {Solutions of Ill-posed problems},
  publisher    = {W.H.Winston and Sons, Washington D.C.},
  year         = 1977,
}

@INPROCEEDINGS{ting97stacked,
  author       = {Kai Ming Ting and Ian H. Witten},
  title        = {Stacked Generalization: When Does It Work?},
  booktitle    = {{IJCAI} (2)},
  pages        = {866--873},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/ting97stacked.html},
}

@INPROCEEDINGS{tresp95combining,
  author       = {Volker Tresp and Michiaki Taniguchi},
  title        = {Combining Estimators Using Non-Constant Weighting
                  Functions},
  pages        = {419--426},
  editor       = {G. Tesauro and D. Touretzky and T. Leen},
  volume       = 7,
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = 1995,
  publisher    = {The {MIT} Press},
  abstract     = {This paper discusses the linearly weighted
                  combination of estimators in which the weighting
                  functions are dependent on the input. We show that
                  the weighting functions can be derived either by
                  evaluating the input dependent variance of each
                  estimator or by estimating how likely it is that a
                  given estimator has seen data in the region of the
                  input space close to the input pattern. The latter
                  solution is closely related to the mixture of
                  experts approach and we show how learning rules for
                  the mixture of experts can be derived from the
                  theory about learning with missing features. The
                  presented approaches are modular since the weighting
                  functions can easily be modified (no retraining) if
                  more estimators are added. Furthermore, it is easy
                  to incorporate estimators which were not derived
                  from data such as expert systems or algorithms.},
}

@ARTICLE{tumerghosh94framework,
  author       = {K. Tumer and J. Ghosh},
  title        = {A framework for estimating performance improvements
                  in hybrid pattern classifiers},
  journal      = {World Congress on Neural Networks},
  volume       = 3,
  pages        = {220--5},
  publisher    = {Lawrence Erlbaum Associates},
  month        = {June},
  year         = 1994,
  abstract     = {Classification methods often perform significantly
                  below Bayesian limits in complex, high-dimensional
                  classification tasks because of model bias,
                  inadequate training data and noise/variability in
                  the data. When several classifiers are used for a
                  given task, selecting one method over all others
                  discards potentially valuable
                  information. Strategies aimed at suitably combining
                  the results of multiple classifiers are expected to
                  perform better than any single method, and reduce
                  overall bias and noise. An underwater passive sonar
                  data set consisting of over 1000 samples processed
                  to produce different 25-dimensional and
                  24-dimensional feature vectors is used in this study
                  to examine an evidence combination framework. An
                  analysis of the conditions that the data sets must
                  satisfy, and the conditions under which improvements
                  can be obtained is provided, and results are
                  presented for hybrid networks using both local and
                  global classifiers. },
}

@INPROCEEDINGS{tumerghosh94limits,
  author       = {K. Tumer and J. Ghosh},
  title        = {Limits to Performance Gains in Combined Neural
                  Classifiers},
  editor       = {Dagli and Akay and Chen and Fernandez and Ghosh},
  booktitle    = {Intelligent engineering systems through artificial
                  neural networks: proceedings of the Artificial
                  Neural Networks in Engineering ({ANNIE} '95)
                  Conference},
  publisher    = {American Society of Mechanical Engineers},
  address      = {United Engineering Center, 345 E. 47th St., New
                  York, NY 10017, USA},
  year         = 1994,
  volume       = 5,
  series       = {ASME Press Series on International Advances in
                  Design Productivity},
  abstract     = {The performance of a single classifier is often
                  inadequate in difficult classification problems. In
                  such cases, several researchers have combined the
                  outputs of multiple classifiers to obtain better
                  performance. However, the amount of improvement
                  possible through such combination techniques is
                  generally not known. We present two approaches to
                  estimating performance limits in hybrid
                  networks. First, we present a framework that
                  estimates Bayes error rates when linear combiners
                  are used. Then we discuss a more general method that
                  provides decision confidences and error bounds based
                  on error types arising from the training data. The
                  methods are illustrated for a difficult four class
                  problem involving underwater acoustic data. For this
                  data, we compute the single classifier and combiner
                  classification performances, as well as the Bayes
                  error rate and an error bound.},
}

@INPROCEEDINGS{tumerghosh95boundary,
  author       = {K. Tumer and J. Ghosh},
  title        = {Boundary variance reduction for improved
                  classification through hybrid networks},
  booktitle    = {Proceedings of the Spie Conf. on Applications and
                  Science of Artificial Neural Networks IV},
  volume       = 2492,
  pages        = {573--585},
  month        = {April},
  year         = 1995,
  address      = {Orlando, FL},
  abstract     = {Several researchers have shown experimentally that
                  substantial improvements can be obtained in
                  difficult pattern recognition problems by combining
                  or integrating the outputs of multiple
                  classifiers. This paper provides an analytical
                  framework that quantifies the improvements in
                  classification results due to linear combination. We
                  show that combining networks in the output space
                  reduces the variance of the actual decision region
                  boundaries around the optimum boundary. In the
                  absence of network bias, the added classification
                  error is directly proportional to the boundary
                  variance. Moreover, if the network errors are
                  independent, then the reduction in variance boundary
                  location is by a factor of N, the number of
                  classifiers that are combined. In the presence of
                  network bias, the reductions are less than or equal
                  to N, depending on the interaction between network
                  biases. We discuss how the individual networks can
                  be selected to achieve significant gains through
                  combination, and we support them with experimental
                  results on 25-dimensional sonar data. The analysis
                  presented facilitates the understanding of the
                  relationships among error rates, classifier boundary
                  distributions and combination in the output space.},
}

@ARTICLE{tumerghosh95orderstats,
  author       = {K. Tumer and J. Ghosh},
  title        = {Order Statistics Combiners for Neural Classifiers},
  journal      = {World Congress on Neural Networks},
  volume       = {Vol. I},
  pages        = {31--34},
  publisher    = {INNS Press},
  address      = {Washington, DC},
  month        = {July},
  year         = 1995,
  abstract     = {Several researchers have shown that linearly
                  combining outputs of multiple neural classifiers
                  results in better performance for many
                  applications. In this paper we introduce a family of
                  order statistics combiners as an alternative to
                  linear combiners. We show analytically that the
                  selection of the median, the maximum and in general,
                  the i-th order statistic improves classification
                  performance. Specifically, we show that order
                  statistics combiners reduce the variance of the
                  actual decision boundaries around the optimum
                  boundary, and that this is directly related to
                  classification error.},
}

@TECHREPORT{tumerghosh95theoretical,
  author       = {Kagan Tumer and Joydeep Ghosh},
  title        = {Theoretical Foundations of Linear and Order
                  Statistics Combiners for Neural Pattern Classifiers},
  institution  = {Computer and Vision Research Center, University of
                  Texas, Austin},
  year         = 1995,
  number       = {TR-95-02-98},
  url          = {http://citeseer.nj.nec.com/tumer96theoretical.html},
}

@ARTICLE{tumerghosh96,
  author       = {Kagan Tumer and Joydeep Ghosh},
  title        = {Error Correlation and Error Reduction in Ensemble
                  Classifiers},
  journal      = {Connection Science},
  volume       = 8,
  number       = {3-4},
  pages        = {385--403},
  year         = 1996,
  abstract     = {Using an ensemble of classifiers, instead of a
                  single classifier, can lead to improved
                  generalization. The gains obtained by combining,
                  however, are often affected more by the selection of
                  what is presented to the combiner than by the actual
                  combining method that is chosen. In this paper, we
                  focus on data selection and classifier training
                  methods, in order to 'prepare' classifiers for
                  combining. We review a combining framework for
                  classification problems that quantifies the need for
                  reducing the correlation among individual
                  classifiers. Then, we discuss several methods that
                  make the classifiers in an ensemble more
                  complementary. Experimental results are provided to
                  illustrate the benefits and pitfalls of reducing the
                  correlation among classifiers, especially when the
                  training data are in limited supply.},
}

@ARTICLE{tumerghosh96analysis,
  author       = {Kagan Tumer and Joydeep Ghosh},
  title        = {Analysis of decision boundaries in linearly combined
                  neural classifiers},
  journal      = {Pattern Recognition},
  year         = 1996,
  volume       = 29,
  number       = 2,
  pages        = {341--348},
  month        = {February},
}

@INPROCEEDINGS{tumerghosh96estimating,
  author       = {Kagan Tumer and Joydeep Ghosh},
  title        = {Estimating the Bayes Error Rate Through Classifier
                  Combining},
  booktitle    = {Proceedings of the 13th International Conference on
                  Pattern Recognition},
  month        = {August},
  year         = 1996,
  abstract     = {The Bayes error provides the lowest achievable error
                  rate for a given pattern classification
                  problem. There are several classical approaches for
                  estimating or finding bounds for the Bayes
                  error. One type of approach focuses on obtaining
                  analytical bounds, which are both difficult to
                  calculate and dependent on distribution parameters
                  that may not be known. Another strategy is to
                  estimate the class densities through non-parametric
                  methods, and use these estimates to obtain bounds on
                  the Bayes error. This article presents a novel
                  approach to estimating the Bayes error based on
                  classifier combining techniques. For an artificial
                  data set where the Bayes error is known, the
                  combiner-based estimate outperforms the classical
                  methods.},
}

@ARTICLE{tumer98rbfmedical,
  author       = {Kagan Tumer and Nirmala Ramanujam and Joydeep Ghosh
                  and Rebecca Richards-Kortum},
  title        = {Ensembles of Radial Basis Function Networks for
                  Spectroscopic Detection of Cervical Pre-Cancer},
  journal      = {IEEE Transactions on Biomedical Engineering},
  year         = 1998,
  volume       = 45,
  number       = 8,
  pages        = {953--961},
  abstract     = {Medical applications usually used Radial Basis
                  Function Networks just as Artificial Neural
                  Networks. However, RBFNs are Knowledge-Based
                  Networks that can be interpreted in several way:
                  Artificial Neural Networks, Regularization Networks,
                  Support Vector Machines, Wavelet Networks, Fuzzy
                  Controllers, Kernel Estimators, Instanced-Based
                  Learners. A survey of their interpretations and of
                  their corresponding learning algorithms is provided
                  as well as a brief survey on dynamic learning
                  algorithms. RBFNs' interpretations can suggest
                  applications that are particularly interesting in
                  medical domains. },
}

@INPROCEEDINGS{uedanakano96,
  booktitle    = {Proceedings of International Conference on Neural
                  Networks},
  title        = {Generalization Error of Ensemble Estimators},
  author       = {N. Ueda and R. Nakano},
  year         = 1996 ,
  pages        = {90--95},
  annote       = {First presentation (in ML literature) of the bias-variance-covariance decomposition.}
}

@INPROCEEDINGS{wahba99biasvariance,
  author       = {G. Wahba and X. Lin and F. Gao and D. Xiang and
                  R. Klein and B. Klein},
  title        = {The bias-variance tradeoff and the randomized
                  {GACV}},
  booktitle    = {Advances in Neural Information Processing Systems},
  number       = 11,
  pages        = {620--626},
  publisher    = {MIT Press},
  editor       = {M. Kearns and S. Solla and D. Cohn},
  year         = 1999,
  url          = {http://citeseer.nj.nec.com/wahba99biasvariance.html},
}

@ARTICLE{wan90bayesneural,
  author       = {E. A. Wan},
  title        = {Neural network classification: A Bayesian
                  interpretation},
  journal      = {IEEE Transactions on Neural Networks},
  type         = {Letter},
  year         = 1990,
  volume       = 1,
  number       = 4,
  pages        = {303--304},
}

@INPROCEEDINGS{wan97:sunspots,
  author       = {Eric A. Wan},
  booktitle    = {International Conference On Neural Networks
                  (ICNN97)},
  year         = 1997 ,
  title        = {Combining Fossil and Sunspot Data: Committee
                  Predictions},
  url          = {http://citeseer.ist.psu.edu/146595.html},
}

@INPROCEEDINGS{wezel98maximum,
  author       = {M.C. van Wezel and W.A. Kosters and J.N. Kok},
  title        = {Maximum Likelihood Weights for a Linear Ensemble of
                  Regression Neural Networks},
  booktitle    = {Proceedings International Conference in Neural
                  Information Processing (ICONIP\'98)},
  year         = 1998,
  publisher    = {IOS Press},
  editor       = {S. Usui and T. Omori},
  pages        = {498--501},
  address      = {Kitakyushu, Japan},
}

@ARTICLE{windeatt97spectral,
  author       = {Windeatt, T. and Tebbs, R.},
  title        = {Spectral technique for hidden layer neural network
                  training},
  journal      = {Pattern Recognition Letters},
  volume       = 18,
  issue        = 8,
  pages        = 723,
  year         = 1997,
  month        = {August},
}

@ARTICLE{wolpert92stacked,
  author       = {D. H. Wolpert},
  title        = {Stacked Generalization},
  journal      = {Neural Networks},
  volume       = 5,
  year         = 1992,
  pages        = {241--259},
  abstract     = {This paper introduces stacked generalization, a
                  scheme for minimizing the generalization error rate
                  of one or more generalizers. Stacked generalization
                  works by deducing the biases of the generalizer(s)
                  with respect to a provided learning set. This
                  deduction proceeds by generalizing in a second space
                  whose inputs are (for example) the guesses of the
                  original generalizers when taught with part of the
                  learning set and trying to guess the rest of it, and
                  whose output is (for example) the correct
                  guess. When used with multiple generalizers, stacked
                  generalization can be seen as a more sophisticated
                  version of cross-validation, exploiting a strategy
                  more sophisticated than cross-validation's crude
                  winner-takes-all for combining the individual
                  generalizers. When used with a single generalizer,
                  stacked generalization is a scheme for estimating
                  (and then correcting for) the error of a generalizer
                  which has been trained on a particular learning set
                  and then asked a particular question. After
                  introducing stacked generalization and justifying
                  its use, this paper presents two numerical
                  experiments. The first demonstrates how stacked
                  generalization improves upon a set of separate
                  generalizers for the NETtalk task or translating
                  text to phonemes. The second demonstrates how
                  stacked generalization improves the performance of a
                  single surface-fitter. With the other experimental
                  evidence in the literature, the usual arguments
                  supporting cross-validation, and the abstract
                  justifications presented in this paper, the
                  conclusion is that for almost any real-world
                  generalization problem one should use some version
                  of stacked generalization to minimize the
                  generalization error rate. This paper ends by
                  discussing some of the variation of stacked
                  generalization, and how it touches on other fields
                  like chaos theory. },
}

@ARTICLE{wolpert97bias,
  author       = {David Wolpert},
  title        = {On Bias Plus Variance},
  journal      = {Neural Computation},
  volume       = 9,
  number       = 6,
  pages        = {1211-1243},
  year         = 1997,
  url          = {http://citeseer.nj.nec.com/article/wolpert96bias.html},
}

@ARTICLE{woods97local,
  author       = {K. Woods and W.P. Kegelmeyer and K. Bowyer},
  title        = {Combination of multiple classifiers using local
                  accuracy estimates},
  journal      = {IEEE Transactions on Pattern Analysis and Machine
                  Intelligence},
  year         = 1997,
  volume       = 19,
  pages        = {405-410},
}

@INCOLLECTION{yao99review,
  publisher    = {IEEE},
  pages        = {1423-1447},
  volume       = 87 ,
  year         = 1999,
  author       = {Xin Yao},
  number       = 9,
  title        = {Evolving Artificial Neural Networks},
  month        = {September},
  booktitle    = {Proceedings of the IEEE},
}

@INCOLLECTION{yaoliu96,
  author       = {Xin Yao and Yong Liu},
  pages        = {229-242},
  booktitle    = {Complex Systems - From Local Interactions to Global
                  Phenomena},
  publisher    = {IOS Press, Amsterdam},
  title        = {How to Make Best Use of Evolutionary Learning},
  year         = 1996,
}

@ARTICLE{yaoliu97epnet,
  author       = {Xin Yao and Yong Liu},
  number       = 3,
  pages        = {694--713},
  journal      = {IEEE Transactions on Neural Networks},
  year         = 1997,
  title        = {A new evolutionary system for evolving artificial
                  neural networks},
  month        = {May},
  volume       = 8,
}

@INCOLLECTION{yaoliu98making_use,
  pages        = {417--425},
  booktitle    = {IEEE Transactions on Systems, Man and Cybernetics,
                  Part B: Cybernetics},
  year         = 1998,
  volume       = 28,
  number       = 3,
  publisher    = {IEEE Press},
  title        = {Making use of Population Information in Evolutionary
                  Artificial Neural Networks},
  month        = {June},
  author       = {Xin Yao and Yong Liu},
}

@INPROCEEDINGS{yaoliu99breastcancer,
  author       = {Xin Yao and Yong Liu},
  title        = {Neural networks for breast cancer diagnosis},
  year         = 1999,
  booktitle    = {Proceedings of the 1999 Congress on Evolutionary
                  Computation},
  pages        = {1760-1767},
  volume       = 3,
  month        = {July},
  publisher    = {IEEE Press},
}

@ARTICLE{yates95use,
  author       = {W. Yates and D. Partridge},
  title        = {Use of methodological diversity to improve neural
                  network generalization},
  journal      = {Neural Computing and Applications},
  year         = 1996,
  volume       = 4,
  pages        = {114--128},
  number       = 2,
  url          = {http://citeseer.nj.nec.com/partridge95use.html},
}

@ARTICLE{condorcet88review,
  author       = {P. Young},
  title        = {Condorcet's Theory of Voting},
  journal      = {American Political Science Review},
  year         = 1988,
  volume       = 82,
  number       = {1231-1244},
}
